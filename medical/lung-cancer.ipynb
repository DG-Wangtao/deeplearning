{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/waylandwong/lung-cancer?scriptVersionId=140248912\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 肺癌检测","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-17T16:21:45.906685Z","iopub.execute_input":"2023-08-17T16:21:45.907124Z","iopub.status.idle":"2023-08-17T16:21:45.930224Z","shell.execute_reply.started":"2023-08-17T16:21:45.907087Z","shell.execute_reply":"2023-08-17T16:21:45.92911Z"}}},{"cell_type":"code","source":"pip install torch torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-17T16:53:16.420393Z","iopub.execute_input":"2023-08-17T16:53:16.420796Z","iopub.status.idle":"2023-08-17T16:53:29.860596Z","shell.execute_reply.started":"2023-08-17T16:53:16.420763Z","shell.execute_reply":"2023-08-17T16:53:29.859413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install diskcache cassandra-driver","metadata":{"execution":{"iopub.status.busy":"2023-08-17T16:57:34.725504Z","iopub.execute_input":"2023-08-17T16:57:34.725942Z","iopub.status.idle":"2023-08-17T16:57:50.497197Z","shell.execute_reply.started":"2023-08-17T16:57:34.725905Z","shell.execute_reply":"2023-08-17T16:57:50.495868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 预先加载磁盘中的CT和标注数据","metadata":{}},{"cell_type":"code","source":"import copy\nimport csv\nimport functools\nimport glob\nimport os\n\nfrom collections import namedtuple\n\nimport numpy as np\n\nimport torch\nimport torch.cuda\nfrom torch.utils.data import Dataset\n\n\nCandidateInfoTuple = namedtuple(\n    'CandidateInfoTuple',\n    'isNodule_bool, diameter_mm, series_uid, center_xyz',\n)\n\n# 加载候选点信息数据\n@functools.lru_cache(1)\ndef getCandidateInfoList(requireOnDisk_bool=True):\n    # We construct a set with all series_uids that are present on disk.\n    # This will let us use the data, even if we haven't downloaded all of\n    # the subsets yet.\n    mhd_list = glob.glob('/kaggle/input/luna-lung-cancer-dataset/seg-lungs-LUNA16/seg-lungs-LUNA16/*.mhd')\n    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n\n    diameter_dict = {}\n    with open('/kaggle/input/luna-lung-cancer-dataset/annotations.csv', \"r\") as f:\n        for row in list(csv.reader(f))[1:]:\n            series_uid = row[0]\n            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n            annotationDiameter_mm = float(row[4])\n\n            diameter_dict.setdefault(series_uid, []).append(\n                (annotationCenter_xyz, annotationDiameter_mm)\n            )\n\n    candidateInfo_list = []\n    with open('/kaggle/input/luna-lung-cancer-dataset/candidates.csv', \"r\") as f:\n        for row in list(csv.reader(f))[1:]:\n            series_uid = row[0]\n\n            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n                continue\n\n            isNodule_bool = bool(int(row[4]))\n            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n\n            candidateDiameter_mm = 0.0\n            for annotation_tup in diameter_dict.get(series_uid, []):\n                annotationCenter_xyz, annotationDiameter_mm = annotation_tup\n                for i in range(3):\n                    delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n                    if delta_mm > annotationDiameter_mm / 4:\n                        break\n                else:\n                    candidateDiameter_mm = annotationDiameter_mm\n                    break\n\n            candidateInfo_list.append(CandidateInfoTuple(\n                isNodule_bool,\n                candidateDiameter_mm,\n                series_uid,\n                candidateCenter_xyz,\n            ))\n    # 按第一个字段（是否是结节）排序\n    candidateInfo_list.sort(reverse=True)\n    return candidateInfo_list\n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T16:38:09.01774Z","iopub.execute_input":"2023-08-17T16:38:09.018191Z","iopub.status.idle":"2023-08-17T16:38:09.036567Z","shell.execute_reply.started":"2023-08-17T16:38:09.018154Z","shell.execute_reply":"2023-08-17T16:38:09.035512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## logconf.py","metadata":{}},{"cell_type":"code","source":"import logging\nimport logging.handlers\n\nroot_logger = logging.getLogger()\nroot_logger.setLevel(logging.INFO)\n\n# Some libraries attempt to add their own root logger handlers. This is\n# annoying and so we get rid of them.\nfor handler in list(root_logger.handlers):\n    root_logger.removeHandler(handler)\n\nlogfmt_str = \"%(asctime)s %(levelname)-8s pid:%(process)d %(name)s:%(lineno)03d:%(funcName)s %(message)s\"\nformatter = logging.Formatter(logfmt_str)\n\nstreamHandler = logging.StreamHandler()\nstreamHandler.setFormatter(formatter)\nstreamHandler.setLevel(logging.DEBUG)\n\nroot_logger.addHandler(streamHandler)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T16:52:00.923556Z","iopub.execute_input":"2023-08-17T16:52:00.924408Z","iopub.status.idle":"2023-08-17T16:52:00.930739Z","shell.execute_reply.started":"2023-08-17T16:52:00.924368Z","shell.execute_reply":"2023-08-17T16:52:00.929931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### util.py","metadata":{}},{"cell_type":"code","source":"import collections\nimport copy\nimport datetime\nimport gc\nimport time\n\n# import torch\nimport numpy as np\n\nimport logging\nlog = logging.getLogger(__name__)\n# log.setLevel(logging.WARN)\n# log.setLevel(logging.INFO)\nlog.setLevel(logging.DEBUG)\n\nIrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col'])\nXyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z'])\n\ndef irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_a):\n    cri_a = np.array(coord_irc)[::-1]\n    origin_a = np.array(origin_xyz)\n    vxSize_a = np.array(vxSize_xyz)\n    coords_xyz = (direction_a @ (cri_a * vxSize_a)) + origin_a\n    # coords_xyz = (direction_a @ (idx * vxSize_a)) + origin_a\n    return XyzTuple(*coords_xyz)\n\ndef xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n    origin_a = np.array(origin_xyz)\n    vxSize_a = np.array(vxSize_xyz)\n    coord_a = np.array(coord_xyz)\n    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a\n    cri_a = np.round(cri_a)\n    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))\n\n\ndef importstr(module_str, from_=None):\n    \"\"\"\n    >>> importstr('os')\n    <module 'os' from '.../os.pyc'>\n    >>> importstr('math', 'fabs')\n    <built-in function fabs>\n    \"\"\"\n    if from_ is None and ':' in module_str:\n        module_str, from_ = module_str.rsplit(':')\n\n    module = __import__(module_str)\n    for sub_str in module_str.split('.')[1:]:\n        module = getattr(module, sub_str)\n\n    if from_:\n        try:\n            return getattr(module, from_)\n        except:\n            raise ImportError('{}.{}'.format(module_str, from_))\n    return module\n\n\n# class dotdict(dict):\n#     '''dict where key can be access as attribute d.key -> d[key]'''\n#     @classmethod\n#     def deep(cls, dic_obj):\n#         '''Initialize from dict with deep conversion'''\n#         return cls(dic_obj).deepConvert()\n#\n#     def __getattr__(self, attr):\n#         if attr in self:\n#             return self[attr]\n#         log.error(sorted(self.keys()))\n#         raise AttributeError(attr)\n#         #return self.get(attr, None)\n#     __setattr__= dict.__setitem__\n#     __delattr__= dict.__delitem__\n#\n#\n#     def __copy__(self):\n#         return dotdict(self)\n#\n#     def __deepcopy__(self, memo):\n#         new_dict = dotdict()\n#         for k, v in self.items():\n#             new_dict[k] = copy.deepcopy(v, memo)\n#         return new_dict\n#\n#     # pylint: disable=multiple-statements\n#     def __getstate__(self): return self.__dict__\n#     def __setstate__(self, d): self.__dict__.update(d)\n#\n#     def deepConvert(self):\n#         '''Convert all dicts at all tree levels into dotdict'''\n#         for k, v in self.items():\n#             if type(v) is dict: # pylint: disable=unidiomatic-typecheck\n#                 self[k] = dotdict(v)\n#                 self[k].deepConvert()\n#             try: # try enumerable types\n#                 for m, x in enumerate(v):\n#                     if type(x) is dict: # pylint: disable=unidiomatic-typecheck\n#                         x = dotdict(x)\n#                         x.deepConvert()\n#                         v[m] = x#\n\n#             except TypeError:\n#                 pass\n#         return self\n#\n#     def copy(self):\n#         # override dict.copy()\n#         return dotdict(self)\n\n\ndef prhist(ary, prefix_str=None, **kwargs):\n    if prefix_str is None:\n        prefix_str = ''\n    else:\n        prefix_str += ' '\n\n    count_ary, bins_ary = np.histogram(ary, **kwargs)\n    for i in range(count_ary.shape[0]):\n        print(\"{}{:-8.2f}\".format(prefix_str, bins_ary[i]), \"{:-10}\".format(count_ary[i]))\n    print(\"{}{:-8.2f}\".format(prefix_str, bins_ary[-1]))\n\n# def dumpCuda():\n#     # small_count = 0\n#     total_bytes = 0\n#     size2count_dict = collections.defaultdict(int)\n#     size2bytes_dict = {}\n#     for obj in gc.get_objects():\n#         if isinstance(obj, torch.cuda._CudaBase):\n#             nbytes = 4\n#             for n in obj.size():\n#                 nbytes *= n\n#\n#             size2count_dict[tuple([obj.get_device()] + list(obj.size()))] += 1\n#             size2bytes_dict[tuple([obj.get_device()] + list(obj.size()))] = nbytes\n#\n#             total_bytes += nbytes\n#\n#     # print(small_count, \"tensors equal to or less than than 16 bytes\")\n#     for size, count in sorted(size2count_dict.items(), key=lambda sc: (size2bytes_dict[sc[0]] * sc[1], sc[1], sc[0])):\n#         print('{:4}x'.format(count), '{:10,}'.format(size2bytes_dict[size]), size)\n#     print('{:10,}'.format(total_bytes), \"total bytes\")\n\n\ndef enumerateWithEstimate(\n        iter,\n        desc_str,\n        start_ndx=0,\n        print_ndx=4,\n        backoff=None,\n        iter_len=None,\n):\n    \"\"\"\n    In terms of behavior, `enumerateWithEstimate` is almost identical\n    to the standard `enumerate` (the differences are things like how\n    our function returns a generator, while `enumerate` returns a\n    specialized `<enumerate object at 0x...>`).\n\n    However, the side effects (logging, specifically) are what make the\n    function interesting.\n\n    :param iter: `iter` is the iterable that will be passed into\n        `enumerate`. Required.\n\n    :param desc_str: This is a human-readable string that describes\n        what the loop is doing. The value is arbitrary, but should be\n        kept reasonably short. Things like `\"epoch 4 training\"` or\n        `\"deleting temp files\"` or similar would all make sense.\n\n    :param start_ndx: This parameter defines how many iterations of the\n        loop should be skipped before timing actually starts. Skipping\n        a few iterations can be useful if there are startup costs like\n        caching that are only paid early on, resulting in a skewed\n        average when those early iterations dominate the average time\n        per iteration.\n\n        NOTE: Using `start_ndx` to skip some iterations makes the time\n        spent performing those iterations not be included in the\n        displayed duration. Please account for this if you use the\n        displayed duration for anything formal.\n\n        This parameter defaults to `0`.\n\n    :param print_ndx: determines which loop interation that the timing\n        logging will start on. The intent is that we don't start\n        logging until we've given the loop a few iterations to let the\n        average time-per-iteration a chance to stablize a bit. We\n        require that `print_ndx` not be less than `start_ndx` times\n        `backoff`, since `start_ndx` greater than `0` implies that the\n        early N iterations are unstable from a timing perspective.\n\n        `print_ndx` defaults to `4`.\n\n    :param backoff: This is used to how many iterations to skip before\n        logging again. Frequent logging is less interesting later on,\n        so by default we double the gap between logging messages each\n        time after the first.\n\n        `backoff` defaults to `2` unless iter_len is > 1000, in which\n        case it defaults to `4`.\n\n    :param iter_len: Since we need to know the number of items to\n        estimate when the loop will finish, that can be provided by\n        passing in a value for `iter_len`. If a value isn't provided,\n        then it will be set by using the value of `len(iter)`.\n\n    :return:\n    \"\"\"\n    if iter_len is None:\n        iter_len = len(iter)\n\n    if backoff is None:\n        backoff = 2\n        while backoff ** 7 < iter_len:\n            backoff *= 2\n\n    assert backoff >= 2\n    while print_ndx < start_ndx * backoff:\n        print_ndx *= backoff\n\n    log.warning(\"{} ----/{}, starting\".format(\n        desc_str,\n        iter_len,\n    ))\n    start_ts = time.time()\n    for (current_ndx, item) in enumerate(iter):\n        yield (current_ndx, item)\n        if current_ndx == print_ndx:\n            # ... <1>\n            duration_sec = ((time.time() - start_ts)\n                            / (current_ndx - start_ndx + 1)\n                            * (iter_len-start_ndx)\n                            )\n\n            done_dt = datetime.datetime.fromtimestamp(start_ts + duration_sec)\n            done_td = datetime.timedelta(seconds=duration_sec)\n\n            log.info(\"{} {:-4}/{}, done at {}, {}\".format(\n                desc_str,\n                current_ndx,\n                iter_len,\n                str(done_dt).rsplit('.', 1)[0],\n                str(done_td).rsplit('.', 1)[0],\n            ))\n\n            print_ndx *= backoff\n\n        if current_ndx + 1 == start_ndx:\n            start_ts = time.time()\n\n    log.warning(\"{} ----/{}, done at {}\".format(\n        desc_str,\n        iter_len,\n        str(datetime.datetime.now()).rsplit('.', 1)[0],\n    ))\n\n#\n# try:\n#     import matplotlib\n#     matplotlib.use('agg', warn=False)\n#\n#     import matplotlib.pyplot as plt\n#     # matplotlib color maps\n#     cdict = {'red':   ((0.0,  1.0, 1.0),\n#                        # (0.5,  1.0, 1.0),\n#                        (1.0,  1.0, 1.0)),\n#\n#              'green': ((0.0,  0.0, 0.0),\n#                        (0.5,  0.0, 0.0),\n#                        (1.0,  0.5, 0.5)),\n#\n#              'blue':  ((0.0,  0.0, 0.0),\n#                        # (0.5,  0.5, 0.5),\n#                        # (0.75, 0.0, 0.0),\n#                        (1.0,  0.0, 0.0)),\n#\n#              'alpha':  ((0.0, 0.0, 0.0),\n#                        (0.75, 0.5, 0.5),\n#                        (1.0,  0.5, 0.5))}\n#\n#     plt.register_cmap(name='mask', data=cdict)\n#\n#     cdict = {'red':   ((0.0,  0.0, 0.0),\n#                        (0.25,  1.0, 1.0),\n#                        (1.0,  1.0, 1.0)),\n#\n#              'green': ((0.0,  1.0, 1.0),\n#                        (0.25,  1.0, 1.0),\n#                        (0.5, 0.0, 0.0),\n#                        (1.0,  0.0, 0.0)),\n#\n#              'blue':  ((0.0,  0.0, 0.0),\n#                        # (0.5,  0.5, 0.5),\n#                        # (0.75, 0.0, 0.0),\n#                        (1.0,  0.0, 0.0)),\n#\n#              'alpha':  ((0.0, 0.15, 0.15),\n#                        (0.5,  0.3, 0.3),\n#                        (0.8,  0.0, 0.0),\n#                        (1.0,  0.0, 0.0))}\n#\n#     plt.register_cmap(name='maskinvert', data=cdict)\n# except ImportError:\n#     pass\n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T16:52:07.785187Z","iopub.execute_input":"2023-08-17T16:52:07.785989Z","iopub.status.idle":"2023-08-17T16:52:07.819275Z","shell.execute_reply.started":"2023-08-17T16:52:07.785941Z","shell.execute_reply":"2023-08-17T16:52:07.818018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### disk.py","metadata":{}},{"cell_type":"code","source":"\nimport gzip\n\nfrom cassandra.cqltypes import BytesType\n\n# And the BytesIO line should be changed to the following:\nfrom diskcache import FanoutCache, Disk,core\nfrom diskcache.core import io\nfrom io import BytesIO\nfrom diskcache.core import MODE_BINARY\n\nimport logging\nlog = logging.getLogger(__name__)\n# log.setLevel(logging.WARN)\nlog.setLevel(logging.INFO)\n# log.setLevel(logging.DEBUG)\n\n\nclass GzipDisk(Disk):\n    def store(self, value, read, key=None):\n        \"\"\"\n        Override from base class diskcache.Disk.\n\n        Chunking is due to needing to work on pythons < 2.7.13:\n        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n          compression and decompression operations did not properly handle results of\n          2 or 4 GiB.\n\n        :param value: value to convert\n        :param bool read: True when value is file-like object\n        :return: (size, mode, filename, value) tuple for Cache table\n        \"\"\"\n        # pylint: disable=unidiomatic-typecheck\n        if type(value) is BytesType:\n            if read:\n                value = value.read()\n                read = False\n\n            str_io = BytesIO()\n            gz_file = gzip.GzipFile(mode='wb', compresslevel=1, fileobj=str_io)\n\n            for offset in range(0, len(value), 2**30):\n                gz_file.write(value[offset:offset+2**30])\n            gz_file.close()\n\n            value = str_io.getvalue()\n\n        return super(GzipDisk, self).store(value, read)\n\n\n    def fetch(self, mode, filename, value, read):\n        \"\"\"\n        Override from base class diskcache.Disk.\n\n        Chunking is due to needing to work on pythons < 2.7.13:\n        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n          compression and decompression operations did not properly handle results of\n          2 or 4 GiB.\n\n        :param int mode: value mode raw, binary, text, or pickle\n        :param str filename: filename of corresponding value\n        :param value: database value\n        :param bool read: when True, return an open file handle\n        :return: corresponding Python value\n        \"\"\"\n        value = super(GzipDisk, self).fetch(mode, filename, value, read)\n\n        if mode == MODE_BINARY:\n            str_io = BytesIO(value)\n            gz_file = gzip.GzipFile(mode='rb', fileobj=str_io)\n            read_csio = BytesIO()\n\n            while True:\n                uncompressed_data = gz_file.read(2**30)\n                if uncompressed_data:\n                    read_csio.write(uncompressed_data)\n                else:\n                    break\n\n            value = read_csio.getvalue()\n\n        return value\n\ndef getCache(scope_str):\n    return FanoutCache('data-unversioned/cache/' + scope_str,\n                       disk=GzipDisk,\n                       shards=64,\n                       timeout=1,\n                       size_limit=3e11,\n                       # disk_min_file_size=2**20,\n                       )\n\n# def disk_cache(base_path, memsize=2):\n#     def disk_cache_decorator(f):\n#         @functools.wraps(f)\n#         def wrapper(*args, **kwargs):\n#             args_str = repr(args) + repr(sorted(kwargs.items()))\n#             file_str = hashlib.md5(args_str.encode('utf8')).hexdigest()\n#\n#             cache_path = os.path.join(base_path, f.__name__, file_str + '.pkl.gz')\n#\n#             if not os.path.exists(os.path.dirname(cache_path)):\n#                 os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n#\n#             if os.path.exists(cache_path):\n#                 return pickle_loadgz(cache_path)\n#             else:\n#                 ret = f(*args, **kwargs)\n#                 pickle_dumpgz(cache_path, ret)\n#                 return ret\n#\n#         return wrapper\n#\n#     return disk_cache_decorator\n#\n#\n# def pickle_dumpgz(file_path, obj):\n#     log.debug(\"Writing {}\".format(file_path))\n#     with open(file_path, 'wb') as file_obj:\n#         with gzip.GzipFile(mode='wb', compresslevel=1, fileobj=file_obj) as gz_file:\n#             pickle.dump(obj, gz_file, pickle.HIGHEST_PROTOCOL)\n#\n#\n# def pickle_loadgz(file_path):\n#     log.debug(\"Reading {}\".format(file_path))\n#     with open(file_path, 'rb') as file_obj:\n#         with gzip.GzipFile(mode='rb', fileobj=file_obj) as gz_file:\n#             return pickle.load(gz_file)\n#\n#\n# def dtpath(dt=None):\n#     if dt is None:\n#         dt = datetime.datetime.now()\n#\n#     return str(dt).rsplit('.', 1)[0].replace(' ', '--').replace(':', '.')\n#\n#\n# def safepath(s):\n#     s = s.replace(' ', '_')\n#     return re.sub('[^A-Za-z0-9_.-]', '', s)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T17:00:14.815876Z","iopub.execute_input":"2023-08-17T17:00:14.816302Z","iopub.status.idle":"2023-08-17T17:00:14.836086Z","shell.execute_reply.started":"2023-08-17T17:00:14.816271Z","shell.execute_reply":"2023-08-17T17:00:14.834914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CT file loader","metadata":{}},{"cell_type":"code","source":"import SimpleITK as sitk\n\n\nclass Ct:\n    def __init__(self, series_uid):\n        mhd_path = glob.glob(\n            '/kaggle/input/luna-lung-cancer-dataset/seg-lungs-LUNA16/seg-lungs-LUNA16/{}.mhd'.format(series_uid)\n        )[0]\n\n        ct_mhd = sitk.ReadImage(mhd_path)\n        # ct_a\n        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n\n        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n        # The upper bound nukes any weird hotspots and clamps bone down\n        ct_a.clip(-1000, 1000, ct_a)\n\n        self.series_uid = series_uid\n        self.hu_a = ct_a\n\n        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n\n    def getRawCandidate(self, center_xyz, width_irc):\n        center_irc = xyz2irc(\n            center_xyz,\n            self.origin_xyz,\n            self.vxSize_xyz,\n            self.direction_a,\n        )\n\n        slice_list = []\n        for axis, center_val in enumerate(center_irc):\n            start_ndx = int(round(center_val - width_irc[axis]/2))\n            end_ndx = int(start_ndx + width_irc[axis])\n\n            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n\n            if start_ndx < 0:\n                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n                start_ndx = 0\n                end_ndx = int(width_irc[axis])\n\n            if end_ndx > self.hu_a.shape[axis]:\n                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n                end_ndx = self.hu_a.shape[axis]\n                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n\n            slice_list.append(slice(start_ndx, end_ndx))\n\n        ct_chunk = self.hu_a[tuple(slice_list)]\n\n        return ct_chunk, center_irc\n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T17:00:34.66417Z","iopub.execute_input":"2023-08-17T17:00:34.664621Z","iopub.status.idle":"2023-08-17T17:00:34.679072Z","shell.execute_reply.started":"2023-08-17T17:00:34.664586Z","shell.execute_reply":"2023-08-17T17:00:34.678096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}