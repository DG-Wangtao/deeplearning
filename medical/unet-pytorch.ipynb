{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 安装依赖","metadata":{}},{"cell_type":"markdown","source":"- https://github.com/milesial/Pytorch-UNet/tree/master\n- https://github.com/njcronin/DL_Track\n- https://github.com/njcronin/DL_Track/blob/master/Labelling_Instructions.pdf","metadata":{}},{"cell_type":"code","source":"!pip install scipy scikit-image torch torchvision pathlib wandb","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:03:47.075246Z","iopub.execute_input":"2023-08-27T11:03:47.075690Z","iopub.status.idle":"2023-08-27T11:03:53.503371Z","shell.execute_reply.started":"2023-08-27T11:03:47.075651Z","shell.execute_reply":"2023-08-27T11:03:53.502111Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.1)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (0.21.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\nRequirement already satisfied: pathlib in /opt/conda/lib/python3.10/site-packages (1.0.1)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.5)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.23.5)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (3.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2.31.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2023.4.12)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.4.1)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (21.3)\nRequirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (0.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# 引用依赖包","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.nn.functional import relu, pad\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom PIL import Image\nfrom typing import Tuple\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom tqdm import tqdm\nimport wandb\nimport logging","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:03:53.505990Z","iopub.execute_input":"2023-08-27T11:03:53.507149Z","iopub.status.idle":"2023-08-27T11:03:53.520076Z","shell.execute_reply.started":"2023-08-27T11:03:53.507105Z","shell.execute_reply":"2023-08-27T11:03:53.519104Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"markdown","source":"## U-Net 网络","metadata":{}},{"cell_type":"code","source":"\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n    \n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=False):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = (DoubleConv(n_channels, 64))\n        self.down1 = (Down(64, 128))\n        self.down2 = (Down(128, 256))\n        self.down3 = (Down(256, 512))\n        factor = 2 if bilinear else 1\n        self.down4 = (Down(512, 1024 // factor))\n        self.up1 = (Up(1024, 512 // factor, bilinear))\n        self.up2 = (Up(512, 256 // factor, bilinear))\n        self.up3 = (Up(256, 128 // factor, bilinear))\n        self.up4 = (Up(128, 64, bilinear))\n        self.outc = (OutConv(64, n_classes))\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits\n\n    def use_checkpointing(self):\n        self.inc = torch.utils.checkpoint(self.inc)\n        self.down1 = torch.utils.checkpoint(self.down1)\n        self.down2 = torch.utils.checkpoint(self.down2)\n        self.down3 = torch.utils.checkpoint(self.down3)\n        self.down4 = torch.utils.checkpoint(self.down4)\n        self.up1 = torch.utils.checkpoint(self.up1)\n        self.up2 = torch.utils.checkpoint(self.up2)\n        self.up3 = torch.utils.checkpoint(self.up3)\n        self.up4 = torch.utils.checkpoint(self.up4)\n        self.outc = torch.utils.checkpoint(self.outc)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:03:53.521794Z","iopub.execute_input":"2023-08-27T11:03:53.522204Z","iopub.status.idle":"2023-08-27T11:03:53.552814Z","shell.execute_reply.started":"2023-08-27T11:03:53.522170Z","shell.execute_reply":"2023-08-27T11:03:53.551709Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"markdown","source":"# 定义数据集加载器","metadata":{}},{"cell_type":"code","source":"# TODO: image和mask名称不一样时跳过\nclass APODataSet(Dataset):\n    # 格式不对的异常数据\n    invalid_img = [10, 184, 185]\n    def __init__(self, img_dir, mask_dir: str, size) -> None:\n        # 获取所有图片路径\n        self.img_paths = list(Path(img_dir).glob(\"*\"))\n        self.mask_paths = list(Path(mask_dir).glob(\"*\"))\n        for idx in self.invalid_img:\n            del self.img_paths[idx]\n            del self.mask_paths[idx]\n        \n        \n        # 设置 transforms\n        self.transform = transforms.Compose([transforms.Resize(size), transforms.ToTensor()])\n#         self.transform = transforms.Compose([transforms.PILToTensor()])\n\n    # 使用函数加载原始图像\n    def load_orig_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.img_paths[index]\n        return Image.open(image_path) \n    \n    # 使用函数加载tmask图像\n    def load_mask_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.mask_paths[index]\n        return Image.open(image_path) \n\n    #  重写 __len__() 方法 (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.img_paths)\n\n    # 重写 __getitem__() 方法 (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"Returns one sample of data, image and mask (X, y).\"\n        orig_img = self.load_orig_image(index)\n        mask_img = self.load_mask_image(index)\n        \n        orig_img = self.transform(orig_img)\n        mask_img = self.transform(mask_img)\n        mask_img = mask_img[0]\n#         if orig_img.size()[0] != 3:\n#             print(\"{}: orig_img size: {}\".format(index,orig_img.size()))\n#             return None\n        # return data, mask (X, y)\n        return orig_img, mask_img\n","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:03:53.555969Z","iopub.execute_input":"2023-08-27T11:03:53.556378Z","iopub.status.idle":"2023-08-27T11:03:53.569065Z","shell.execute_reply.started":"2023-08-27T11:03:53.556340Z","shell.execute_reply":"2023-08-27T11:03:53.568210Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":"# 加载数据集","metadata":{}},{"cell_type":"code","source":"dataset =  APODataSet(img_dir = \"/kaggle/input/dltrack/apo_images\",\n                      mask_dir = \"/kaggle/input/dltrack/apo_masks\",\n                     size = [512, 512])\n\ntotal = len(dataset)\ntrain_size = int(0.8*total)\nvalidate_size = total - train_size\ntrain_data, validate_data = random_split(dataset, [train_size, validate_size])\nprint(\"dataset info\\ntotal: {}, train_size: {}, validate_size: {}\".format(total, len(train_data), len(validate_data)))\n\ntrainloader = DataLoader(dataset=train_data,\n                                     batch_size=2,\n                                     num_workers=0,\n                                     shuffle=True)\n\nvalloader = DataLoader(dataset=validate_data,\n                                    batch_size=1, \n                                    num_workers=0, \n                                    shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:03:53.570916Z","iopub.execute_input":"2023-08-27T11:03:53.571358Z","iopub.status.idle":"2023-08-27T11:03:53.592759Z","shell.execute_reply.started":"2023-08-27T11:03:53.571325Z","shell.execute_reply":"2023-08-27T11:03:53.591720Z"},"trusted":true},"execution_count":135,"outputs":[{"name":"stdout","text":"dataset info\ntotal: 571, train_size: 456, validate_size: 115\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 检查异常图片\n要把找到的异常数据去除","metadata":{}},{"cell_type":"code","source":"for index in range(len(dataset)):\n    orig_img, mask_img = dataset[index]\n    if orig_img.size()[0] != 3:\n        print(\"{}: orig_img size: {}\".format(index,orig_img.size()))\nprint(\"[done]\")","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:03:53.594389Z","iopub.execute_input":"2023-08-27T11:03:53.594729Z","iopub.status.idle":"2023-08-27T11:03:56.910634Z","shell.execute_reply.started":"2023-08-27T11:03:53.594696Z","shell.execute_reply":"2023-08-27T11:03:56.908910Z"},"trusted":true},"execution_count":136,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[136], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[0;32m----> 2\u001b[0m     orig_img, mask_img \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m orig_img\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: orig_img size: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(index,orig_img\u001b[38;5;241m.\u001b[39msize()))\n","Cell \u001b[0;32mIn[134], line 42\u001b[0m, in \u001b[0;36mAPODataSet.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     39\u001b[0m         mask_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_mask_image(index)\n\u001b[1;32m     41\u001b[0m         orig_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(orig_img)\n\u001b[0;32m---> 42\u001b[0m         mask_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m         mask_img \u001b[38;5;241m=\u001b[39m mask_img[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#         if orig_img.size()[0] != 3:\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#             print(\"{}: orig_img size: {}\".format(index,orig_img.size()))\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#             return None\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m# return data, mask (X, y)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    488\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:2157\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   2155\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(size)\n\u001b[0;32m-> 2157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2159\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:1201\u001b[0m, in \u001b[0;36mTiffImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtile \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_load_libtiff:\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_libtiff()\n\u001b[0;32m-> 1201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py:200\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# use mmap, if possible\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmmap\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap \u001b[38;5;241m=\u001b[39m mmap\u001b[38;5;241m.\u001b[39mmmap(fp\u001b[38;5;241m.\u001b[39mfileno(), \u001b[38;5;241m0\u001b[39m, access\u001b[38;5;241m=\u001b[39mmmap\u001b[38;5;241m.\u001b[39mACCESS_READ)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m args[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# buffer is not large enough\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m    byte sequences.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    310\u001b[0m         IncrementalDecoder\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors)\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;66;03m# undecoded input that is kept between calls to decode()\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"### 随机显示一张原始图片和其对应的标记图片","metadata":{}},{"cell_type":"code","source":"idx = random.randint(0, len(dataset))\norig_img, mask_img = dataset[idx]\nprint(orig_img.size())\n\ntransform = transforms.ToPILImage()\nprint(\"showing image of {}: \".format(idx))\n\norig_img = transform(orig_img)\nmask_img = transform(mask_img)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 12))\n\nax1.imshow(orig_img)\nax1.set_title(\"origin_img\")\n\nax2.imshow(mask_img)\nax2.set_title(\"mask_img\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:03:56.911868Z","iopub.status.idle":"2023-08-27T11:03:56.913093Z","shell.execute_reply.started":"2023-08-27T11:03:56.912811Z","shell.execute_reply":"2023-08-27T11:03:56.912836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 训练网络","metadata":{}},{"cell_type":"code","source":"\ndef dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n    # Average of Dice coefficient for all batches, or for a single mask\n#     print(\"input.size: {}, target.size: {}\".format(input.size(), target.size()))\n    assert input.size() == target.size()\n    assert input.dim() == 3 or not reduce_batch_first\n\n    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n\n    inter = 2 * (input * target).sum(dim=sum_dim)\n    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n\n    dice = (inter + epsilon) / (sets_sum + epsilon)\n    return dice.mean()\n\n\ndef multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n    # Average of Dice coefficient for all classes\n    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)\n\n\ndef dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n    # Dice loss (objective to minimize) between 0 and 1\n    fn = multiclass_dice_coeff if multiclass else dice_coeff\n    return 1 - fn(input, target, reduce_batch_first=True)\n\n\n@torch.inference_mode()\ndef evaluate(net, dataloader, device, amp):\n    net.eval()\n    num_val_batches = len(dataloader)\n    dice_score = 0\n\n    # iterate over the validation set\n    with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n        for batch in tqdm(dataloader, total=num_val_batches, desc='Validation round', unit='batch', leave=False):\n            image, mask_true = batch\n\n            # move images and labels to correct device and type\n            image = image.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n            mask_true = mask_true.to(device=device, dtype=torch.float32)\n\n            # predict the mask\n            mask_pred = net(image)[0]\n#             print(\"mask_pred: {}, mask_true: {}\".format(mask_pred.size(), len(mask_true.size())))\n\n            if net.n_classes == 1:\n                assert mask_true.min() >= 0 and mask_true.max() <= 1, 'True mask indices should be in [0, 1]'\n                mask_pred = (F.sigmoid(mask_pred) > 0.5).float()\n                # compute the Dice score\n                dice_score += dice_coeff(mask_pred, mask_true, reduce_batch_first=False)\n            else:\n                assert mask_true.min() >= 0 and mask_true.max() < net.n_classes, 'True mask indices should be in [0, n_classes['\n                # convert to one-hot format\n                mask_true = F.one_hot(mask_true, net.n_classes).permute(0, 3, 1, 2).float()\n                mask_pred = F.one_hot(mask_pred.argmax(dim=1), net.n_classes).permute(0, 3, 1, 2).float()\n                # compute the Dice score, ignoring background\n                dice_score += multiclass_dice_coeff(mask_pred[:, 1:], mask_true[:, 1:], reduce_batch_first=False)\n\n    net.train()\n    return dice_score / max(num_val_batches, 1)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:03:56.914681Z","iopub.status.idle":"2023-08-27T11:03:56.915178Z","shell.execute_reply.started":"2023-08-27T11:03:56.914910Z","shell.execute_reply":"2023-08-27T11:03:56.914932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 设置wandb账号\n用作统计与数据分析","metadata":{}},{"cell_type":"code","source":"os.environ['WANDB_API_KEY']='d561f1229ba7c4e207ca34042f29a43552a7447e'\n!wandb login","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:04:10.314566Z","iopub.execute_input":"2023-08-27T11:04:10.314972Z","iopub.status.idle":"2023-08-27T11:04:13.242411Z","shell.execute_reply.started":"2023-08-27T11:04:10.314937Z","shell.execute_reply":"2023-08-27T11:04:13.241123Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"d561f1229ba7c4e207ca34042f29a43552a7447e\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtorwayland\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nimport torch.optim as optim\n\nn_train = len(train_data)\nn_val = len(validate_data)\ndir_checkpoint = Path('./checkpoints/')\n\n\ndef train(model, device, \n          epochs: int = 5,\n          learning_rate: float = 1e-5, \n          weight_decay: float = 1e-8,\n          momentum: float = 0.999,\n          batch_size: int = 2,\n          amp: bool = False,\n          val_percent: float = 0.1,\n          gradient_clipping: float = 1.0):\n    \n    # (Initialize logging)\n    experiment = wandb.init(project='U-Net', resume='allow', anonymous='must')\n    experiment.config.update(\n        dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n             val_percent=val_percent, amp=amp)\n    )\n\n    logging.info(f'''Starting training:\n        Epochs:          {epochs}\n        Batch size:      {batch_size}\n        Learning rate:   {learning_rate}\n        Training size:   {n_train}\n        Validation size: {n_val}\n        Device:          {device.type}\n        Mixed Precision: {amp}\n    ''')\n    \n     # Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n    optimizer = optim.RMSprop(model.parameters(),\n                              lr=learning_rate, weight_decay=weight_decay, momentum=momentum, foreach=True)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)  # goal: maximize Dice score\n    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n    criterion = nn.CrossEntropyLoss() if model.n_classes > 1 else nn.BCEWithLogitsLoss()\n    global_step = 0\n\n    # 5. Begin training\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss = 0\n        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n            for batch in trainloader:\n                images, true_masks = batch\n\n                assert images.shape[1] == model.n_channels, \\\n                    f'Network has been defined with {model.n_channels} input channels, ' \\\n                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n                    'the images are loaded correctly.'\n\n                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n                \n                true_masks = true_masks.to(device=device, dtype=torch.float32)\n\n                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n                    masks_pred = model(images)\n                \n                    if model.n_classes == 1:\n                        loss = criterion(masks_pred.squeeze(1), true_masks.float())\n                        loss += dice_loss(F.sigmoid(masks_pred.squeeze(1)), true_masks.float(), multiclass=False)\n                    else:\n                        \n                        loss = criterion(masks_pred, true_masks)\n                        loss += dice_loss(\n                            F.softmax(masks_pred, dim=1).float(),\n                            F.one_hot(true_masks, model.n_classes).permute(0, 3, 1, 2).float(),\n                            multiclass=True\n                        )\n\n                optimizer.zero_grad(set_to_none=True)\n                grad_scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n                grad_scaler.step(optimizer)\n                grad_scaler.update()\n\n                pbar.update(images.shape[0])\n                global_step += 1\n                epoch_loss += loss.item()\n                experiment.log({\n                    'train loss': loss.item(),\n                    'step': global_step,\n                    'epoch': epoch\n                })\n                pbar.set_postfix(**{'loss (batch)': loss.item()})\n\n                # Evaluation round\n                division_step = (n_train // (5 * batch_size))\n                if division_step > 0:\n                    if global_step % division_step == 0:\n                        histograms = {}\n                        for tag, value in model.named_parameters():\n                            tag = tag.replace('/', '.')\n                            if not (torch.isinf(value) | torch.isnan(value)).any():\n                                histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n                            if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():\n                                histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n\n                        val_score = evaluate(model, valloader, device, amp)\n                        scheduler.step(val_score)\n\n                        logging.info('Validation Dice score: {}'.format(val_score))\n                        try:\n                            experiment.log({\n                                'learning rate': optimizer.param_groups[0]['lr'],\n                                'validation Dice': val_score,\n                                'images': wandb.Image(images[0].cpu()),\n                                'masks': {\n                                    'true': wandb.Image(true_masks[0].float().cpu()),\n                                    'pred': wandb.Image(masks_pred.argmax(dim=1)[0].float().cpu()),\n                                },\n                                'step': global_step,\n                                'epoch': epoch,\n                                **histograms\n                            })\n                        except:\n                            pass\n            \n\nif __name__ == '__main__':\n    model = UNet(n_channels=3, n_classes=1, bilinear=False)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#     if torch.cuda.device_count() > 1:\n#         print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n#         #dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n#         model = nn.DataParallel(model)\n        \n    model = model.to(memory_format=torch.channels_last)\n    model.to(device)\n    \n    train(model, device)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:03:56.922970Z","iopub.status.idle":"2023-08-27T11:03:56.923774Z","shell.execute_reply.started":"2023-08-27T11:03:56.923511Z","shell.execute_reply":"2023-08-27T11:03:56.923536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 推理","metadata":{}}]}