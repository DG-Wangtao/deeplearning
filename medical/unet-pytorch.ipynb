{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":6361775,"sourceType":"datasetVersion","datasetId":3664633}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 安装依赖","metadata":{}},{"cell_type":"markdown","source":"- https://github.com/milesial/Pytorch-UNet/tree/master\n- https://github.com/njcronin/DL_Track\n- https://github.com/njcronin/DL_Track/blob/master/Labelling_Instructions.pdf","metadata":{}},{"cell_type":"code","source":"!pip install scipy scikit-image torch torchvision pathlib wandb segmentation-models-pytorch\n!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2024-03-11T16:56:10.076141Z","iopub.execute_input":"2024-03-11T16:56:10.076514Z","iopub.status.idle":"2024-03-11T16:56:35.534857Z","shell.execute_reply.started":"2024-03-11T16:56:10.076485Z","shell.execute_reply":"2024-03-11T16:56:35.533490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 引用依赖包","metadata":{}},{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"execution":{"iopub.status.busy":"2024-03-11T16:56:35.537755Z","iopub.execute_input":"2024-03-11T16:56:35.538155Z","iopub.status.idle":"2024-03-11T16:56:35.548050Z","shell.execute_reply.started":"2024-03-11T16:56:35.538119Z","shell.execute_reply":"2024-03-11T16:56:35.546890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torchvision.transforms import v2\nfrom torch.nn.functional import relu, pad\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom PIL import Image\nfrom typing import Tuple\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom tqdm import tqdm\nimport wandb\nimport logging","metadata":{"execution":{"iopub.status.busy":"2024-03-11T16:56:35.549224Z","iopub.execute_input":"2024-03-11T16:56:35.549580Z","iopub.status.idle":"2024-03-11T16:56:35.564262Z","shell.execute_reply.started":"2024-03-11T16:56:35.549550Z","shell.execute_reply":"2024-03-11T16:56:35.562982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## U-Net 网络","metadata":{}},{"cell_type":"code","source":"\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super(DoubleConv,self).__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, dropout = 0.1):\n        super(Down, self).__init__()\n        self.double_conv = DoubleConv(in_channels, out_channels, kernel_size)\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            nn.Dropout2d(p=dropout),\n        )\n\n    def forward(self, x):\n        skip_out = self.double_conv(x)\n        down_output = self.maxpool_conv(skip_out)\n        return (down_output, skip_out)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size = 2, dropout = 0.1, stride = 2):\n        super().__init__()\n        \n        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size = kernel_size, stride = stride)\n        \n        self.conv = nn.Sequential(\n            nn.Dropout2d(p=dropout),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x1, x2):\n        x = self.up(x1)\n        x = torch.cat([x, x2], dim = 1)\n        return self.conv(x)\n    \n\nsigmoid = nn.Sigmoid()\nif torch.cuda.is_available():\n    sigmoid = sigmoid.cuda()\n\nclass UNet(nn.Module):\n    def __init__(self, input_channels, num_classes, n_filters = 64, bilinear=False):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n        self.input_channels = input_channels\n        kernel_size = 3\n        dropout = 0.25\n\n        self.down1 = Down(input_channels, n_filters, kernel_size, dropout)\n        self.down2 = Down(n_filters, n_filters * 2, kernel_size, dropout)\n        self.down3 = Down(n_filters * 2, n_filters * 4, kernel_size, dropout)\n        self.down4 = Down(n_filters * 4, n_filters * 8, kernel_size, dropout)\n        \n        self.bottle_conv = DoubleConv(n_filters * 8, n_filters * 16, kernel_size)\n        \n        kernel_size = kernel_size - 1\n        self.up4 = Up(n_filters * 16, n_filters * 8, kernel_size, dropout)\n        self.up3 = Up(n_filters * 8, n_filters * 4, kernel_size, dropout)\n        self.up2 = Up(n_filters * 4, n_filters * 2, kernel_size, dropout)\n        self.up1 = Up(n_filters * 2, n_filters, kernel_size, dropout)\n        \n        self.outc = nn.Conv2d(n_filters, num_classes, kernel_size=1)\n        \n\n    def forward(self, x):\n        \n        x, skip1 = self.down1(x)\n        x, skip2 = self.down2(x)\n        x, skip3 = self.down3(x)\n        x, skip4 = self.down4(x)\n        \n        x = self.bottle_conv(x)\n        \n        x = self.up4(x, skip4)\n        x = self.up3(x, skip3)\n        x = self.up2(x, skip2)\n        x = self.up1(x, skip1)\n        \n        out = self.outc(x)\n        if not self.training:\n            out = sigmoid(out)\n            out = torch.where(out>0.5,torch.ones_like(out),torch.zeros_like(out))\n        return out\n\n    def use_checkpointing(self):\n        self.down1 = torch.utils.checkpoint(self.down1)\n        self.down2 = torch.utils.checkpoint(self.down2)\n        self.down3 = torch.utils.checkpoint(self.down3)\n        self.down4 = torch.utils.checkpoint(self.down4)\n        self.bottle_conv = torch.utils.checkpoint(self.bottle_conv)\n        self.up1 = torch.utils.checkpoint(self.up1)\n        self.up2 = torch.utils.checkpoint(self.up2)\n        self.up3 = torch.utils.checkpoint(self.up3)\n        self.up4 = torch.utils.checkpoint(self.up4)\n        self.outc = torch.utils.checkpoint(self.outc)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T16:56:35.566007Z","iopub.execute_input":"2024-03-11T16:56:35.566396Z","iopub.status.idle":"2024-03-11T16:56:35.601110Z","shell.execute_reply.started":"2024-03-11T16:56:35.566357Z","shell.execute_reply":"2024-03-11T16:56:35.600132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 定义数据集加载器","metadata":{}},{"cell_type":"code","source":"# TODO: image和mask名称不一样时跳过\nclass APODataSet(Dataset):\n    # 格式不对的异常数据\n    def __init__(self, img_dir, mask_dir: str, size, trans = None, mixCut: bool = False) -> None:\n        self.mixCut = mixCut\n        # 获取所有图片路径\n        img_paths = list(Path(img_dir).glob(\"*\"))\n        mask_paths = list(Path(mask_dir).glob(\"*\"))\n        self.images = []\n        self.masks = []\n        for img_idx in range(len(img_paths)):\n            img_path = img_paths[img_idx]\n            img = self.load_image(img_path)\n            num_channels = len(img.getbands())\n            if num_channels != 3:\n                continue\n            \n            mask_path = mask_paths[img_idx]\n            self.images.append(img_path)\n            self.masks.append(mask_path)\n            \n        # 设置 transforms\n#         self.transform = transforms.Compose([ transforms.Resize(size), \n#                                              transforms.RandomHorizontalFlip(),  # 随机水平翻转\n#                                              transforms.RandomVerticalFlip(),    # 随机垂直旋转\n#                                              transforms.RandomRotation(10) ,     # 随机旋转 （-10,10）度\n#                                              transforms.ToTensor()])\n        if trans == None:\n            trans = transforms.Compose([ v2.Resize(size), \n                                             v2.RandomHorizontalFlip(),  # 随机水平翻转\n                                             v2.RandomVerticalFlip(),    # 随机垂直旋转\n                                             v2.RandomRotation(10) ,     # 随机旋转 （-10,10）度\n                                             v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n\n        self.transform = trans\n\n#         self.transform = transforms.Compose([transforms.PILToTensor()])\n\n    def load_image(self, path) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        return Image.open(path)\n    \n    #  重写 __len__() 方法 (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.images)\n    \n    #  CutMix 的切块功能\n \n    def rand_bbox(self, size, lam):\n        W = size[1]\n        H = size[2]\n        cut_rat = np.sqrt(1. - lam)\n        cut_w = int(W * cut_rat)\n        cut_h = int(H * cut_rat)\n\n        # uniform\n        cx = np.random.randint(W)\n        cy = np.random.randint(H)\n\n        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n        bby1 = np.clip(cy - cut_h // 2, 0, H)\n        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n        bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n        return bbx1, bby1, bbx2, bby2\n    \n    # 重写 __getitem__() 方法 (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"Returns one sample of data, image and mask (X, y).\"\n        orig_img = self.load_image(self.images[index])\n        mask_img = self.load_image(self.masks[index])\n        \n        seed = np.random.randint(2147483647)\n        torch.manual_seed(seed)   # 指定同样的随机种子，以保证图片和label的旋转（水平、垂直和rotation）一致\n        orig_img = self.transform(orig_img)\n        torch.manual_seed(seed)  # 指定同样的随机种子，以保证图片和label的旋转（水平、垂直和rotation）一致\n        mask_img = self.transform(mask_img)\n        \n        \n        #cutmix\n        if self.mixCut:\n            rand_index = random.randint(0, len(self)-1)\n            rand_orig_img = self.load_image(self.images[rand_index])\n            rand_mask_img = self.load_image(self.masks[rand_index])\n\n\n            torch.manual_seed(seed)   # 指定同样的随机种子，以保证图片和label的旋转（水平、垂直和rotation）一致\n            rand_orig_img = self.transform(rand_orig_img)\n            torch.manual_seed(seed)  # 指定同样的随机种子，以保证图片和label的旋转（水平、垂直和rotation）一致\n            rand_mask_img = self.transform(rand_mask_img)\n\n\n            lam = np.random.beta(1., 1.)\n            rand_index = torch.randperm(len(self)).cuda()\n            bbx1, bby1, bbx2, bby2 = self.rand_bbox(mask_img.size(), lam)\n\n\n            orig_img[:, bbx1:bbx2, bby1:bby2] = rand_orig_img[:, bbx1:bbx2, bby1:bby2]\n            mask_img[:, bbx1:bbx2, bby1:bby2] = rand_mask_img[:, bbx1:bbx2, bby1:bby2]\n        \n        \n        mask_img = torch.where(mask_img>0.5,torch.ones_like(mask_img),torch.zeros_like(mask_img))\n        \n        \n        return orig_img, mask_img\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-11T16:56:35.603815Z","iopub.execute_input":"2024-03-11T16:56:35.604110Z","iopub.status.idle":"2024-03-11T16:56:35.626399Z","shell.execute_reply.started":"2024-03-11T16:56:35.604084Z","shell.execute_reply":"2024-03-11T16:56:35.625486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 定义训练和验证的方法","metadata":{}},{"cell_type":"code","source":"\n@torch.inference_mode()\ndef evaluate(net, dataloader, device, amp, experiment, epoch, logging = False):\n    net.eval()\n    \n    num_val_batches = len(dataloader)\n    bce_loss = 0\n    dice_loss = 0\n    iou_score = 0\n\n    if isinstance(model, nn.DataParallel):\n        num_classes = net.module.num_classes\n    else:\n        num_classes = net.num_classes\n    \n    # 因为在非训练过程（推理过程中），已经在网络最后一层加了log和过滤\n    # 因此这里的损失函数都要使用不带log的\n    criterion = nn.BCELoss().cuda()\n    diceloss = smp.losses.DiceLoss(mode='binary', from_logits=False).cuda()\n    \n    g_bce_loss = 0\n    g_dice_loss = 0\n    g_iou_score = 0\n            \n    g_accuracy = 0\n    g_precision= 0\n    g_f1_score = 0\n    g_f2_score= 0\n    \n    idx = -1\n    # iterate over the validation set\n    with tqdm(total=num_val_batches, desc='Validation round', unit='batch', position=0 ,leave=True) as pbar:\n        for batch in dataloader:\n            idx += 1\n            \n            images, mask_true = batch\n\n            # move images and labels to correct device and type\n            images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n            mask_true = mask_true.to(device=device, dtype=torch.float32)\n\n            # predict the mask\n            mask_pred = net(images)\n            bce_loss = criterion(mask_pred, mask_true.float())\n            dice_loss = diceloss(mask_pred, mask_true)\n\n            tp, fp, fn, tn = smp.metrics.get_stats(mask_pred, mask_true.long(), mode='binary', threshold=0.5)\n\n            iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n            pbar.update(images.shape[0])\n            \n            f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n            f2_score = smp.metrics.fbeta_score(tp, fp, fn, tn, beta=2, reduction=\"micro\")\n        \n            accuracy = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"macro\")\n            precision = smp.metrics.precision(tp, fp, fn, tn, reduction=\"macro\")\n    \n\n        \n            g_bce_loss += bce_loss\n            g_dice_loss += dice_loss\n        \n            g_iou_score += iou_score\n        \n            g_f1_score += f1_score\n            g_f2_score += f2_score\n            \n            g_accuracy += accuracy\n            g_precision += precision\n            \n            \n            if logging:\n                try:\n                    experiment.log({\n                        'validation Loss': bce_loss + dice_loss,\n                        'accuracy': accuracy,\n                        'precision': precision,\n                        'f1_score': f1_score,\n                        'f2_score': f2_score,\n                        'images{}'.format(idx): wandb.Image(images[0].cpu(), caption=\"epoch{}\".format(epoch)),\n                        'masks{}'.format(idx): {\n                            'true': wandb.Image(mask_true[0].float().cpu(), caption=\"epoch{}\".format(epoch)),\n                            'pred': wandb.Image(mask_pred[0].float().cpu(), caption=\"epoch{}\".format(epoch)),\n                        },\n                        'validation IoU Score': iou_score,\n                    })\n                except Exception as e:\n                    print(e)\n                    pass\n\n\n\n        g_bce_loss = (g_bce_loss / max(num_val_batches, 1))\n        g_dice_loss = (g_dice_loss / max(num_val_batches, 1))\n        g_iou_score = (g_iou_score / max(num_val_batches, 1))\n        g_accuracy = (g_accuracy / max(num_val_batches, 1))\n        g_precision= (g_precision / max(num_val_batches, 1))\n        g_f1_score = (g_f1_score / max(num_val_batches, 1))\n        g_f2_score= (g_f2_score / max(num_val_batches, 1))\n\n        pbar.set_postfix(**{\"Validation bce loss\": bce_loss.item(), \"dice loss\": dice_loss.item(), \"IoU Score\": iou_score.item()})\n    \n    if logging:\n        try:\n            experiment.log({\n                'ave_validation Loss': g_bce_loss + g_dice_loss,\n                'ave_accuracy': g_accuracy,\n                'ave_precision':g_precision,\n                'ave_f1_score':g_f1_score,\n                'ave_f2_score':g_f2_score,\n                'average validation IoU Score': g_iou_score,\n            })\n        except Exception as e:\n            print(e)\n            pass\n    \n    return (dice_loss, iou_score)    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-11T16:56:35.628099Z","iopub.execute_input":"2024-03-11T16:56:35.628471Z","iopub.status.idle":"2024-03-11T16:56:35.661732Z","shell.execute_reply.started":"2024-03-11T16:56:35.628441Z","shell.execute_reply":"2024-03-11T16:56:35.660726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport torch.optim as optim\nimport segmentation_models_pytorch as smp\n\ndef train(model, device, project,\n          epochs: int = 60,\n          learning_rate: float = 1e-5, \n          weight_decay: float = 1e-8,\n          momentum: float = 0.999,\n          batch_size: int = 6,\n          amp: bool = False,\n          gradient_clipping: float = 1.0):\n    n_train = len(train_data)\n    n_val = len(validate_data)\n\n\n    if isinstance(model, nn.DataParallel):\n        num_classes = model.module.num_classes\n        input_channels = model.module.input_channels\n    else:\n        num_classes = model.num_classes\n        input_channels = model.input_channels\n        \n\n    # (Initialize logging)\n    experiment = wandb.init(project=project, resume='allow', anonymous='must', notes='水平和垂直翻转，旋转(-10,10)度，mixcut')\n    experiment.config.update(\n        dict(epochs=epochs, batch_size=batch_size, amp=True)\n    )\n    logging.info(f'''Starting training:\n        Epochs:          {epochs}\n        Batch size:      {batch_size}\n        Learning rate:   {learning_rate}\n        Training size:   {n_train}\n        Validation size: {n_val}\n        Device:          {device.type}\n        Mixed Precision: {amp}\n    ''')\n    \n\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2, eta_min=5e-5)  # goal: maximize Dice scor\n    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n    \n    # 训练过程中，网络最后一层没有添加log，所以要使用带log的损失函数\n    criterion = nn.BCEWithLogitsLoss().cuda()\n    dice_loss = smp.losses.DiceLoss(mode='binary', from_logits=True).cuda()\n\n    global_step = 0\n\n    # 5. Begin training\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss = 0\n        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='batch') as pbar:\n            for batch in trainloader:\n                images, true_masks = batch\n\n                assert images.shape[1] == input_channels, \\\n                    f'Network has been defined with {input_channels} input channels, ' \\\n                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n                    'the images are loaded correctly.'\n\n                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n                \n                true_masks = true_masks.to(device=device, dtype=torch.long)\n\n                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n                    masks_pred = model(images)\n                    loss = criterion(masks_pred, true_masks.float())\n                    loss += dice_loss(masks_pred, true_masks)\n                    tp, fp, fn, tn = smp.metrics.get_stats(masks_pred, true_masks.long(), mode='binary', threshold=0.5)\n                    iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n    \n                optimizer.zero_grad(set_to_none=True)\n                grad_scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n                grad_scaler.step(optimizer)\n                grad_scaler.update()\n\n                pbar.update(images.shape[0])\n                global_step += 1\n                epoch_loss += loss.item()\n                pbar.set_postfix(**{'loss (batch)': epoch_loss/n_train})\n                \n                if global_step % 10 == 0:\n                    experiment.log({\n                        'learning rate': optimizer.param_groups[0]['lr'],\n                        'train iou': iou_score,\n                        'train loss': loss.item(),\n                        'step': global_step,\n                        'epoch': epoch\n                    })\n                \n\n           # Evaluation round\n                division_step = (n_train // batch_size)\n                if division_step > 0:\n                    if global_step % division_step == 0:\n                        val_score, iou_score = evaluate(model, valloader, device, amp, experiment, epoch, logging = False)\n                        \n                        model.train()\n                        scheduler.step(val_score)\n        # 每10个 epoch 更新一遍 wandb\n        if epoch % 1 == 0:\n            evaluate(model, valloader, device, amp, experiment, epoch, logging = True)\n\n    experiment.finish()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-11T16:56:35.663290Z","iopub.execute_input":"2024-03-11T16:56:35.663693Z","iopub.status.idle":"2024-03-11T16:56:35.690695Z","shell.execute_reply.started":"2024-03-11T16:56:35.663661Z","shell.execute_reply":"2024-03-11T16:56:35.689786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 添加 MixCut 和 MixUp\n> https://pytorch.org/vision/main/auto_examples/transforms/plot_cutmix_mixup.html#as-part-of-the-collation-function","metadata":{}},{"cell_type":"code","source":"# print(torch.__version__)\n# from torchvision.transforms import v2\n# from torch.utils.data import default_collate\n# NUM_CLASSES = 1\n# cutmix = v2.CutMix(num_classes=NUM_CLASSES)\n# mixup = v2.MixUp(num_classes=NUM_CLASSES)\n# cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n\n# def collate_fn(batch):\n#     return cutmix_or_mixup(*default_collate(batch))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-11T16:56:35.693148Z","iopub.execute_input":"2024-03-11T16:56:35.693944Z","iopub.status.idle":"2024-03-11T16:56:35.707426Z","shell.execute_reply.started":"2024-03-11T16:56:35.693917Z","shell.execute_reply":"2024-03-11T16:56:35.706491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 加载数据集","metadata":{}},{"cell_type":"code","source":"batch_size=8\ndataset =  APODataSet(img_dir = \"/kaggle/input/dltrack/apo_images\",\n                      mask_dir = \"/kaggle/input/dltrack/apo_masks\",\n                     size = [512, 512], mixCut = True)\n\nvalDataset =  APODataSet(img_dir = \"/kaggle/input/dltrack/apo_images\",\n                      mask_dir = \"/kaggle/input/dltrack/apo_masks\",\n                    size = [512, 512], \n                    trans = transforms.Compose([ transforms.Resize([512, 512]), transforms.ToTensor()]),\n                    mixCut = False)\n\ntotal = len(dataset)\ntrain_size = int(0.8*total)\nvalidate_size = total - train_size\ntrain_data, validate_data = random_split(dataset, [train_size, validate_size])\nprint(\"dataset info\\ntotal: {}, train_size: {}, validate_size: {}\".format(total, len(train_data), len(validate_data)))\ntrainloader = DataLoader(dataset=train_data,\n                                     batch_size=batch_size,\n                                     num_workers=0,\n                                     shuffle=True)\nvalloader = DataLoader(dataset=valDataset,\n                                    batch_size=1, \n                                    num_workers=0, \n                                    shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T16:56:35.708776Z","iopub.execute_input":"2024-03-11T16:56:35.709066Z","iopub.status.idle":"2024-03-11T16:56:38.608633Z","shell.execute_reply.started":"2024-03-11T16:56:35.709042Z","shell.execute_reply":"2024-03-11T16:56:38.607605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 随机显示一张原始图片和其对应的标记图片","metadata":{}},{"cell_type":"code","source":"idx = random.randint(0, len(dataset))\norig_img, mask_img = dataset[idx]\n\nprint(orig_img.size())\nprint(mask_img.size())\n\n\nprint(\"showing image of {}: \".format(idx))\n\norig_img = orig_img.cpu().numpy().transpose(1, 2, 0)\nmask_img = mask_img.cpu().numpy().transpose(1, 2, 0)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 12))\n\nax1.imshow(orig_img)\nax1.grid(False)\nax1.axis('off')\nax1.set_title(\"origin_img\")\n\nax2.imshow(mask_img, cmap=\"gray\")\nax2.grid(False)\nax2.axis('off')\nax2.set_title(\"mask_img\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T16:56:38.609800Z","iopub.execute_input":"2024-03-11T16:56:38.610109Z","iopub.status.idle":"2024-03-11T16:56:39.110263Z","shell.execute_reply.started":"2024-03-11T16:56:38.610083Z","shell.execute_reply":"2024-03-11T16:56:39.109309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 训练网络","metadata":{}},{"cell_type":"markdown","source":"## 设置wandb账号\n用作统计与数据分析","metadata":{}},{"cell_type":"code","source":"os.environ['WANDB_API_KEY']='d561f1229ba7c4e207ca34042f29a43552a7447e'\n!wandb login","metadata":{"execution":{"iopub.status.busy":"2024-03-11T16:56:39.111501Z","iopub.execute_input":"2024-03-11T16:56:39.111810Z","iopub.status.idle":"2024-03-11T16:56:41.524954Z","shell.execute_reply.started":"2024-03-11T16:56:39.111784Z","shell.execute_reply":"2024-03-11T16:56:41.523806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=1000\nif __name__ == '__main__':\n    model = UNet(input_channels=3, num_classes=1, bilinear=False)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    if torch.cuda.device_count() > 1:\n        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n        model = nn.DataParallel(model)\n\n    model = model.to(memory_format=torch.channels_last)\n    model.to(device)\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"模型参数量为：{total_params}\")\n    print(\"其详情为：\")\n    for name,parameters in model.named_parameters():\n        print(name,':',parameters.size())\n    train(model, device, project=\"U-Net\", epochs=epochs, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T16:56:41.526518Z","iopub.execute_input":"2024-03-11T16:56:41.526878Z","iopub.status.idle":"2024-03-11T16:56:42.426457Z","shell.execute_reply.started":"2024-03-11T16:56:41.526851Z","shell.execute_reply":"2024-03-11T16:56:42.425080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 推理","metadata":{}}]}