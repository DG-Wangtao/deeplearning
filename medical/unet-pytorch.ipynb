{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6361775,"sourceType":"datasetVersion","datasetId":3664633}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 安装依赖","metadata":{}},{"cell_type":"markdown","source":"- https://github.com/milesial/Pytorch-UNet/tree/master\n- https://github.com/njcronin/DL_Track\n- https://github.com/njcronin/DL_Track/blob/master/Labelling_Instructions.pdf","metadata":{}},{"cell_type":"code","source":"!pip install scipy scikit-image torch torchvision pathlib wandb segmentation-models-pytorch\n!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:01.130318Z","iopub.execute_input":"2024-03-14T17:18:01.131006Z","iopub.status.idle":"2024-03-14T17:18:25.760464Z","shell.execute_reply.started":"2024-03-14T17:18:01.130976Z","shell.execute_reply":"2024-03-14T17:18:25.759224Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.4)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (0.22.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: pathlib in /opt/conda/lib/python3.10/site-packages (1.0.1)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.3)\nRequirement already satisfied: segmentation-models-pytorch in /opt/conda/lib/python3.10/site-packages (0.3.3)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.26.4)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (3.2.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2023.12.9)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (21.3)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.40.5)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.7.4)\nRequirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.7.1)\nRequirement already satisfied: timm==0.9.2 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.9.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.1)\nRequirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.2)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.3)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.40.5)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 引用依赖包","metadata":{}},{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:25.762340Z","iopub.execute_input":"2024-03-14T17:18:25.762664Z","iopub.status.idle":"2024-03-14T17:18:25.771957Z","shell.execute_reply.started":"2024-03-14T17:18:25.762635Z","shell.execute_reply":"2024-03-14T17:18:25.771179Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torchvision.transforms import v2\nfrom torch.nn.functional import relu, pad\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom PIL import Image\nfrom typing import Tuple\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom tqdm import tqdm\nimport wandb\nimport logging","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:25.772929Z","iopub.execute_input":"2024-03-14T17:18:25.773209Z","iopub.status.idle":"2024-03-14T17:18:25.787864Z","shell.execute_reply.started":"2024-03-14T17:18:25.773187Z","shell.execute_reply":"2024-03-14T17:18:25.787096Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## U-Net 网络","metadata":{}},{"cell_type":"code","source":"\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super(DoubleConv,self).__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, dropout = 0.1):\n        super(Down, self).__init__()\n        self.double_conv = DoubleConv(in_channels, out_channels, kernel_size)\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            nn.Dropout2d(p=dropout),\n        )\n\n    def forward(self, x):\n        skip_out = self.double_conv(x)\n        down_output = self.maxpool_conv(skip_out)\n        return (down_output, skip_out)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size = 2, dropout = 0.1, stride = 2):\n        super().__init__()\n        \n        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size = kernel_size, stride = stride)\n        \n        self.conv = nn.Sequential(\n            nn.Dropout2d(p=dropout),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x1, x2):\n        x = self.up(x1)\n        x = torch.cat([x, x2], dim = 1)\n        return self.conv(x)\n    \n\nsigmoid = nn.Sigmoid()\nif torch.cuda.is_available():\n    sigmoid = sigmoid.cuda()\n\nclass UNet(nn.Module):\n    def __init__(self, input_channels, num_classes, n_filters = 64, bilinear=False):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n        self.input_channels = input_channels\n        kernel_size = 3\n        dropout = 0.25\n\n        self.down1 = Down(input_channels, n_filters, kernel_size, dropout)\n        self.down2 = Down(n_filters, n_filters * 2, kernel_size, dropout)\n        self.down3 = Down(n_filters * 2, n_filters * 4, kernel_size, dropout)\n        self.down4 = Down(n_filters * 4, n_filters * 8, kernel_size, dropout)\n        \n        self.bottle_conv = DoubleConv(n_filters * 8, n_filters * 16, kernel_size)\n        \n        kernel_size = kernel_size - 1\n        self.up4 = Up(n_filters * 16, n_filters * 8, kernel_size, dropout)\n        self.up3 = Up(n_filters * 8, n_filters * 4, kernel_size, dropout)\n        self.up2 = Up(n_filters * 4, n_filters * 2, kernel_size, dropout)\n        self.up1 = Up(n_filters * 2, n_filters, kernel_size, dropout)\n        \n        self.outc = nn.Conv2d(n_filters, num_classes, kernel_size=1)\n        \n\n    def forward(self, x):\n        \n        x, skip1 = self.down1(x)\n        x, skip2 = self.down2(x)\n        x, skip3 = self.down3(x)\n        x, skip4 = self.down4(x)\n        \n        x = self.bottle_conv(x)\n        \n        x = self.up4(x, skip4)\n        x = self.up3(x, skip3)\n        x = self.up2(x, skip2)\n        x = self.up1(x, skip1)\n        \n        out = self.outc(x)\n        if not self.training:\n            out = sigmoid(out)\n            out = torch.where(out>0.5,torch.ones_like(out),torch.zeros_like(out))\n        return out\n\n    def use_checkpointing(self):\n        self.down1 = torch.utils.checkpoint(self.down1)\n        self.down2 = torch.utils.checkpoint(self.down2)\n        self.down3 = torch.utils.checkpoint(self.down3)\n        self.down4 = torch.utils.checkpoint(self.down4)\n        self.bottle_conv = torch.utils.checkpoint(self.bottle_conv)\n        self.up1 = torch.utils.checkpoint(self.up1)\n        self.up2 = torch.utils.checkpoint(self.up2)\n        self.up3 = torch.utils.checkpoint(self.up3)\n        self.up4 = torch.utils.checkpoint(self.up4)\n        self.outc = torch.utils.checkpoint(self.outc)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:25.790453Z","iopub.execute_input":"2024-03-14T17:18:25.790781Z","iopub.status.idle":"2024-03-14T17:18:25.814675Z","shell.execute_reply.started":"2024-03-14T17:18:25.790745Z","shell.execute_reply":"2024-03-14T17:18:25.813881Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# 定义数据集加载器","metadata":{}},{"cell_type":"code","source":"# TODO: image和mask名称不一样时跳过\nclass APODataSet(Dataset):\n    # 格式不对的异常数据\n    def __init__(self, img_dir, mask_dir: str, size, mixCut: bool = False) -> None:\n        self.mixCut = mixCut\n        # 获取所有图片路径\n        img_paths = list(Path(img_dir).glob(\"*\"))\n        mask_paths = list(Path(mask_dir).glob(\"*\"))\n        self.images = []\n        self.masks = []\n        for img_idx in range(len(img_paths)):\n            img_path = img_paths[img_idx]\n            img = self.load_image(img_path)\n            num_channels = len(img.getbands())\n            if num_channels != 3:\n                continue\n            \n            mask_path = mask_paths[img_idx]\n            self.images.append(img_path)\n            self.masks.append(mask_path)\n            \n        self.transform = transforms.Compose([ transforms.Resize(size), transforms.ToTensor()])\n\n    def load_image(self, path) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        return Image.open(path)\n    \n    #  重写 __len__() 方法 (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.images)\n    \n    #  CutMix 的切块功能\n \n    def rand_bbox(self, size, lam):\n        W = size[1]\n        H = size[2]\n        cut_rat = np.sqrt(1. - lam)\n        cut_w = int(W * cut_rat)\n        cut_h = int(H * cut_rat)\n\n        # uniform\n        cx = np.random.randint(W)\n        cy = np.random.randint(H)\n\n        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n        bby1 = np.clip(cy - cut_h // 2, 0, H)\n        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n        bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n        return bbx1, bby1, bbx2, bby2\n    \n    # 重写 __getitem__() 方法 (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"Returns one sample of data, image and mask (X, y).\"\n        orig_img = self.load_image(self.images[index])\n        mask_img = self.load_image(self.masks[index])\n\n        orig_img = self.transform(orig_img)\n        mask_img = self.transform(mask_img)\n        \n        \n        #cutmix\n        if self.mixCut:\n            rand_index = random.randint(0, len(self)-1)\n            rand_orig_img = self.load_image(self.images[rand_index])\n            rand_mask_img = self.load_image(self.masks[rand_index])\n\n            rand_orig_img = self.transform(rand_orig_img)=\n            rand_mask_img = self.transform(rand_mask_img)\n\n\n            lam = np.random.beta(1., 1.)\n            rand_index = torch.randperm(len(self)).cuda()\n            bbx1, bby1, bbx2, bby2 = self.rand_bbox(mask_img.size(), lam)\n\n\n            orig_img[:, bbx1:bbx2, bby1:bby2] = rand_orig_img[:, bbx1:bbx2, bby1:bby2]\n            mask_img[:, bbx1:bbx2, bby1:bby2] = rand_mask_img[:, bbx1:bbx2, bby1:bby2]\n        \n        \n        mask_img = torch.where(mask_img>0.5,torch.ones_like(mask_img),torch.zeros_like(mask_img))\n        \n        \n        return orig_img, mask_img\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:25.815846Z","iopub.execute_input":"2024-03-14T17:18:25.816161Z","iopub.status.idle":"2024-03-14T17:18:25.836236Z","shell.execute_reply.started":"2024-03-14T17:18:25.816105Z","shell.execute_reply":"2024-03-14T17:18:25.835382Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## 定义训练和验证的方法","metadata":{}},{"cell_type":"code","source":"class_labels= { 1: \"target\" }\n@torch.inference_mode()\ndef evaluate(net, dataloader, device, amp, experiment, epoch, test_table, logging = False):\n    net.eval()\n    \n    num_val_batches = len(dataloader)\n    bce_loss = 0\n    dice_loss = 0\n    iou_score = 0\n\n    if isinstance(model, nn.DataParallel):\n        num_classes = net.module.num_classes\n    else:\n        num_classes = net.num_classes\n    \n    # 因为在非训练过程（推理过程中），已经在网络最后一层加了log和过滤\n    # 因此这里的损失函数都要使用不带log的\n    criterion = nn.BCELoss().cuda()\n    diceloss = smp.losses.DiceLoss(mode='binary', from_logits=False).cuda()\n    \n    g_bce_loss = 0\n    g_dice_loss = 0\n    g_iou_score = 0\n            \n    g_accuracy = 0\n    g_precision= 0\n    g_f1_score = 0\n    g_f2_score= 0\n    \n    idx = -1\n    # iterate over the validation set\n    with tqdm(total=num_val_batches, desc='Validation round', unit='batch', position=0 ,leave=True) as pbar:\n        for batch in dataloader:\n            idx += 1\n            \n            images, mask_true = batch\n\n            # move images and labels to correct device and type\n            images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n            mask_true = mask_true.to(device=device, dtype=torch.float32)\n\n            # predict the mask\n            mask_pred = net(images)\n            bce_loss = criterion(mask_pred, mask_true.float())\n            dice_loss = diceloss(mask_pred, mask_true)\n\n            tp, fp, fn, tn = smp.metrics.get_stats(mask_pred, mask_true.long(), mode='binary', threshold=0.5)\n\n            iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n            pbar.update(images.shape[0])\n            \n            f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n            f2_score = smp.metrics.fbeta_score(tp, fp, fn, tn, beta=2, reduction=\"micro\")\n        \n            accuracy = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"macro\")\n            precision = smp.metrics.precision(tp, fp, fn, tn, reduction=\"macro\")\n    \n\n        \n            g_bce_loss += bce_loss\n            g_dice_loss += dice_loss\n        \n            g_iou_score += iou_score\n        \n            g_f1_score += f1_score\n            g_f2_score += f2_score\n            \n            g_accuracy += accuracy\n            g_precision += precision\n            \n            \n            if logging:\n                test_table.add_data(epoch, idx, \n                                    wandb.Image(images[0].float().cpu(),\n                                                masks = { \n                                                    \"predictions\": {\n                                                        \"mask_data\": mask_pred[0][0].cpu().numpy(), \"class_labels\": class_labels\n                                                    },\n                                                    \"ground_truth\": {\n                                                        \"mask_data\": mask_true[0][0].cpu().numpy(), \"class_labels\": class_labels\n                                                    },\n                                    }),\n                                    bce_loss, dice_loss, f1_score,\n                                    iou_score, accuracy, precision)\n\n        g_bce_loss = (g_bce_loss / max(num_val_batches, 1))\n        g_dice_loss = (g_dice_loss / max(num_val_batches, 1))\n        g_iou_score = (g_iou_score / max(num_val_batches, 1))\n        g_accuracy = (g_accuracy / max(num_val_batches, 1))\n        g_precision= (g_precision / max(num_val_batches, 1))\n        g_f1_score = (g_f1_score / max(num_val_batches, 1))\n        g_f2_score= (g_f2_score / max(num_val_batches, 1))\n\n        pbar.set_postfix(**{\"Validation bce loss\": bce_loss.item(), \"dice loss\": dice_loss.item(), \"IoU Score\": iou_score.item()})\n    \n    if logging:\n        try:\n            experiment.log({\n                'ave_validation Loss': g_bce_loss + g_dice_loss,\n                'ave_accuracy': g_accuracy,\n                'ave_precision':g_precision,\n                'ave_f1_score':g_f1_score,\n                'ave_f2_score':g_f2_score,\n                'average validation IoU Score': g_iou_score,\n                'test_predictions': test_table,\n            })\n        except Exception as e:\n            print(e)\n            pass\n    \n    return (dice_loss, iou_score)    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:25.837348Z","iopub.execute_input":"2024-03-14T17:18:25.837580Z","iopub.status.idle":"2024-03-14T17:18:25.859049Z","shell.execute_reply.started":"2024-03-14T17:18:25.837559Z","shell.execute_reply":"2024-03-14T17:18:25.858192Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import time\nimport torch.optim as optim\nimport segmentation_models_pytorch as smp\n\ndef train(model, device, project,\n          epochs: int = 60,\n          learning_rate: float = 1e-5, \n          weight_decay: float = 1e-8,\n          momentum: float = 0.999,\n          batch_size: int = 6,\n          amp: bool = False,\n          gradient_clipping: float = 1.0):\n    n_train = len(train_data)\n    n_val = len(validate_data)\n\n\n    if isinstance(model, nn.DataParallel):\n        num_classes = model.module.num_classes\n        input_channels = model.module.input_channels\n    else:\n        num_classes = model.num_classes\n        input_channels = model.input_channels\n        \n\n    # (Initialize logging)\n    experiment = wandb.init(project=project, resume='allow', anonymous='must', notes='水平和垂直翻转，旋转(-10,10)度，mixcut')\n    experiment.config.update(\n        dict(epochs=epochs, batch_size=batch_size, amp=True)\n    )\n    logging.info(f'''Starting training:\n        Epochs:          {epochs}\n        Batch size:      {batch_size}\n        Learning rate:   {learning_rate}\n        Training size:   {n_train}\n        Validation size: {n_val}\n        Device:          {device.type}\n        Mixed Precision: {amp}\n    ''')\n    \n\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2, eta_min=5e-5)  # goal: maximize Dice scor\n    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n    \n    # 训练过程中，网络最后一层没有添加log，所以要使用带log的损失函数\n    criterion = nn.BCEWithLogitsLoss().cuda()\n    dice_loss = smp.losses.DiceLoss(mode='binary', from_logits=True).cuda()\n\n    global_step = 0\n    columns = [\"epoch\", \"id\", \"image\", \"bceLoss\", \"diceLoss\", \"f1_score\", \"iouScore\", \"accuracy\", \"precision\",]\n    test_table = wandb.Table(columns=columns)\n    # 5. Begin training\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss = 0\n        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='batch') as pbar:\n            for batch in trainloader:\n                images, true_masks = batch\n\n                assert images.shape[1] == input_channels, \\\n                    f'Network has been defined with {input_channels} input channels, ' \\\n                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n                    'the images are loaded correctly.'\n\n                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n                \n                true_masks = true_masks.to(device=device, dtype=torch.long)\n\n                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n                    masks_pred = model(images)\n                    loss = criterion(masks_pred, true_masks.float())\n                    loss += dice_loss(masks_pred, true_masks)\n                    tp, fp, fn, tn = smp.metrics.get_stats(masks_pred, true_masks.long(), mode='binary', threshold=0.5)\n                    iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n    \n                optimizer.zero_grad(set_to_none=True)\n                grad_scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n                grad_scaler.step(optimizer)\n                grad_scaler.update()\n\n                pbar.update(images.shape[0])\n                global_step += 1\n                epoch_loss += loss.item()\n                pbar.set_postfix(**{'loss (batch)': epoch_loss/n_train})\n                \n                if global_step % 10 == 0:\n                    experiment.log({\n                        'learning rate': optimizer.param_groups[0]['lr'],\n                        'train iou': iou_score,\n                        'train loss': loss.item(),\n                        'step': global_step,\n                        'epoch': epoch\n                    })\n                    \n                \n#                 val_score, iou_score = evaluate(model, valloader, device, amp, experiment, epoch, test_table, logging = True)\n\n           # Evaluation round\n                division_step = (n_train // batch_size)\n                if division_step > 0:\n                    if global_step % division_step == 0:\n                        val_score, iou_score = evaluate(model, valloader, device, amp, experiment, epoch, test_table, logging = False)\n                        \n                        model.train()\n                        scheduler.step(val_score)\n        # 每10个 epoch 更新一遍 wandb\n        if epoch % 1 == 0:\n            evaluate(model, valloader, device, amp, experiment, epoch, test_table, logging = True)\n\n    experiment.finish()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:25.860498Z","iopub.execute_input":"2024-03-14T17:18:25.860774Z","iopub.status.idle":"2024-03-14T17:18:25.881661Z","shell.execute_reply.started":"2024-03-14T17:18:25.860750Z","shell.execute_reply":"2024-03-14T17:18:25.880826Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## 添加 MixCut 和 MixUp\n> https://pytorch.org/vision/main/auto_examples/transforms/plot_cutmix_mixup.html#as-part-of-the-collation-function","metadata":{}},{"cell_type":"code","source":"# print(torch.__version__)\n# from torchvision.transforms import v2\n# from torch.utils.data import default_collate\n# NUM_CLASSES = 1\n# cutmix = v2.CutMix(num_classes=NUM_CLASSES)\n# mixup = v2.MixUp(num_classes=NUM_CLASSES)\n# cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n\n# def collate_fn(batch):\n#     return cutmix_or_mixup(*default_collate(batch))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:25.882640Z","iopub.execute_input":"2024-03-14T17:18:25.882889Z","iopub.status.idle":"2024-03-14T17:18:25.895459Z","shell.execute_reply.started":"2024-03-14T17:18:25.882858Z","shell.execute_reply":"2024-03-14T17:18:25.894567Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# 加载数据集","metadata":{}},{"cell_type":"code","source":"size = [512, 512]\ndef train_collate_fn(batch):\n    trans = transforms.Compose([ \n                transforms.RandomHorizontalFlip(),  # 随机水平翻转\n                transforms.RandomVerticalFlip(),    # 随机垂直旋转\n                transforms.RandomRotation(10) ,     # 随机旋转 （-10,10）度\n    ])\n    images = torch.empty(len(batch), 3, size[0], size[1])\n    masks = torch.empty(len(batch),1, size[0], size[1])\n    for i in range(len(batch)):\n        image, mask = batch[i]\n        seed = np.random.randint(2147483647)\n        torch.manual_seed(seed)\n        image = trans(image)\n        torch.manual_seed(seed)\n        mask = trans(mask)\n        images[i] = image\n        masks[i] = mask\n    \n    return images, masks","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:25.896507Z","iopub.execute_input":"2024-03-14T17:18:25.896786Z","iopub.status.idle":"2024-03-14T17:18:25.908095Z","shell.execute_reply.started":"2024-03-14T17:18:25.896764Z","shell.execute_reply":"2024-03-14T17:18:25.907315Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"batch_size=8\ndataset =  APODataSet(img_dir = \"/kaggle/input/dltrack/apo_images\",\n                      mask_dir = \"/kaggle/input/dltrack/apo_masks\",\n                     size = [512, 512], mixCut = True)\n\ntotal = len(dataset)\ntrain_size = int(0.8*total)\nvalidate_size = total - train_size\n\ntrain_data, validate_data = random_split(dataset, [train_size, validate_size])\n\nprint(\"dataset info\\ntotal: {}, train_size: {}, validate_size: {}\".format(total, len(train_data), len(validate_data)))\n\ntrainloader = DataLoader(dataset=train_data,\n                                     batch_size=batch_size, collate_fn = train_collate_fn,\n                                     num_workers=0, \n                                     shuffle=True)\nvalloader = DataLoader(dataset=validate_data,\n                                    batch_size=1, \n                                    num_workers=0,\n                                    shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:25.910543Z","iopub.execute_input":"2024-03-14T17:18:25.910878Z","iopub.status.idle":"2024-03-14T17:18:29.711466Z","shell.execute_reply.started":"2024-03-14T17:18:25.910855Z","shell.execute_reply":"2024-03-14T17:18:29.710560Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"dataset info\ntotal: 572, train_size: 457, validate_size: 115\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 随机显示一张原始图片和其对应的标记图片","metadata":{}},{"cell_type":"code","source":"for i, data in enumerate(trainloader):\n    images, masks = data\n    orig_img = images[0]\n    mask_img = masks[0]\n    break\n    \n# idx = random.randint(0, len(dataset))\n# orig_img, mask_img = dataset[idx]\n\nprint(orig_img.size())\nprint(mask_img.size())\n\n\norig_img = orig_img.cpu().numpy().transpose(1, 2, 0)\nmask_img = mask_img.cpu().numpy().transpose(1, 2, 0)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 12))\n\nax1.imshow(orig_img)\nax1.grid(False)\nax1.axis('off')\nax1.set_title(\"origin_img\")\n\nax2.imshow(mask_img, cmap=\"gray\")\nax2.grid(False)\nax2.axis('off')\nax2.set_title(\"mask_img\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:29.712701Z","iopub.execute_input":"2024-03-14T17:18:29.713016Z","iopub.status.idle":"2024-03-14T17:18:30.662083Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([3, 512, 512])\ntorch.Size([1, 512, 512])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 训练网络","metadata":{}},{"cell_type":"markdown","source":"## 设置wandb账号\n用作统计与数据分析","metadata":{}},{"cell_type":"code","source":"os.environ['WANDB_API_KEY']='d561f1229ba7c4e207ca34042f29a43552a7447e'\n!wandb login","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:30.663191Z","iopub.execute_input":"2024-03-14T17:18:30.663477Z","iopub.status.idle":"2024-03-14T17:18:33.161725Z","shell.execute_reply.started":"2024-03-14T17:18:30.663453Z","shell.execute_reply":"2024-03-14T17:18:33.160613Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtorwayland\u001b[0m (\u001b[33mcupes-wangtao\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}]},{"cell_type":"code","source":"epochs=1000\nif __name__ == '__main__':\n    model = UNet(input_channels=3, num_classes=1, bilinear=False)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    if torch.cuda.device_count() > 1:\n        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n        model = nn.DataParallel(model)\n\n    model = model.to(memory_format=torch.channels_last)\n    model.to(device)\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"模型参数量为：{total_params}\")\n    print(\"其详情为：\")\n    for name,parameters in model.named_parameters():\n        print(name,':',parameters.size())\n    train(model, device, project=\"U-Net\", epochs=epochs, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T17:18:33.163626Z","iopub.execute_input":"2024-03-14T17:18:33.164635Z","iopub.status.idle":"2024-03-14T17:21:29.411900Z","shell.execute_reply.started":"2024-03-14T17:18:33.164579Z","shell.execute_reply":"2024-03-14T17:21:29.410300Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n模型参数量为：31043521\n其详情为：\nmodule.down1.double_conv.double_conv.0.weight : torch.Size([64, 3, 3, 3])\nmodule.down1.double_conv.double_conv.0.bias : torch.Size([64])\nmodule.down1.double_conv.double_conv.1.weight : torch.Size([64])\nmodule.down1.double_conv.double_conv.1.bias : torch.Size([64])\nmodule.down1.double_conv.double_conv.3.weight : torch.Size([64, 64, 3, 3])\nmodule.down1.double_conv.double_conv.3.bias : torch.Size([64])\nmodule.down1.double_conv.double_conv.4.weight : torch.Size([64])\nmodule.down1.double_conv.double_conv.4.bias : torch.Size([64])\nmodule.down2.double_conv.double_conv.0.weight : torch.Size([128, 64, 3, 3])\nmodule.down2.double_conv.double_conv.0.bias : torch.Size([128])\nmodule.down2.double_conv.double_conv.1.weight : torch.Size([128])\nmodule.down2.double_conv.double_conv.1.bias : torch.Size([128])\nmodule.down2.double_conv.double_conv.3.weight : torch.Size([128, 128, 3, 3])\nmodule.down2.double_conv.double_conv.3.bias : torch.Size([128])\nmodule.down2.double_conv.double_conv.4.weight : torch.Size([128])\nmodule.down2.double_conv.double_conv.4.bias : torch.Size([128])\nmodule.down3.double_conv.double_conv.0.weight : torch.Size([256, 128, 3, 3])\nmodule.down3.double_conv.double_conv.0.bias : torch.Size([256])\nmodule.down3.double_conv.double_conv.1.weight : torch.Size([256])\nmodule.down3.double_conv.double_conv.1.bias : torch.Size([256])\nmodule.down3.double_conv.double_conv.3.weight : torch.Size([256, 256, 3, 3])\nmodule.down3.double_conv.double_conv.3.bias : torch.Size([256])\nmodule.down3.double_conv.double_conv.4.weight : torch.Size([256])\nmodule.down3.double_conv.double_conv.4.bias : torch.Size([256])\nmodule.down4.double_conv.double_conv.0.weight : torch.Size([512, 256, 3, 3])\nmodule.down4.double_conv.double_conv.0.bias : torch.Size([512])\nmodule.down4.double_conv.double_conv.1.weight : torch.Size([512])\nmodule.down4.double_conv.double_conv.1.bias : torch.Size([512])\nmodule.down4.double_conv.double_conv.3.weight : torch.Size([512, 512, 3, 3])\nmodule.down4.double_conv.double_conv.3.bias : torch.Size([512])\nmodule.down4.double_conv.double_conv.4.weight : torch.Size([512])\nmodule.down4.double_conv.double_conv.4.bias : torch.Size([512])\nmodule.bottle_conv.double_conv.0.weight : torch.Size([1024, 512, 3, 3])\nmodule.bottle_conv.double_conv.0.bias : torch.Size([1024])\nmodule.bottle_conv.double_conv.1.weight : torch.Size([1024])\nmodule.bottle_conv.double_conv.1.bias : torch.Size([1024])\nmodule.bottle_conv.double_conv.3.weight : torch.Size([1024, 1024, 3, 3])\nmodule.bottle_conv.double_conv.3.bias : torch.Size([1024])\nmodule.bottle_conv.double_conv.4.weight : torch.Size([1024])\nmodule.bottle_conv.double_conv.4.bias : torch.Size([1024])\nmodule.up4.up.weight : torch.Size([1024, 512, 2, 2])\nmodule.up4.up.bias : torch.Size([512])\nmodule.up4.conv.1.double_conv.0.weight : torch.Size([512, 1024, 3, 3])\nmodule.up4.conv.1.double_conv.0.bias : torch.Size([512])\nmodule.up4.conv.1.double_conv.1.weight : torch.Size([512])\nmodule.up4.conv.1.double_conv.1.bias : torch.Size([512])\nmodule.up4.conv.1.double_conv.3.weight : torch.Size([512, 512, 3, 3])\nmodule.up4.conv.1.double_conv.3.bias : torch.Size([512])\nmodule.up4.conv.1.double_conv.4.weight : torch.Size([512])\nmodule.up4.conv.1.double_conv.4.bias : torch.Size([512])\nmodule.up3.up.weight : torch.Size([512, 256, 2, 2])\nmodule.up3.up.bias : torch.Size([256])\nmodule.up3.conv.1.double_conv.0.weight : torch.Size([256, 512, 3, 3])\nmodule.up3.conv.1.double_conv.0.bias : torch.Size([256])\nmodule.up3.conv.1.double_conv.1.weight : torch.Size([256])\nmodule.up3.conv.1.double_conv.1.bias : torch.Size([256])\nmodule.up3.conv.1.double_conv.3.weight : torch.Size([256, 256, 3, 3])\nmodule.up3.conv.1.double_conv.3.bias : torch.Size([256])\nmodule.up3.conv.1.double_conv.4.weight : torch.Size([256])\nmodule.up3.conv.1.double_conv.4.bias : torch.Size([256])\nmodule.up2.up.weight : torch.Size([256, 128, 2, 2])\nmodule.up2.up.bias : torch.Size([128])\nmodule.up2.conv.1.double_conv.0.weight : torch.Size([128, 256, 3, 3])\nmodule.up2.conv.1.double_conv.0.bias : torch.Size([128])\nmodule.up2.conv.1.double_conv.1.weight : torch.Size([128])\nmodule.up2.conv.1.double_conv.1.bias : torch.Size([128])\nmodule.up2.conv.1.double_conv.3.weight : torch.Size([128, 128, 3, 3])\nmodule.up2.conv.1.double_conv.3.bias : torch.Size([128])\nmodule.up2.conv.1.double_conv.4.weight : torch.Size([128])\nmodule.up2.conv.1.double_conv.4.bias : torch.Size([128])\nmodule.up1.up.weight : torch.Size([128, 64, 2, 2])\nmodule.up1.up.bias : torch.Size([64])\nmodule.up1.conv.1.double_conv.0.weight : torch.Size([64, 128, 3, 3])\nmodule.up1.conv.1.double_conv.0.bias : torch.Size([64])\nmodule.up1.conv.1.double_conv.1.weight : torch.Size([64])\nmodule.up1.conv.1.double_conv.1.bias : torch.Size([64])\nmodule.up1.conv.1.double_conv.3.weight : torch.Size([64, 64, 3, 3])\nmodule.up1.conv.1.double_conv.3.bias : torch.Size([64])\nmodule.up1.conv.1.double_conv.4.weight : torch.Size([64])\nmodule.up1.conv.1.double_conv.4.bias : torch.Size([64])\nmodule.outc.weight : torch.Size([1, 64, 1, 1])\nmodule.outc.bias : torch.Size([1])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:2ubqfv97) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">mango-bun-142</strong> at: <a href='https://wandb.ai/cupes-wangtao/U-Net/runs/2ubqfv97' target=\"_blank\">https://wandb.ai/cupes-wangtao/U-Net/runs/2ubqfv97</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240314_171648-2ubqfv97/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:2ubqfv97). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240314_171833-fg59nt2e</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cupes-wangtao/U-Net/runs/fg59nt2e' target=\"_blank\">cherry-bun-143</a></strong> to <a href='https://wandb.ai/cupes-wangtao/U-Net' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cupes-wangtao/U-Net' target=\"_blank\">https://wandb.ai/cupes-wangtao/U-Net</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cupes-wangtao/U-Net/runs/fg59nt2e' target=\"_blank\">https://wandb.ai/cupes-wangtao/U-Net/runs/fg59nt2e</a>"},"metadata":{}},{"name":"stderr","text":"Validation round: 100%|██████████| 115/115 [00:15<00:00,  7.31batch/s, IoU Score=0.154, Validation bce loss=16.7, dice loss=0.732]\nEpoch 1/1000: 100%|██████████| 457/457 [01:48<00:00,  4.21batch/s, loss (batch)=0.198]\nValidation round:   1%|          | 1/115 [00:00<00:14,  8.11batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:   2%|▏         | 2/115 [00:00<00:20,  5.59batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:   3%|▎         | 3/115 [00:00<00:21,  5.20batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:   3%|▎         | 4/115 [00:00<00:22,  4.96batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:   4%|▍         | 5/115 [00:00<00:23,  4.77batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:   5%|▌         | 6/115 [00:01<00:23,  4.71batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:   6%|▌         | 7/115 [00:01<00:22,  4.77batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:   8%|▊         | 9/115 [00:01<00:21,  4.91batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  10%|▉         | 11/115 [00:02<00:21,  4.78batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  10%|█         | 12/115 [00:02<00:21,  4.69batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  11%|█▏        | 13/115 [00:02<00:21,  4.65batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  12%|█▏        | 14/115 [00:02<00:21,  4.73batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  14%|█▍        | 16/115 [00:03<00:20,  4.73batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  16%|█▌        | 18/115 [00:03<00:20,  4.75batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  17%|█▋        | 20/115 [00:04<00:19,  4.77batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  18%|█▊        | 21/115 [00:04<00:20,  4.55batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  20%|██        | 23/115 [00:04<00:19,  4.69batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  21%|██        | 24/115 [00:05<00:19,  4.59batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  22%|██▏       | 25/115 [00:05<00:20,  4.32batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  23%|██▎       | 26/115 [00:05<00:20,  4.40batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  23%|██▎       | 27/115 [00:05<00:20,  4.30batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  24%|██▍       | 28/115 [00:06<00:19,  4.36batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  26%|██▌       | 30/115 [00:06<00:18,  4.57batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  27%|██▋       | 31/115 [00:06<00:18,  4.64batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  29%|██▊       | 33/115 [00:07<00:17,  4.74batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  30%|██▉       | 34/115 [00:07<00:17,  4.60batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  31%|███▏      | 36/115 [00:07<00:16,  4.89batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  32%|███▏      | 37/115 [00:07<00:16,  4.62batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  33%|███▎      | 38/115 [00:08<00:16,  4.71batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  34%|███▍      | 39/115 [00:08<00:16,  4.70batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  35%|███▍      | 40/115 [00:08<00:16,  4.58batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  36%|███▌      | 41/115 [00:08<00:16,  4.57batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  37%|███▋      | 42/115 [00:09<00:16,  4.53batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  37%|███▋      | 43/115 [00:09<00:15,  4.60batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  39%|███▉      | 45/115 [00:09<00:14,  4.76batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  40%|████      | 46/115 [00:09<00:13,  5.01batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  41%|████      | 47/115 [00:10<00:14,  4.85batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  42%|████▏     | 48/115 [00:10<00:13,  4.86batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  43%|████▎     | 50/115 [00:10<00:13,  4.88batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  44%|████▍     | 51/115 [00:10<00:13,  4.70batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  45%|████▌     | 52/115 [00:11<00:13,  4.62batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  46%|████▌     | 53/115 [00:11<00:13,  4.52batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  47%|████▋     | 54/115 [00:11<00:13,  4.50batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  48%|████▊     | 55/115 [00:11<00:13,  4.52batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  49%|████▊     | 56/115 [00:12<00:14,  4.21batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  50%|████▉     | 57/115 [00:12<00:13,  4.33batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  50%|█████     | 58/115 [00:12<00:13,  4.29batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  51%|█████▏    | 59/115 [00:12<00:12,  4.32batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  53%|█████▎    | 61/115 [00:13<00:11,  4.63batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  55%|█████▍    | 63/115 [00:13<00:11,  4.57batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  57%|█████▋    | 65/115 [00:14<00:10,  4.76batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  58%|█████▊    | 67/115 [00:14<00:10,  4.67batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  59%|█████▉    | 68/115 [00:14<00:09,  4.80batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  60%|██████    | 69/115 [00:14<00:09,  4.81batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  61%|██████    | 70/115 [00:15<00:09,  4.70batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  62%|██████▏   | 71/115 [00:15<00:09,  4.64batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  63%|██████▎   | 73/115 [00:15<00:08,  4.77batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  64%|██████▍   | 74/115 [00:15<00:08,  4.69batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  65%|██████▌   | 75/115 [00:16<00:08,  4.76batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  66%|██████▌   | 76/115 [00:16<00:08,  4.51batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  67%|██████▋   | 77/115 [00:16<00:08,  4.54batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  68%|██████▊   | 78/115 [00:16<00:08,  4.48batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  69%|██████▊   | 79/115 [00:17<00:07,  4.59batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  70%|██████▉   | 80/115 [00:17<00:07,  4.44batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  70%|███████   | 81/115 [00:17<00:07,  4.55batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  71%|███████▏  | 82/115 [00:17<00:07,  4.49batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  73%|███████▎  | 84/115 [00:18<00:06,  4.58batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  74%|███████▍  | 85/115 [00:18<00:06,  4.54batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  76%|███████▌  | 87/115 [00:18<00:05,  4.72batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  77%|███████▋  | 88/115 [00:19<00:05,  4.62batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  78%|███████▊  | 90/115 [00:19<00:05,  4.66batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  79%|███████▉  | 91/115 [00:19<00:05,  4.58batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  80%|████████  | 92/115 [00:19<00:04,  4.63batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  81%|████████  | 93/115 [00:20<00:04,  4.70batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  82%|████████▏ | 94/115 [00:20<00:04,  4.63batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  83%|████████▎ | 95/115 [00:20<00:04,  4.69batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  84%|████████▍ | 97/115 [00:20<00:03,  4.64batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  85%|████████▌ | 98/115 [00:21<00:03,  4.60batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  86%|████████▌ | 99/115 [00:21<00:03,  4.42batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  88%|████████▊ | 101/115 [00:21<00:03,  4.61batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  89%|████████▊ | 102/115 [00:22<00:02,  4.68batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  90%|█████████ | 104/115 [00:22<00:02,  4.82batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  92%|█████████▏| 106/115 [00:22<00:01,  4.82batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  93%|█████████▎| 107/115 [00:23<00:01,  4.74batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  94%|█████████▍| 108/115 [00:23<00:01,  4.71batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  96%|█████████▌| 110/115 [00:23<00:01,  4.76batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  97%|█████████▋| 111/115 [00:23<00:00,  4.91batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  97%|█████████▋| 112/115 [00:24<00:00,  4.76batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  98%|█████████▊| 113/115 [00:24<00:00,  4.62batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round:  99%|█████████▉| 114/115 [00:24<00:00,  4.53batch/s]","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Validation round: 100%|██████████| 115/115 [00:24<00:00,  4.62batch/s, IoU Score=0.157, Validation bce loss=16.6, dice loss=0.729]\n","output_type":"stream"},{"name":"stdout","text":"(1, 512, 512)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/1000:   4%|▎         | 16/457 [00:04<01:50,  3.99batch/s, loss (batch)=0.00656]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name,parameters \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m,parameters\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mU-Net\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[26], line 74\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, project, epochs, learning_rate, weight_decay, momentum, batch_size, amp, gradient_clipping)\u001b[0m\n\u001b[1;32m     72\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dice_loss(masks_pred, true_masks)\n\u001b[1;32m     73\u001b[0m     tp, fp, fn, tn \u001b[38;5;241m=\u001b[39m smp\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mget_stats(masks_pred, true_masks\u001b[38;5;241m.\u001b[39mlong(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m     iou_score \u001b[38;5;241m=\u001b[39m \u001b[43msmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miou_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     77\u001b[0m grad_scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/metrics/functional.py:427\u001b[0m, in \u001b[0;36miou_score\u001b[0;34m(tp, fp, fn, tn, reduction, class_weights, zero_division)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miou_score\u001b[39m(\n\u001b[1;32m    418\u001b[0m     tp: torch\u001b[38;5;241m.\u001b[39mLongTensor,\n\u001b[1;32m    419\u001b[0m     fp: torch\u001b[38;5;241m.\u001b[39mLongTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m     zero_division: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m    425\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    426\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"IoU score or Jaccard index\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compute_metric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_iou_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/metrics/functional.py:253\u001b[0m, in \u001b[0;36m_compute_metric\u001b[0;34m(metric_fn, tp, fp, fn, tn, reduction, class_weights, zero_division, **metric_kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass weights should be provided for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` reduction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m class_weights \u001b[38;5;28;01mif\u001b[39;00m class_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m--> 253\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m class_weights \u001b[38;5;241m/\u001b[39m class_weights\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# 推理","metadata":{}}]}