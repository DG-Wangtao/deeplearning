{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TransUNet\n\nhttps://github.com/Beckschen/TransUNet","metadata":{}},{"cell_type":"markdown","source":"## 准备依赖环境","metadata":{}},{"cell_type":"markdown","source":"## 准备数据集\n参考 https://github.com/Beckschen/TransUNet/tree/main#2-prepare-data\n- transunet-synapse\n- transunet-lists\n","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision numpy scipy tqdm tensorboard tensorboardX ml-collections medpy SimpleITK h5py","metadata":{"execution":{"iopub.status.busy":"2023-09-07T18:25:45.850056Z","iopub.execute_input":"2023-09-07T18:25:45.850770Z","iopub.status.idle":"2023-09-07T18:26:11.812079Z","shell.execute_reply.started":"2023-09-07T18:25:45.850733Z","shell.execute_reply":"2023-09-07T18:26:11.810937Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.23.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.12.3)\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (2.6)\nCollecting ml-collections\n  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting medpy\n  Downloading MedPy-0.4.0.tar.gz (151 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: SimpleITK in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (3.9.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.4.3)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (68.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.3.7)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.40.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (21.3)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from ml-collections) (6.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from ml-collections) (1.16.0)\nCollecting contextlib2 (from ml-collections)\n  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorboardX) (3.0.9)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\nBuilding wheels for collected packages: ml-collections, medpy\n  Building wheel for ml-collections (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=fb08286a7a984544c0dbbb4acd6f46d954370be30ba17694c9f10993fcf52158\n  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n  Building wheel for medpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for medpy: filename=MedPy-0.4.0-py3-none-any.whl size=214946 sha256=12342a1ddfbf6b3f47f08f36d55af433cd54c3ee7943a5b84916d7a09aaec90c\n  Stored in directory: /root/.cache/pip/wheels/d4/32/c7/6380ab2edb8cca018d39a0f1d43250fd9791922c963117de46\nSuccessfully built ml-collections medpy\nInstalling collected packages: contextlib2, ml-collections, medpy\nSuccessfully installed contextlib2-21.6.0 medpy-0.4.0 ml-collections-0.1.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 下载Google imagenet 21预训练模型\n\n关于ViT预训练模型简介：\n- https://github.com/google-research/vision_transformer\n- https://zhuanlan.zhihu.com/p/445122996","metadata":{}},{"cell_type":"code","source":"# !gsutil -m cp \\\n#   \"gs://vit_models/imagenet21k/R26+ViT-B_32.npz\" \\\n#   \"gs://vit_models/imagenet21k/R50+ViT-B_16.npz\" \\\n#   \"gs://vit_models/imagenet21k/R50+ViT-L_32.npz\" \\\n#   \"gs://vit_models/imagenet21k/ViT-B_16.npz\" \\\n#   \"gs://vit_models/imagenet21k/ViT-B_32.npz\" \\\n#   \"gs://vit_models/imagenet21k/ViT-B_8.npz\" \\\n#   \"gs://vit_models/imagenet21k/ViT-H_14.npz\" \\\n#   \"gs://vit_models/imagenet21k/ViT-L_16.npz\" \\\n#   \"gs://vit_models/imagenet21k/ViT-L_32.npz\" \\\n#   .\n\n# 下载一次就够了","metadata":{"execution":{"iopub.status.busy":"2023-09-07T18:26:11.814682Z","iopub.execute_input":"2023-09-07T18:26:11.815060Z","iopub.status.idle":"2023-09-07T18:28:22.754723Z","shell.execute_reply.started":"2023-09-07T18:26:11.815025Z","shell.execute_reply":"2023-09-07T18:28:22.753561Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Copying gs://vit_models/imagenet21k/R50+ViT-B_16.npz...\nCopying gs://vit_models/imagenet21k/R26+ViT-B_32.npz...                         \nCopying gs://vit_models/imagenet21k/R50+ViT-L_32.npz...                         \nCopying gs://vit_models/imagenet21k/ViT-B_16.npz...                             \nCopying gs://vit_models/imagenet21k/ViT-B_32.npz...                             \nCopying gs://vit_models/imagenet21k/ViT-B_8.npz...                              \nCopying gs://vit_models/imagenet21k/ViT-H_14.npz...                             \nCopying gs://vit_models/imagenet21k/ViT-L_16.npz...                             \nCopying gs://vit_models/imagenet21k/ViT-L_32.npz...                             \n| [9/9 files][  8.2 GiB/  8.2 GiB] 100% Done  14.2 MiB/s ETA 00:00:00           \nOperation completed over 9 objects/8.2 GiB.                                      \n","output_type":"stream"}]},{"cell_type":"markdown","source":"### dataset.py\n数据集处理工具，实现pytorch的DataSet类，用于加载训练集、标签和测试数据","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport h5py\nimport numpy as np\nimport torch\nfrom scipy import ndimage\nfrom scipy.ndimage.interpolation import zoom\nfrom torch.utils.data import Dataset\n\n\ndef random_rot_flip(image, label):\n    k = np.random.randint(0, 4)\n    image = np.rot90(image, k)\n    label = np.rot90(label, k)\n    axis = np.random.randint(0, 2)\n    image = np.flip(image, axis=axis).copy()\n    label = np.flip(label, axis=axis).copy()\n    return image, label\n\n\ndef random_rotate(image, label):\n    angle = np.random.randint(-20, 20)\n    image = ndimage.rotate(image, angle, order=0, reshape=False)\n    label = ndimage.rotate(label, angle, order=0, reshape=False)\n    return image, label\n\n\nclass RandomGenerator(object):\n    def __init__(self, output_size):\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['label']\n\n        if random.random() > 0.5:\n            image, label = random_rot_flip(image, label)\n        elif random.random() > 0.5:\n            image, label = random_rotate(image, label)\n        x, y = image.shape\n        if x != self.output_size[0] or y != self.output_size[1]:\n            image = zoom(image, (self.output_size[0] / x, self.output_size[1] / y), order=3)  # why not 3?\n            label = zoom(label, (self.output_size[0] / x, self.output_size[1] / y), order=0)\n        image = torch.from_numpy(image.astype(np.float32)).unsqueeze(0)\n        label = torch.from_numpy(label.astype(np.float32))\n        sample = {'image': image, 'label': label.long()}\n        return sample\n\n\nclass Synapse_dataset(Dataset):\n    def __init__(self, base_dir, list_dir, split, transform=None):\n        self.transform = transform  # using transform in torch!\n        self.split = split\n        self.sample_list = open(os.path.join(list_dir, self.split+'.txt')).readlines()\n        self.data_dir = base_dir\n\n    def __len__(self):\n        return len(self.sample_list)\n\n    def __getitem__(self, idx):\n        if self.split == \"train\":\n            slice_name = self.sample_list[idx].strip('\\n')\n            data_path = os.path.join(self.data_dir, slice_name+'.npz')\n            data = np.load(data_path)\n            image, label = data['image'], data['label']\n        else:\n            vol_name = self.sample_list[idx].strip('\\n')\n            filepath = self.data_dir + \"/{}.npy.h5\".format(vol_name)\n            data = h5py.File(filepath)\n            image, label = data['image'][:], data['label'][:]\n\n        sample = {'image': image, 'label': label}\n        if self.transform:\n            sample = self.transform(sample)\n        sample['case_name'] = self.sample_list[idx].strip('\\n')\n        return sample","metadata":{"execution":{"iopub.status.busy":"2023-09-07T18:28:22.756427Z","iopub.execute_input":"2023-09-07T18:28:22.756817Z","iopub.status.idle":"2023-09-07T18:28:26.297635Z","shell.execute_reply.started":"2023-09-07T18:28:22.756781Z","shell.execute_reply":"2023-09-07T18:28:26.294280Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### utils.py\n\n一些工具方法\n- DiceLoss 损失函数","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom medpy import metric\nfrom scipy.ndimage import zoom\nimport torch.nn as nn\nimport SimpleITK as sitk\n\n\nclass DiceLoss(nn.Module):\n    def __init__(self, n_classes):\n        super(DiceLoss, self).__init__()\n        self.n_classes = n_classes\n\n    def _one_hot_encoder(self, input_tensor):\n        tensor_list = []\n        for i in range(self.n_classes):\n            temp_prob = input_tensor == i  # * torch.ones_like(input_tensor)\n            tensor_list.append(temp_prob.unsqueeze(1))\n        output_tensor = torch.cat(tensor_list, dim=1)\n        return output_tensor.float()\n\n    def _dice_loss(self, score, target):\n        target = target.float()\n        smooth = 1e-5\n        intersect = torch.sum(score * target)\n        y_sum = torch.sum(target * target)\n        z_sum = torch.sum(score * score)\n        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n        loss = 1 - loss\n        return loss\n\n    def forward(self, inputs, target, weight=None, softmax=False):\n        if softmax:\n            inputs = torch.softmax(inputs, dim=1)\n        target = self._one_hot_encoder(target)\n        if weight is None:\n            weight = [1] * self.n_classes\n        assert inputs.size() == target.size(), 'predict {} & target {} shape do not match'.format(inputs.size(), target.size())\n        class_wise_dice = []\n        loss = 0.0\n        for i in range(0, self.n_classes):\n            dice = self._dice_loss(inputs[:, i], target[:, i])\n            class_wise_dice.append(1.0 - dice.item())\n            loss += dice * weight[i]\n        return loss / self.n_classes\n\n\ndef calculate_metric_percase(pred, gt):\n    pred[pred > 0] = 1\n    gt[gt > 0] = 1\n    if pred.sum() > 0 and gt.sum()>0:\n        dice = metric.binary.dc(pred, gt)\n        hd95 = metric.binary.hd95(pred, gt)\n        return dice, hd95\n    elif pred.sum() > 0 and gt.sum()==0:\n        return 1, 0\n    else:\n        return 0, 0\n\n\ndef test_single_volume(image, label, net, classes, patch_size=[256, 256], test_save_path=None, case=None, z_spacing=1):\n    image, label = image.squeeze(0).cpu().detach().numpy(), label.squeeze(0).cpu().detach().numpy()\n    if len(image.shape) == 3:\n        prediction = np.zeros_like(label)\n        for ind in range(image.shape[0]):\n            slice = image[ind, :, :]\n            x, y = slice.shape[0], slice.shape[1]\n            if x != patch_size[0] or y != patch_size[1]:\n                slice = zoom(slice, (patch_size[0] / x, patch_size[1] / y), order=3)  # previous using 0\n            input = torch.from_numpy(slice).unsqueeze(0).unsqueeze(0).float().cuda()\n            net.eval()\n            with torch.no_grad():\n                outputs = net(input)\n                out = torch.argmax(torch.softmax(outputs, dim=1), dim=1).squeeze(0)\n                out = out.cpu().detach().numpy()\n                if x != patch_size[0] or y != patch_size[1]:\n                    pred = zoom(out, (x / patch_size[0], y / patch_size[1]), order=0)\n                else:\n                    pred = out\n                prediction[ind] = pred\n    else:\n        input = torch.from_numpy(image).unsqueeze(\n            0).unsqueeze(0).float().cuda()\n        net.eval()\n        with torch.no_grad():\n            out = torch.argmax(torch.softmax(net(input), dim=1), dim=1).squeeze(0)\n            prediction = out.cpu().detach().numpy()\n    metric_list = []\n    for i in range(1, classes):\n        metric_list.append(calculate_metric_percase(prediction == i, label == i))\n\n    if test_save_path is not None:\n        img_itk = sitk.GetImageFromArray(image.astype(np.float32))\n        prd_itk = sitk.GetImageFromArray(prediction.astype(np.float32))\n        lab_itk = sitk.GetImageFromArray(label.astype(np.float32))\n        img_itk.SetSpacing((1, 1, z_spacing))\n        prd_itk.SetSpacing((1, 1, z_spacing))\n        lab_itk.SetSpacing((1, 1, z_spacing))\n        sitk.WriteImage(prd_itk, test_save_path + '/'+case + \"_pred.nii.gz\")\n        sitk.WriteImage(img_itk, test_save_path + '/'+ case + \"_img.nii.gz\")\n        sitk.WriteImage(lab_itk, test_save_path + '/'+ case + \"_gt.nii.gz\")\n    return metric_list","metadata":{"execution":{"iopub.status.busy":"2023-09-07T18:28:26.300633Z","iopub.execute_input":"2023-09-07T18:28:26.301258Z","iopub.status.idle":"2023-09-07T18:28:27.137372Z","shell.execute_reply.started":"2023-09-07T18:28:26.301204Z","shell.execute_reply":"2023-09-07T18:28:27.136360Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## networks\n获取预训练模型的配置，用于在训练之前预先加载该模型","metadata":{}},{"cell_type":"markdown","source":"### vit_seg_configs.py","metadata":{}},{"cell_type":"code","source":"import ml_collections\n\ndef get_b16_config():\n    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 768\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 3072\n    config.transformer.num_heads = 12\n    config.transformer.num_layers = 12\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n\n    config.classifier = 'seg'\n    config.representation_size = None\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '/kaggle/working/ViT-B_16.npz'\n    config.patch_size = 16\n\n    config.decoder_channels = (256, 128, 64, 16)\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_testing():\n    \"\"\"Returns a minimal configuration for testing.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 1\n    config.transformer.num_heads = 1\n    config.transformer.num_layers = 1\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config\n\ndef get_r50_b16_config():\n    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.pretrained_path = '/kaggle/working/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 2\n    config.n_skip = 3\n    config.activation = 'softmax'\n\n    return config\n\n\ndef get_b32_config():\n    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.size = (32, 32)\n    config.pretrained_path = '/kaggle/working/ViT-B_32.npz'\n    return config\n\n\ndef get_l16_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1024\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 4096\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 24\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.representation_size = None\n\n    # custom\n    config.classifier = 'seg'\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '/kaggle/working/ViT-L_16.npz_.gstmp'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_r50_l16_config():\n    \"\"\"Returns the Resnet50 + ViT-L/16 configuration. customized \"\"\"\n    config = get_l16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.resnet_pretrained_path = '/kaggle/working/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_l32_config():\n    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n    config = get_l16_config()\n    config.patches.size = (32, 32)\n    return config\n\n\ndef get_h14_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n    config.hidden_size = 1280\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 5120\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 32\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n\n    return config","metadata":{"execution":{"iopub.status.busy":"2023-09-07T18:28:27.138929Z","iopub.execute_input":"2023-09-07T18:28:27.139271Z","iopub.status.idle":"2023-09-07T18:28:27.215213Z","shell.execute_reply.started":"2023-09-07T18:28:27.139238Z","shell.execute_reply":"2023-09-07T18:28:27.214248Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### vit_seg_modeling.py\n\n网络模型定义","metadata":{}},{"cell_type":"code","source":"# coding=utf-8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\nimport math\n\nfrom os.path import join as pjoin\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nfrom torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\nfrom torch.nn.modules.utils import _pair\nfrom scipy import ndimage\n# from . import vit_seg_configs as configs\n# from .vit_seg_modeling_resnet_skip import ResNetV2\n\n\nlogger = logging.getLogger(__name__)\n\n\nATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\nATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\nATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\nATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\nFC_0 = \"MlpBlock_3/Dense_0\"\nFC_1 = \"MlpBlock_3/Dense_1\"\nATTENTION_NORM = \"LayerNorm_0\"\nMLP_NORM = \"LayerNorm_2\"\n\n\ndef np2th(weights, conv=False):\n    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n    if conv:\n        weights = weights.transpose([3, 2, 0, 1])\n    return torch.from_numpy(weights)\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n\n\nclass Attention(nn.Module):\n    def __init__(self, config, vis):\n        super(Attention, self).__init__()\n        self.vis = vis\n        self.num_attention_heads = config.transformer[\"num_heads\"]\n        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = Linear(config.hidden_size, self.all_head_size)\n        self.key = Linear(config.hidden_size, self.all_head_size)\n        self.value = Linear(config.hidden_size, self.all_head_size)\n\n        self.out = Linear(config.hidden_size, config.hidden_size)\n        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n\n        self.softmax = Softmax(dim=-1)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        attention_probs = self.softmax(attention_scores)\n        weights = attention_probs if self.vis else None\n        attention_probs = self.attn_dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        attention_output = self.out(context_layer)\n        attention_output = self.proj_dropout(attention_output)\n        return attention_output, weights\n\n\nclass Mlp(nn.Module):\n    def __init__(self, config):\n        super(Mlp, self).__init__()\n        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n        self.act_fn = ACT2FN[\"gelu\"]\n        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n\n        self._init_weights()\n\n    def _init_weights(self):\n        nn.init.xavier_uniform_(self.fc1.weight)\n        nn.init.xavier_uniform_(self.fc2.weight)\n        nn.init.normal_(self.fc1.bias, std=1e-6)\n        nn.init.normal_(self.fc2.bias, std=1e-6)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act_fn(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Embeddings(nn.Module):\n    \"\"\"Construct the embeddings from patch, position embeddings.\n    \"\"\"\n    def __init__(self, config, img_size, in_channels=3):\n        super(Embeddings, self).__init__()\n        self.hybrid = None\n        self.config = config\n        img_size = _pair(img_size)\n\n        if config.patches.get(\"grid\") is not None:   # ResNet\n            grid_size = config.patches[\"grid\"]\n            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n            patch_size_real = (patch_size[0] * 16, patch_size[1] * 16)\n            n_patches = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1])  \n            self.hybrid = True\n        else:\n            patch_size = _pair(config.patches[\"size\"])\n            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n            self.hybrid = False\n\n        if self.hybrid:\n            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers, width_factor=config.resnet.width_factor)\n            in_channels = self.hybrid_model.width * 16\n        self.patch_embeddings = Conv2d(in_channels=in_channels,\n                                       out_channels=config.hidden_size,\n                                       kernel_size=patch_size,\n                                       stride=patch_size)\n        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, config.hidden_size))\n\n        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n\n\n    def forward(self, x):\n        if self.hybrid:\n            x, features = self.hybrid_model(x)\n        else:\n            features = None\n        x = self.patch_embeddings(x)  # (B, hidden. n_patches^(1/2), n_patches^(1/2))\n        x = x.flatten(2)\n        x = x.transpose(-1, -2)  # (B, n_patches, hidden)\n\n        embeddings = x + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n        return embeddings, features\n\n\nclass Block(nn.Module):\n    def __init__(self, config, vis):\n        super(Block, self).__init__()\n        self.hidden_size = config.hidden_size\n        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        self.ffn = Mlp(config)\n        self.attn = Attention(config, vis)\n\n    def forward(self, x):\n        h = x\n        x = self.attention_norm(x)\n        x, weights = self.attn(x)\n        x = x + h\n\n        h = x\n        x = self.ffn_norm(x)\n        x = self.ffn(x)\n        x = x + h\n        return x, weights\n\n    def load_from(self, weights, n_block):\n        ROOT = f\"Transformer/encoderblock_{n_block}\"\n        with torch.no_grad():\n            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n\n            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n\n            self.attn.query.weight.copy_(query_weight)\n            self.attn.key.weight.copy_(key_weight)\n            self.attn.value.weight.copy_(value_weight)\n            self.attn.out.weight.copy_(out_weight)\n            self.attn.query.bias.copy_(query_bias)\n            self.attn.key.bias.copy_(key_bias)\n            self.attn.value.bias.copy_(value_bias)\n            self.attn.out.bias.copy_(out_bias)\n\n            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n\n            self.ffn.fc1.weight.copy_(mlp_weight_0)\n            self.ffn.fc2.weight.copy_(mlp_weight_1)\n            self.ffn.fc1.bias.copy_(mlp_bias_0)\n            self.ffn.fc2.bias.copy_(mlp_bias_1)\n\n            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n\n\nclass Encoder(nn.Module):\n    def __init__(self, config, vis):\n        super(Encoder, self).__init__()\n        self.vis = vis\n        self.layer = nn.ModuleList()\n        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        for _ in range(config.transformer[\"num_layers\"]):\n            layer = Block(config, vis)\n            self.layer.append(copy.deepcopy(layer))\n\n    def forward(self, hidden_states):\n        attn_weights = []\n        for layer_block in self.layer:\n            hidden_states, weights = layer_block(hidden_states)\n            if self.vis:\n                attn_weights.append(weights)\n        encoded = self.encoder_norm(hidden_states)\n        return encoded, attn_weights\n\n\nclass Transformer(nn.Module):\n    def __init__(self, config, img_size, vis):\n        super(Transformer, self).__init__()\n        self.embeddings = Embeddings(config, img_size=img_size)\n        self.encoder = Encoder(config, vis)\n\n    def forward(self, input_ids):\n        embedding_output, features = self.embeddings(input_ids)\n        encoded, attn_weights = self.encoder(embedding_output)  # (B, n_patch, hidden)\n        return encoded, attn_weights, features\n\n\nclass Conv2dReLU(nn.Sequential):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding=0,\n            stride=1,\n            use_batchnorm=True,\n    ):\n        conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=not (use_batchnorm),\n        )\n        relu = nn.ReLU(inplace=True)\n\n        bn = nn.BatchNorm2d(out_channels)\n\n        super(Conv2dReLU, self).__init__(conv, bn, relu)\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            skip_channels=0,\n            use_batchnorm=True,\n    ):\n        super().__init__()\n        self.conv1 = Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.conv2 = Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n    def forward(self, x, skip=None):\n        x = self.up(x)\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\n\nclass SegmentationHead(nn.Sequential):\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n        super().__init__(conv2d, upsampling)\n\n\nclass DecoderCup(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        head_channels = 512\n        self.conv_more = Conv2dReLU(\n            config.hidden_size,\n            head_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=True,\n        )\n        decoder_channels = config.decoder_channels\n        in_channels = [head_channels] + list(decoder_channels[:-1])\n        out_channels = decoder_channels\n\n        if self.config.n_skip != 0:\n            skip_channels = self.config.skip_channels\n            for i in range(4-self.config.n_skip):  # re-select the skip channels according to n_skip\n                skip_channels[3-i]=0\n\n        else:\n            skip_channels=[0,0,0,0]\n\n        blocks = [\n            DecoderBlock(in_ch, out_ch, sk_ch) for in_ch, out_ch, sk_ch in zip(in_channels, out_channels, skip_channels)\n        ]\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, hidden_states, features=None):\n        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n        x = hidden_states.permute(0, 2, 1)\n        x = x.contiguous().view(B, hidden, h, w)\n        x = self.conv_more(x)\n        for i, decoder_block in enumerate(self.blocks):\n            if features is not None:\n                skip = features[i] if (i < self.config.n_skip) else None\n            else:\n                skip = None\n            x = decoder_block(x, skip=skip)\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n        super(VisionTransformer, self).__init__()\n        self.num_classes = num_classes\n        self.zero_head = zero_head\n        self.classifier = config.classifier\n        self.transformer = Transformer(config, img_size, vis)\n        self.decoder = DecoderCup(config)\n        self.segmentation_head = SegmentationHead(\n            in_channels=config['decoder_channels'][-1],\n            out_channels=config['n_classes'],\n            kernel_size=3,\n        )\n        self.config = config\n\n    def forward(self, x):\n        if x.size()[1] == 1:\n            x = x.repeat(1,3,1,1)\n        x, attn_weights, features = self.transformer(x)  # (B, n_patch, hidden)\n        x = self.decoder(x, features)\n        logits = self.segmentation_head(x)\n        return logits\n\n    def load_from(self, weights):\n        with torch.no_grad():\n\n            res_weight = weights\n            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n\n            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n\n            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n\n            posemb_new = self.transformer.embeddings.position_embeddings\n            if posemb.size() == posemb_new.size():\n                self.transformer.embeddings.position_embeddings.copy_(posemb)\n            elif posemb.size()[1]-1 == posemb_new.size()[1]:\n                posemb = posemb[:, 1:]\n                self.transformer.embeddings.position_embeddings.copy_(posemb)\n            else:\n                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n                ntok_new = posemb_new.size(1)\n                if self.classifier == \"seg\":\n                    _, posemb_grid = posemb[:, :1], posemb[0, 1:]\n                gs_old = int(np.sqrt(len(posemb_grid)))\n                gs_new = int(np.sqrt(ntok_new))\n                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)  # th2np\n                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n                posemb = posemb_grid\n                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n\n            # Encoder whole\n            for bname, block in self.transformer.encoder.named_children():\n                for uname, unit in block.named_children():\n                    unit.load_from(weights, n_block=uname)\n\n            if self.transformer.embeddings.hybrid:\n                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(res_weight[\"conv_root/kernel\"], conv=True))\n                gn_weight = np2th(res_weight[\"gn_root/scale\"]).view(-1)\n                gn_bias = np2th(res_weight[\"gn_root/bias\"]).view(-1)\n                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n\n                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n                    for uname, unit in block.named_children():\n                        unit.load_from(res_weight, n_block=bname, n_unit=uname)\n\nCONFIGS = {\n    'ViT-B_16': get_b16_config(),\n    'ViT-B_32': get_b32_config(),\n    'ViT-L_16': get_l16_config(),\n    'ViT-L_32': get_l32_config(),\n    'ViT-H_14': get_h14_config(),\n    'R50-ViT-B_16': get_r50_b16_config(),\n    'R50-ViT-L_16': get_r50_l16_config(),\n    'testing': get_testing(),\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-07T18:28:27.217010Z","iopub.execute_input":"2023-09-07T18:28:27.217361Z","iopub.status.idle":"2023-09-07T18:28:27.304187Z","shell.execute_reply.started":"2023-09-07T18:28:27.217329Z","shell.execute_reply":"2023-09-07T18:28:27.303195Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### vit_seg_modeling_resnet_skip.py\n\n残差网络ResNet模型","metadata":{}},{"cell_type":"code","source":"import math\n\nfrom os.path import join as pjoin\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef np2th(weights, conv=False):\n    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n    if conv:\n        weights = weights.transpose([3, 2, 0, 1])\n    return torch.from_numpy(weights)\n\n\nclass StdConv2d(nn.Conv2d):\n\n    def forward(self, x):\n        w = self.weight\n        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n        w = (w - m) / torch.sqrt(v + 1e-5)\n        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n                        self.dilation, self.groups)\n\n\ndef conv3x3(cin, cout, stride=1, groups=1, bias=False):\n    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n                     padding=1, bias=bias, groups=groups)\n\n\ndef conv1x1(cin, cout, stride=1, bias=False):\n    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n                     padding=0, bias=bias)\n\n\nclass PreActBottleneck(nn.Module):\n    \"\"\"Pre-activation (v2) bottleneck block.\n    \"\"\"\n\n    def __init__(self, cin, cout=None, cmid=None, stride=1):\n        super().__init__()\n        cout = cout or cin\n        cmid = cmid or cout//4\n\n        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n        self.conv1 = conv1x1(cin, cmid, bias=False)\n        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n        self.conv3 = conv1x1(cmid, cout, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n\n        if (stride != 1 or cin != cout):\n            # Projection also with pre-activation according to paper.\n            self.downsample = conv1x1(cin, cout, stride, bias=False)\n            self.gn_proj = nn.GroupNorm(cout, cout)\n\n    def forward(self, x):\n\n        # Residual branch\n        residual = x\n        if hasattr(self, 'downsample'):\n            residual = self.downsample(x)\n            residual = self.gn_proj(residual)\n\n        # Unit's branch\n        y = self.relu(self.gn1(self.conv1(x)))\n        y = self.relu(self.gn2(self.conv2(y)))\n        y = self.gn3(self.conv3(y))\n\n        y = self.relu(residual + y)\n        return y\n\n    def load_from(self, weights, n_block, n_unit):\n        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n\n        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n\n        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n\n        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n\n        self.conv1.weight.copy_(conv1_weight)\n        self.conv2.weight.copy_(conv2_weight)\n        self.conv3.weight.copy_(conv3_weight)\n\n        self.gn1.weight.copy_(gn1_weight.view(-1))\n        self.gn1.bias.copy_(gn1_bias.view(-1))\n\n        self.gn2.weight.copy_(gn2_weight.view(-1))\n        self.gn2.bias.copy_(gn2_bias.view(-1))\n\n        self.gn3.weight.copy_(gn3_weight.view(-1))\n        self.gn3.bias.copy_(gn3_bias.view(-1))\n\n        if hasattr(self, 'downsample'):\n            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n\n            self.downsample.weight.copy_(proj_conv_weight)\n            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))\n\nclass ResNetV2(nn.Module):\n    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n\n    def __init__(self, block_units, width_factor):\n        super().__init__()\n        width = int(64 * width_factor)\n        self.width = width\n\n        self.root = nn.Sequential(OrderedDict([\n            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n            ('relu', nn.ReLU(inplace=True)),\n            # ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n        ]))\n\n        self.body = nn.Sequential(OrderedDict([\n            ('block1', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n                ))),\n            ('block2', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n                ))),\n            ('block3', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n                ))),\n        ]))\n\n    def forward(self, x):\n        features = []\n        b, c, in_size, _ = x.size()\n        x = self.root(x)\n        features.append(x)\n        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)(x)\n        for i in range(len(self.body)-1):\n            x = self.body[i](x)\n            right_size = int(in_size / 4 / (i+1))\n            if x.size()[2] != right_size:\n                pad = right_size - x.size()[2]\n                assert pad < 3 and pad > 0, \"x {} should {}\".format(x.size(), right_size)\n                feat = torch.zeros((b, x.size()[1], right_size, right_size), device=x.device)\n                feat[:, :, 0:x.size()[2], 0:x.size()[3]] = x[:]\n            else:\n                feat = x\n            features.append(feat)\n        x = self.body[-1](x)\n        return x, features[::-1]","metadata":{"execution":{"iopub.status.busy":"2023-09-07T18:28:27.305818Z","iopub.execute_input":"2023-09-07T18:28:27.306259Z","iopub.status.idle":"2023-09-07T18:28:27.344095Z","shell.execute_reply.started":"2023-09-07T18:28:27.306223Z","shell.execute_reply":"2023-09-07T18:28:27.343032Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## train","metadata":{}},{"cell_type":"markdown","source":"### trianer.py\n\n用函数定义训练过程","metadata":{}},{"cell_type":"code","source":"import argparse\nimport logging\nimport os\nimport random\nimport sys\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tensorboardX import SummaryWriter\nfrom torch.nn.modules.loss import CrossEntropyLoss\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n# from utils import DiceLoss\nfrom torchvision import transforms\n\ndef trainer_synapse(args, model, snapshot_path):\n#     from datasets.dataset_synapse import Synapse_dataset, RandomGenerator\n    logging.basicConfig(filename=snapshot_path + \"/log.txt\", level=logging.INFO,\n                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.info(str(args))\n    base_lr = args.base_lr\n    num_classes = args.num_classes\n    batch_size = args.batch_size * args.n_gpu\n    # max_iterations = args.max_iterations\n    db_train = Synapse_dataset(base_dir=args.root_path, list_dir=args.list_dir, split=\"train\",\n                               transform=transforms.Compose(\n                                   [RandomGenerator(output_size=[args.img_size, args.img_size])]))\n    print(\"The length of train set is: {}\".format(len(db_train)))\n\n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n\n    trainloader = DataLoader(db_train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True,\n                             worker_init_fn=worker_init_fn)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    model.train()\n    ce_loss = CrossEntropyLoss()\n    dice_loss = DiceLoss(num_classes)\n    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n    writer = SummaryWriter(snapshot_path + '/log')\n    iter_num = 0\n    max_epoch = args.max_epochs\n    max_iterations = args.max_epochs * len(trainloader)  # max_epoch = max_iterations // len(trainloader) + 1\n    logging.info(\"{} iterations per epoch. {} max iterations \".format(len(trainloader), max_iterations))\n    best_performance = 0.0\n    iterator = tqdm(range(max_epoch), ncols=70)\n    for epoch_num in iterator:\n        for i_batch, sampled_batch in enumerate(trainloader):\n            image_batch, label_batch = sampled_batch['image'], sampled_batch['label']\n            image_batch, label_batch = image_batch.cuda(), label_batch.cuda()\n            outputs = model(image_batch)\n            loss_ce = ce_loss(outputs, label_batch[:].long())\n            loss_dice = dice_loss(outputs, label_batch, softmax=True)\n            loss = 0.5 * loss_ce + 0.5 * loss_dice\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            lr_ = base_lr * (1.0 - iter_num / max_iterations) ** 0.9\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr_\n\n            iter_num = iter_num + 1\n            writer.add_scalar('info/lr', lr_, iter_num)\n            writer.add_scalar('info/total_loss', loss, iter_num)\n            writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n\n            logging.info('iteration %d : loss : %f, loss_ce: %f' % (iter_num, loss.item(), loss_ce.item()))\n\n            if iter_num % 20 == 0:\n                image = image_batch[1, 0:1, :, :]\n                image = (image - image.min()) / (image.max() - image.min())\n                writer.add_image('train/Image', image, iter_num)\n                outputs = torch.argmax(torch.softmax(outputs, dim=1), dim=1, keepdim=True)\n                writer.add_image('train/Prediction', outputs[1, ...] * 50, iter_num)\n                labs = label_batch[1, ...].unsqueeze(0) * 50\n                writer.add_image('train/GroundTruth', labs, iter_num)\n\n        save_interval = 50  # int(max_epoch/6)\n        if epoch_num > int(max_epoch / 2) and (epoch_num + 1) % save_interval == 0:\n            save_mode_path = os.path.join(snapshot_path, 'epoch_' + str(epoch_num) + '.pth')\n            torch.save(model.state_dict(), save_mode_path)\n            logging.info(\"save model to {}\".format(save_mode_path))\n\n        if epoch_num >= max_epoch - 1:\n            save_mode_path = os.path.join(snapshot_path, 'epoch_' + str(epoch_num) + '.pth')\n            torch.save(model.state_dict(), save_mode_path)\n            logging.info(\"save model to {}\".format(save_mode_path))\n            iterator.close()\n            break\n\n    writer.close()\n    return \"Training Finished!\"","metadata":{"execution":{"iopub.status.busy":"2023-09-07T18:28:27.345618Z","iopub.execute_input":"2023-09-07T18:28:27.346174Z","iopub.status.idle":"2023-09-07T18:28:34.007105Z","shell.execute_reply.started":"2023-09-07T18:28:27.346140Z","shell.execute_reply":"2023-09-07T18:28:34.006035Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### train.py\n\n训练过程入口文件","metadata":{}},{"cell_type":"code","source":"import argparse\nimport logging\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\n# from networks.vit_seg_modeling import VisionTransformer as ViT_seg\n# from networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\n# from trainer import trainer_synapse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--root_path', type=str,\n                    default='/kaggle/input/transeunet-synapse/data/Synapse/train_npz', help='root dir for data')\nparser.add_argument('--dataset', type=str,\n                    default='Synapse', help='experiment_name')\nparser.add_argument('--list_dir', type=str,\n                    default='/kaggle/input/transunet-lists/lists/lists_Synapse', help='list dir')\nparser.add_argument('--num_classes', type=int,\n                    default=9, help='output channel of network')\nparser.add_argument('--max_iterations', type=int,\n                    default=30000, help='maximum epoch number to train')\nparser.add_argument('--max_epochs', type=int,\n                    default=150, help='maximum epoch number to train')\nparser.add_argument('--batch_size', type=int,\n                    default=24, help='batch_size per gpu')\nparser.add_argument('--n_gpu', type=int, default=1, help='total gpu')\nparser.add_argument('--deterministic', type=int,  default=1,\n                    help='whether use deterministic training')\nparser.add_argument('--base_lr', type=float,  default=0.01,\n                    help='segmentation network learning rate')\nparser.add_argument('--img_size', type=int,\n                    default=224, help='input patch size of network input')\nparser.add_argument('--seed', type=int,\n                    default=1234, help='random seed')\nparser.add_argument('--n_skip', type=int,\n                    default=3, help='using number of skip-connect, default is num')\nparser.add_argument('--vit_name', type=str,\n                    default='R50-ViT-B_16', help='select one vit model')\nparser.add_argument('--vit_patches_size', type=int,\n                    default=16, help='vit_patches_size, default is 16')\nparser.add_argument('-f', type=str,\n                    default='', help='unkonw')\nargs = parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    if not args.deterministic:\n        cudnn.benchmark = True\n        cudnn.deterministic = False\n    else:\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    dataset_name = args.dataset\n    dataset_config = {\n        'Synapse': {\n            'root_path': '/kaggle/input/transeunet-synapse/data/Synapse/train_npz',\n            'list_dir': '/kaggle/input/transunet-lists/lists/lists_Synapse',\n            'num_classes': 9,\n        },\n    }\n    args.num_classes = dataset_config[dataset_name]['num_classes']\n    args.root_path = dataset_config[dataset_name]['root_path']\n    args.list_dir = dataset_config[dataset_name]['list_dir']\n    args.is_pretrain = True\n    args.exp = 'TU_' + dataset_name + str(args.img_size)\n    snapshot_path = \"./model/{}/{}\".format(args.exp, 'TU')\n    snapshot_path = snapshot_path + '_pretrain' if args.is_pretrain else snapshot_path\n    snapshot_path += '_' + args.vit_name\n    snapshot_path = snapshot_path + '_skip' + str(args.n_skip)\n    snapshot_path = snapshot_path + '_vitpatch' + str(args.vit_patches_size) if args.vit_patches_size!=16 else snapshot_path\n    snapshot_path = snapshot_path+'_'+str(args.max_iterations)[0:2]+'k' if args.max_iterations != 30000 else snapshot_path\n    snapshot_path = snapshot_path + '_epo' +str(args.max_epochs) if args.max_epochs != 30 else snapshot_path\n    snapshot_path = snapshot_path+'_bs'+str(args.batch_size)\n    snapshot_path = snapshot_path + '_lr' + str(args.base_lr) if args.base_lr != 0.01 else snapshot_path\n    snapshot_path = snapshot_path + '_'+str(args.img_size)\n    snapshot_path = snapshot_path + '_s'+str(args.seed) if args.seed!=1234 else snapshot_path\n\n    if not os.path.exists(snapshot_path):\n        os.makedirs(snapshot_path)\n    config_vit = CONFIGS[args.vit_name]\n    config_vit.n_classes = args.num_classes\n    config_vit.n_skip = args.n_skip\n    if args.vit_name.find('R50') != -1:\n        config_vit.patches.grid = (int(args.img_size / args.vit_patches_size), int(args.img_size / args.vit_patches_size))\n    net = VisionTransformer(config_vit, img_size=args.img_size, num_classes=config_vit.n_classes).cuda()\n    net.load_from(weights=np.load(config_vit.pretrained_path))\n\n    trainer = {'Synapse': trainer_synapse,}\n    trainer[dataset_name](args, net, snapshot_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T18:28:34.008391Z","iopub.execute_input":"2023-09-07T18:28:34.009533Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Namespace(root_path='/kaggle/input/transeunet-synapse/data/Synapse/train_npz', dataset='Synapse', list_dir='/kaggle/input/transunet-lists/lists/lists_Synapse', num_classes=9, max_iterations=30000, max_epochs=150, batch_size=24, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, n_skip=3, vit_name='R50-ViT-B_16', vit_patches_size=16, f='/root/.local/share/jupyter/runtime/kernel-60c48a3b-5674-4c7d-8414-280a57086b09.json', is_pretrain=True, exp='TU_Synapse224')\nThe length of train set is: 2211\n93 iterations per epoch. 13950 max iterations \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n  0%|                                         | 0/150 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"iteration 1 : loss : 1.466003, loss_ce: 2.004940\niteration 2 : loss : 1.420407, loss_ce: 1.903964\niteration 3 : loss : 1.334036, loss_ce: 1.751428\niteration 4 : loss : 1.225502, loss_ce: 1.540249\niteration 5 : loss : 1.105148, loss_ce: 1.325826\niteration 6 : loss : 0.980690, loss_ce: 1.079068\niteration 7 : loss : 0.871795, loss_ce: 0.883454\niteration 8 : loss : 0.770909, loss_ce: 0.678118\niteration 9 : loss : 0.701696, loss_ce: 0.554170\niteration 10 : loss : 0.652355, loss_ce: 0.455139\niteration 11 : loss : 0.615083, loss_ce: 0.394449\niteration 12 : loss : 0.598377, loss_ce: 0.358515\niteration 13 : loss : 0.545183, loss_ce: 0.250961\niteration 14 : loss : 0.539589, loss_ce: 0.249216\niteration 15 : loss : 0.546154, loss_ce: 0.257001\niteration 16 : loss : 0.540671, loss_ce: 0.246594\niteration 17 : loss : 0.534073, loss_ce: 0.246964\niteration 18 : loss : 0.563305, loss_ce: 0.298953\niteration 19 : loss : 0.500778, loss_ce: 0.194236\niteration 20 : loss : 0.522871, loss_ce: 0.234245\niteration 21 : loss : 0.489202, loss_ce: 0.181414\niteration 22 : loss : 0.471955, loss_ce: 0.135114\niteration 23 : loss : 0.528986, loss_ce: 0.244699\niteration 24 : loss : 0.530733, loss_ce: 0.227917\niteration 25 : loss : 0.476734, loss_ce: 0.153718\niteration 26 : loss : 0.483339, loss_ce: 0.152726\niteration 27 : loss : 0.504443, loss_ce: 0.202376\niteration 28 : loss : 0.477075, loss_ce: 0.161072\niteration 29 : loss : 0.472795, loss_ce: 0.158002\niteration 30 : loss : 0.480939, loss_ce: 0.169907\niteration 31 : loss : 0.508234, loss_ce: 0.212722\niteration 32 : loss : 0.469138, loss_ce: 0.149455\niteration 33 : loss : 0.479669, loss_ce: 0.161228\niteration 34 : loss : 0.472833, loss_ce: 0.149552\niteration 35 : loss : 0.497066, loss_ce: 0.189058\niteration 36 : loss : 0.465464, loss_ce: 0.141688\niteration 37 : loss : 0.466359, loss_ce: 0.137091\niteration 38 : loss : 0.447946, loss_ce: 0.096685\niteration 39 : loss : 0.478622, loss_ce: 0.165551\niteration 40 : loss : 0.499750, loss_ce: 0.191004\niteration 41 : loss : 0.471811, loss_ce: 0.146843\niteration 42 : loss : 0.500400, loss_ce: 0.207431\niteration 43 : loss : 0.472603, loss_ce: 0.153254\niteration 44 : loss : 0.490007, loss_ce: 0.194152\niteration 45 : loss : 0.496329, loss_ce: 0.206375\niteration 46 : loss : 0.457722, loss_ce: 0.145004\niteration 47 : loss : 0.461005, loss_ce: 0.137798\niteration 48 : loss : 0.460010, loss_ce: 0.135892\niteration 49 : loss : 0.475368, loss_ce: 0.153355\niteration 50 : loss : 0.470591, loss_ce: 0.149025\niteration 51 : loss : 0.469177, loss_ce: 0.146341\niteration 52 : loss : 0.476581, loss_ce: 0.161318\niteration 53 : loss : 0.467160, loss_ce: 0.148384\niteration 54 : loss : 0.457584, loss_ce: 0.110955\niteration 55 : loss : 0.497397, loss_ce: 0.203985\niteration 56 : loss : 0.459658, loss_ce: 0.149376\niteration 57 : loss : 0.452797, loss_ce: 0.128427\niteration 58 : loss : 0.477343, loss_ce: 0.178253\niteration 59 : loss : 0.465539, loss_ce: 0.157092\niteration 60 : loss : 0.498620, loss_ce: 0.207323\niteration 61 : loss : 0.444006, loss_ce: 0.108709\niteration 62 : loss : 0.471975, loss_ce: 0.166964\niteration 63 : loss : 0.482254, loss_ce: 0.178325\niteration 64 : loss : 0.463562, loss_ce: 0.153242\niteration 65 : loss : 0.474371, loss_ce: 0.175275\niteration 66 : loss : 0.455220, loss_ce: 0.135551\niteration 67 : loss : 0.455232, loss_ce: 0.146853\niteration 68 : loss : 0.506181, loss_ce: 0.215258\niteration 69 : loss : 0.454856, loss_ce: 0.139869\niteration 70 : loss : 0.448175, loss_ce: 0.134524\niteration 71 : loss : 0.445430, loss_ce: 0.120855\niteration 72 : loss : 0.447112, loss_ce: 0.124030\niteration 73 : loss : 0.444153, loss_ce: 0.113742\niteration 74 : loss : 0.449316, loss_ce: 0.127850\niteration 75 : loss : 0.460227, loss_ce: 0.133448\niteration 76 : loss : 0.456351, loss_ce: 0.152670\niteration 77 : loss : 0.446541, loss_ce: 0.140956\niteration 78 : loss : 0.459523, loss_ce: 0.162994\niteration 79 : loss : 0.440303, loss_ce: 0.119876\niteration 80 : loss : 0.425250, loss_ce: 0.108052\niteration 81 : loss : 0.440732, loss_ce: 0.121747\niteration 82 : loss : 0.413787, loss_ce: 0.085234\niteration 83 : loss : 0.428279, loss_ce: 0.109731\niteration 84 : loss : 0.459547, loss_ce: 0.153754\niteration 85 : loss : 0.443664, loss_ce: 0.142610\niteration 86 : loss : 0.415300, loss_ce: 0.103526\niteration 87 : loss : 0.406006, loss_ce: 0.115822\niteration 88 : loss : 0.400492, loss_ce: 0.103081\niteration 89 : loss : 0.408150, loss_ce: 0.114647\niteration 90 : loss : 0.421078, loss_ce: 0.120723\niteration 91 : loss : 0.423774, loss_ce: 0.114447\niteration 92 : loss : 0.425345, loss_ce: 0.118037\niteration 93 : loss : 0.525988, loss_ce: 0.260128\n","output_type":"stream"},{"name":"stderr","text":"  1%|▏                             | 1/150 [01:45<4:20:54, 105.06s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 94 : loss : 0.407299, loss_ce: 0.108901\niteration 95 : loss : 0.421394, loss_ce: 0.108470\niteration 96 : loss : 0.432079, loss_ce: 0.143300\niteration 97 : loss : 0.410951, loss_ce: 0.118729\niteration 98 : loss : 0.395022, loss_ce: 0.083684\niteration 99 : loss : 0.444192, loss_ce: 0.159585\niteration 100 : loss : 0.397321, loss_ce: 0.089705\niteration 101 : loss : 0.402358, loss_ce: 0.085298\niteration 102 : loss : 0.408267, loss_ce: 0.095860\niteration 103 : loss : 0.405689, loss_ce: 0.103493\niteration 104 : loss : 0.416467, loss_ce: 0.119406\niteration 105 : loss : 0.425790, loss_ce: 0.136129\niteration 106 : loss : 0.419472, loss_ce: 0.131133\niteration 107 : loss : 0.417674, loss_ce: 0.132879\niteration 108 : loss : 0.442881, loss_ce: 0.150723\niteration 109 : loss : 0.429851, loss_ce: 0.127707\niteration 110 : loss : 0.405823, loss_ce: 0.118261\niteration 111 : loss : 0.424092, loss_ce: 0.116333\niteration 112 : loss : 0.423039, loss_ce: 0.123241\niteration 113 : loss : 0.446936, loss_ce: 0.169466\niteration 114 : loss : 0.422303, loss_ce: 0.128799\niteration 115 : loss : 0.409140, loss_ce: 0.112648\niteration 116 : loss : 0.400937, loss_ce: 0.106175\niteration 117 : loss : 0.398686, loss_ce: 0.094423\niteration 118 : loss : 0.408266, loss_ce: 0.097727\niteration 119 : loss : 0.418675, loss_ce: 0.123812\niteration 120 : loss : 0.419696, loss_ce: 0.139163\niteration 121 : loss : 0.411590, loss_ce: 0.103209\niteration 122 : loss : 0.409862, loss_ce: 0.077251\niteration 123 : loss : 0.411152, loss_ce: 0.117444\niteration 124 : loss : 0.405861, loss_ce: 0.099826\niteration 125 : loss : 0.413851, loss_ce: 0.121800\niteration 126 : loss : 0.425644, loss_ce: 0.109939\niteration 127 : loss : 0.390077, loss_ce: 0.089283\niteration 128 : loss : 0.410365, loss_ce: 0.126786\niteration 129 : loss : 0.411281, loss_ce: 0.127222\niteration 130 : loss : 0.407651, loss_ce: 0.123885\niteration 131 : loss : 0.402933, loss_ce: 0.118922\niteration 132 : loss : 0.394630, loss_ce: 0.098320\niteration 133 : loss : 0.396193, loss_ce: 0.102098\niteration 134 : loss : 0.407375, loss_ce: 0.109568\niteration 135 : loss : 0.401088, loss_ce: 0.089759\niteration 136 : loss : 0.417274, loss_ce: 0.134229\niteration 137 : loss : 0.395554, loss_ce: 0.089688\niteration 138 : loss : 0.414988, loss_ce: 0.118138\niteration 139 : loss : 0.413665, loss_ce: 0.135542\niteration 140 : loss : 0.396812, loss_ce: 0.101594\niteration 141 : loss : 0.393637, loss_ce: 0.106602\niteration 142 : loss : 0.386708, loss_ce: 0.091322\niteration 143 : loss : 0.387333, loss_ce: 0.090717\niteration 144 : loss : 0.413172, loss_ce: 0.118913\niteration 145 : loss : 0.410320, loss_ce: 0.114131\niteration 146 : loss : 0.412634, loss_ce: 0.120933\niteration 147 : loss : 0.391044, loss_ce: 0.095587\niteration 148 : loss : 0.386070, loss_ce: 0.099551\niteration 149 : loss : 0.391991, loss_ce: 0.088018\niteration 150 : loss : 0.407603, loss_ce: 0.118979\niteration 151 : loss : 0.391421, loss_ce: 0.089716\niteration 152 : loss : 0.393270, loss_ce: 0.078332\niteration 153 : loss : 0.383136, loss_ce: 0.088693\niteration 154 : loss : 0.385707, loss_ce: 0.080282\niteration 155 : loss : 0.404067, loss_ce: 0.113813\niteration 156 : loss : 0.386047, loss_ce: 0.088153\niteration 157 : loss : 0.395958, loss_ce: 0.098052\niteration 158 : loss : 0.400913, loss_ce: 0.114373\niteration 159 : loss : 0.408080, loss_ce: 0.120139\niteration 160 : loss : 0.388729, loss_ce: 0.103404\niteration 161 : loss : 0.390445, loss_ce: 0.102153\niteration 162 : loss : 0.372508, loss_ce: 0.068376\niteration 163 : loss : 0.401807, loss_ce: 0.107360\niteration 164 : loss : 0.389182, loss_ce: 0.094979\niteration 165 : loss : 0.377841, loss_ce: 0.079206\niteration 166 : loss : 0.402910, loss_ce: 0.116574\niteration 167 : loss : 0.385459, loss_ce: 0.097308\niteration 168 : loss : 0.386755, loss_ce: 0.095191\niteration 169 : loss : 0.397353, loss_ce: 0.084616\niteration 170 : loss : 0.418218, loss_ce: 0.153238\niteration 171 : loss : 0.384147, loss_ce: 0.100429\niteration 172 : loss : 0.395011, loss_ce: 0.101638\niteration 173 : loss : 0.367652, loss_ce: 0.068142\niteration 174 : loss : 0.387552, loss_ce: 0.103889\niteration 175 : loss : 0.393940, loss_ce: 0.116540\niteration 176 : loss : 0.390475, loss_ce: 0.097818\niteration 177 : loss : 0.391509, loss_ce: 0.110824\niteration 178 : loss : 0.382995, loss_ce: 0.098694\niteration 179 : loss : 0.381625, loss_ce: 0.100274\niteration 180 : loss : 0.388439, loss_ce: 0.101201\niteration 181 : loss : 0.387754, loss_ce: 0.095511\niteration 182 : loss : 0.384314, loss_ce: 0.100876\niteration 183 : loss : 0.391003, loss_ce: 0.108116\niteration 184 : loss : 0.373074, loss_ce: 0.090335\niteration 185 : loss : 0.358772, loss_ce: 0.073319\niteration 186 : loss : 0.443792, loss_ce: 0.101179\n","output_type":"stream"},{"name":"stderr","text":"  1%|▍                              | 2/150 [03:20<4:04:55, 99.29s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 187 : loss : 0.397356, loss_ce: 0.084027\niteration 188 : loss : 0.441982, loss_ce: 0.147638\niteration 189 : loss : 0.400175, loss_ce: 0.083308\niteration 190 : loss : 0.402611, loss_ce: 0.088981\niteration 191 : loss : 0.409600, loss_ce: 0.091849\niteration 192 : loss : 0.398014, loss_ce: 0.087957\niteration 193 : loss : 0.407065, loss_ce: 0.108402\niteration 194 : loss : 0.395915, loss_ce: 0.099678\niteration 195 : loss : 0.400732, loss_ce: 0.109938\niteration 196 : loss : 0.399665, loss_ce: 0.104434\niteration 197 : loss : 0.397852, loss_ce: 0.104609\niteration 198 : loss : 0.401319, loss_ce: 0.104875\niteration 199 : loss : 0.398447, loss_ce: 0.101812\niteration 200 : loss : 0.415576, loss_ce: 0.134384\niteration 201 : loss : 0.432615, loss_ce: 0.150784\niteration 202 : loss : 0.425791, loss_ce: 0.144260\niteration 203 : loss : 0.415150, loss_ce: 0.122453\niteration 204 : loss : 0.398320, loss_ce: 0.101149\niteration 205 : loss : 0.389179, loss_ce: 0.094750\niteration 206 : loss : 0.401236, loss_ce: 0.114757\niteration 207 : loss : 0.394873, loss_ce: 0.099426\niteration 208 : loss : 0.413414, loss_ce: 0.143501\niteration 209 : loss : 0.391669, loss_ce: 0.117544\niteration 210 : loss : 0.411028, loss_ce: 0.143211\niteration 211 : loss : 0.384080, loss_ce: 0.099322\niteration 212 : loss : 0.378200, loss_ce: 0.087251\niteration 213 : loss : 0.374498, loss_ce: 0.089104\niteration 214 : loss : 0.386124, loss_ce: 0.083601\niteration 215 : loss : 0.387371, loss_ce: 0.092363\niteration 216 : loss : 0.387686, loss_ce: 0.099731\niteration 217 : loss : 0.366452, loss_ce: 0.060881\niteration 218 : loss : 0.405659, loss_ce: 0.133614\niteration 219 : loss : 0.371196, loss_ce: 0.079736\niteration 220 : loss : 0.355173, loss_ce: 0.063576\niteration 221 : loss : 0.402807, loss_ce: 0.112757\niteration 222 : loss : 0.391069, loss_ce: 0.112124\niteration 223 : loss : 0.392368, loss_ce: 0.127183\niteration 224 : loss : 0.420524, loss_ce: 0.084182\niteration 225 : loss : 0.405649, loss_ce: 0.149805\niteration 226 : loss : 0.389415, loss_ce: 0.119257\niteration 227 : loss : 0.378726, loss_ce: 0.100777\niteration 228 : loss : 0.373726, loss_ce: 0.095168\niteration 229 : loss : 0.381701, loss_ce: 0.093414\niteration 230 : loss : 0.387658, loss_ce: 0.109524\niteration 231 : loss : 0.370124, loss_ce: 0.089502\niteration 232 : loss : 0.367052, loss_ce: 0.060531\niteration 233 : loss : 0.373926, loss_ce: 0.086128\niteration 234 : loss : 0.353560, loss_ce: 0.062197\niteration 235 : loss : 0.372234, loss_ce: 0.075028\niteration 236 : loss : 0.345266, loss_ce: 0.044005\niteration 237 : loss : 0.387138, loss_ce: 0.100384\niteration 238 : loss : 0.375449, loss_ce: 0.079569\niteration 239 : loss : 0.352067, loss_ce: 0.061851\niteration 240 : loss : 0.361899, loss_ce: 0.081939\niteration 241 : loss : 0.364588, loss_ce: 0.080702\niteration 242 : loss : 0.359626, loss_ce: 0.085524\niteration 243 : loss : 0.370183, loss_ce: 0.089208\niteration 244 : loss : 0.398555, loss_ce: 0.123593\niteration 245 : loss : 0.369376, loss_ce: 0.093425\niteration 246 : loss : 0.383536, loss_ce: 0.094490\niteration 247 : loss : 0.367712, loss_ce: 0.089785\niteration 248 : loss : 0.360757, loss_ce: 0.081628\niteration 249 : loss : 0.390683, loss_ce: 0.089510\niteration 250 : loss : 0.358161, loss_ce: 0.062247\niteration 251 : loss : 0.388215, loss_ce: 0.120219\niteration 252 : loss : 0.396079, loss_ce: 0.116139\niteration 253 : loss : 0.361668, loss_ce: 0.078474\niteration 254 : loss : 0.369282, loss_ce: 0.082013\niteration 255 : loss : 0.381913, loss_ce: 0.107968\niteration 256 : loss : 0.365461, loss_ce: 0.089428\niteration 257 : loss : 0.374472, loss_ce: 0.101241\niteration 258 : loss : 0.364175, loss_ce: 0.088124\niteration 259 : loss : 0.381136, loss_ce: 0.111973\niteration 260 : loss : 0.377055, loss_ce: 0.102609\niteration 261 : loss : 0.356742, loss_ce: 0.074771\niteration 262 : loss : 0.364084, loss_ce: 0.096002\niteration 263 : loss : 0.348152, loss_ce: 0.078290\niteration 264 : loss : 0.366355, loss_ce: 0.095686\niteration 265 : loss : 0.364358, loss_ce: 0.089610\niteration 266 : loss : 0.370608, loss_ce: 0.093381\niteration 267 : loss : 0.366257, loss_ce: 0.097166\niteration 268 : loss : 0.359844, loss_ce: 0.079656\niteration 269 : loss : 0.367947, loss_ce: 0.089669\niteration 270 : loss : 0.351986, loss_ce: 0.067647\niteration 271 : loss : 0.377653, loss_ce: 0.098222\niteration 272 : loss : 0.353562, loss_ce: 0.074055\niteration 273 : loss : 0.397826, loss_ce: 0.134461\niteration 274 : loss : 0.366894, loss_ce: 0.073909\niteration 275 : loss : 0.343585, loss_ce: 0.062681\niteration 276 : loss : 0.343040, loss_ce: 0.057252\niteration 277 : loss : 0.390352, loss_ce: 0.113925\niteration 278 : loss : 0.346267, loss_ce: 0.064246\niteration 279 : loss : 0.372925, loss_ce: 0.078361\n","output_type":"stream"},{"name":"stderr","text":"  2%|▌                              | 3/150 [04:56<3:59:31, 97.76s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 280 : loss : 0.361096, loss_ce: 0.067261\niteration 281 : loss : 0.364436, loss_ce: 0.073140\niteration 282 : loss : 0.350183, loss_ce: 0.073041\niteration 283 : loss : 0.365184, loss_ce: 0.076755\niteration 284 : loss : 0.358799, loss_ce: 0.079553\niteration 285 : loss : 0.354540, loss_ce: 0.076291\niteration 286 : loss : 0.375098, loss_ce: 0.111719\niteration 287 : loss : 0.354783, loss_ce: 0.085509\niteration 288 : loss : 0.360041, loss_ce: 0.089164\niteration 289 : loss : 0.344768, loss_ce: 0.060346\niteration 290 : loss : 0.353330, loss_ce: 0.082673\niteration 291 : loss : 0.377262, loss_ce: 0.125149\niteration 292 : loss : 0.353943, loss_ce: 0.069006\niteration 293 : loss : 0.359453, loss_ce: 0.087850\niteration 294 : loss : 0.368132, loss_ce: 0.096034\niteration 295 : loss : 0.370201, loss_ce: 0.106636\niteration 296 : loss : 0.345613, loss_ce: 0.065857\niteration 297 : loss : 0.368479, loss_ce: 0.115897\niteration 298 : loss : 0.354842, loss_ce: 0.062761\niteration 299 : loss : 0.361500, loss_ce: 0.093414\niteration 300 : loss : 0.363826, loss_ce: 0.092774\niteration 301 : loss : 0.362502, loss_ce: 0.082731\niteration 302 : loss : 0.369798, loss_ce: 0.096363\niteration 303 : loss : 0.351723, loss_ce: 0.054263\niteration 304 : loss : 0.400891, loss_ce: 0.153874\niteration 305 : loss : 0.369168, loss_ce: 0.098248\niteration 306 : loss : 0.360700, loss_ce: 0.101431\niteration 307 : loss : 0.351758, loss_ce: 0.079987\niteration 308 : loss : 0.358770, loss_ce: 0.090792\niteration 309 : loss : 0.342247, loss_ce: 0.075012\niteration 310 : loss : 0.372282, loss_ce: 0.120161\niteration 311 : loss : 0.362295, loss_ce: 0.099855\niteration 312 : loss : 0.342088, loss_ce: 0.074833\niteration 313 : loss : 0.351721, loss_ce: 0.083185\niteration 314 : loss : 0.343816, loss_ce: 0.083393\niteration 315 : loss : 0.353283, loss_ce: 0.065952\niteration 316 : loss : 0.362013, loss_ce: 0.101206\niteration 317 : loss : 0.363512, loss_ce: 0.096571\niteration 318 : loss : 0.350407, loss_ce: 0.077293\niteration 319 : loss : 0.350485, loss_ce: 0.091687\niteration 320 : loss : 0.349976, loss_ce: 0.081682\niteration 321 : loss : 0.351127, loss_ce: 0.081347\niteration 322 : loss : 0.361783, loss_ce: 0.088792\niteration 323 : loss : 0.354673, loss_ce: 0.091033\niteration 324 : loss : 0.334058, loss_ce: 0.070761\niteration 325 : loss : 0.370490, loss_ce: 0.109980\niteration 326 : loss : 0.362086, loss_ce: 0.085848\niteration 327 : loss : 0.349991, loss_ce: 0.088805\niteration 328 : loss : 0.338073, loss_ce: 0.072406\niteration 329 : loss : 0.345284, loss_ce: 0.068833\niteration 330 : loss : 0.369673, loss_ce: 0.093654\niteration 331 : loss : 0.350181, loss_ce: 0.076085\niteration 332 : loss : 0.355195, loss_ce: 0.102539\niteration 333 : loss : 0.338537, loss_ce: 0.071475\niteration 334 : loss : 0.345378, loss_ce: 0.075844\niteration 335 : loss : 0.336994, loss_ce: 0.065048\niteration 336 : loss : 0.363050, loss_ce: 0.098326\niteration 337 : loss : 0.344235, loss_ce: 0.083392\niteration 338 : loss : 0.341566, loss_ce: 0.080654\niteration 339 : loss : 0.344520, loss_ce: 0.081051\niteration 340 : loss : 0.346002, loss_ce: 0.083234\niteration 341 : loss : 0.342286, loss_ce: 0.080987\niteration 342 : loss : 0.348153, loss_ce: 0.083162\niteration 343 : loss : 0.347204, loss_ce: 0.076520\niteration 344 : loss : 0.344189, loss_ce: 0.067397\niteration 345 : loss : 0.337593, loss_ce: 0.075509\niteration 346 : loss : 0.348996, loss_ce: 0.084818\niteration 347 : loss : 0.350411, loss_ce: 0.096496\niteration 348 : loss : 0.331509, loss_ce: 0.056759\niteration 349 : loss : 0.324132, loss_ce: 0.052431\niteration 350 : loss : 0.320145, loss_ce: 0.047915\niteration 351 : loss : 0.348702, loss_ce: 0.091403\niteration 352 : loss : 0.343308, loss_ce: 0.075843\niteration 353 : loss : 0.346195, loss_ce: 0.088432\niteration 354 : loss : 0.338409, loss_ce: 0.067682\niteration 355 : loss : 0.333742, loss_ce: 0.080622\niteration 356 : loss : 0.328193, loss_ce: 0.060916\niteration 357 : loss : 0.366761, loss_ce: 0.114116\niteration 358 : loss : 0.328002, loss_ce: 0.063732\niteration 359 : loss : 0.344876, loss_ce: 0.096916\niteration 360 : loss : 0.332653, loss_ce: 0.088352\niteration 361 : loss : 0.332285, loss_ce: 0.075581\niteration 362 : loss : 0.328994, loss_ce: 0.069643\niteration 363 : loss : 0.326294, loss_ce: 0.084093\niteration 364 : loss : 0.357939, loss_ce: 0.097774\niteration 365 : loss : 0.342339, loss_ce: 0.078153\niteration 366 : loss : 0.336948, loss_ce: 0.087319\niteration 367 : loss : 0.313158, loss_ce: 0.052947\niteration 368 : loss : 0.356455, loss_ce: 0.120688\niteration 369 : loss : 0.321799, loss_ce: 0.072848\niteration 370 : loss : 0.315913, loss_ce: 0.056243\niteration 371 : loss : 0.343350, loss_ce: 0.084072\niteration 372 : loss : 0.459448, loss_ce: 0.103470\n","output_type":"stream"},{"name":"stderr","text":"  3%|▊                              | 4/150 [06:32<3:56:06, 97.03s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 373 : loss : 0.327873, loss_ce: 0.093627\niteration 374 : loss : 0.352464, loss_ce: 0.103662\niteration 375 : loss : 0.323572, loss_ce: 0.059153\niteration 376 : loss : 0.329877, loss_ce: 0.065046\niteration 377 : loss : 0.332912, loss_ce: 0.084312\niteration 378 : loss : 0.315710, loss_ce: 0.057467\niteration 379 : loss : 0.328317, loss_ce: 0.078968\niteration 380 : loss : 0.319702, loss_ce: 0.069900\niteration 381 : loss : 0.343080, loss_ce: 0.104169\niteration 382 : loss : 0.336534, loss_ce: 0.085483\niteration 383 : loss : 0.322340, loss_ce: 0.085693\niteration 384 : loss : 0.322490, loss_ce: 0.088991\niteration 385 : loss : 0.333345, loss_ce: 0.092030\niteration 386 : loss : 0.309623, loss_ce: 0.059549\niteration 387 : loss : 0.319238, loss_ce: 0.070108\niteration 388 : loss : 0.303258, loss_ce: 0.054977\niteration 389 : loss : 0.311429, loss_ce: 0.070971\niteration 390 : loss : 0.320649, loss_ce: 0.065435\niteration 391 : loss : 0.317978, loss_ce: 0.081706\niteration 392 : loss : 0.293243, loss_ce: 0.052597\niteration 393 : loss : 0.288684, loss_ce: 0.047729\niteration 394 : loss : 0.325117, loss_ce: 0.091996\niteration 395 : loss : 0.295101, loss_ce: 0.054698\niteration 396 : loss : 0.308148, loss_ce: 0.070127\niteration 397 : loss : 0.312522, loss_ce: 0.078097\niteration 398 : loss : 0.305559, loss_ce: 0.068144\niteration 399 : loss : 0.301735, loss_ce: 0.064744\niteration 400 : loss : 0.311099, loss_ce: 0.058009\niteration 401 : loss : 0.303124, loss_ce: 0.066661\niteration 402 : loss : 0.309733, loss_ce: 0.064812\niteration 403 : loss : 0.325525, loss_ce: 0.091444\niteration 404 : loss : 0.299837, loss_ce: 0.058017\niteration 405 : loss : 0.302551, loss_ce: 0.074674\niteration 406 : loss : 0.316692, loss_ce: 0.049998\niteration 407 : loss : 0.303875, loss_ce: 0.077505\niteration 408 : loss : 0.319660, loss_ce: 0.075039\niteration 409 : loss : 0.348055, loss_ce: 0.115040\niteration 410 : loss : 0.325393, loss_ce: 0.079617\niteration 411 : loss : 0.306115, loss_ce: 0.072716\niteration 412 : loss : 0.317156, loss_ce: 0.073045\niteration 413 : loss : 0.309948, loss_ce: 0.069218\niteration 414 : loss : 0.367604, loss_ce: 0.123072\niteration 415 : loss : 0.321553, loss_ce: 0.080565\niteration 416 : loss : 0.322435, loss_ce: 0.075438\niteration 417 : loss : 0.316993, loss_ce: 0.059050\niteration 418 : loss : 0.309345, loss_ce: 0.047420\niteration 419 : loss : 0.329575, loss_ce: 0.085923\niteration 420 : loss : 0.306226, loss_ce: 0.061366\niteration 421 : loss : 0.332073, loss_ce: 0.097114\niteration 422 : loss : 0.315404, loss_ce: 0.067941\niteration 423 : loss : 0.322981, loss_ce: 0.098744\niteration 424 : loss : 0.312084, loss_ce: 0.075384\niteration 425 : loss : 0.303852, loss_ce: 0.078893\niteration 426 : loss : 0.309152, loss_ce: 0.061597\niteration 427 : loss : 0.337141, loss_ce: 0.081212\niteration 428 : loss : 0.308367, loss_ce: 0.069921\niteration 429 : loss : 0.350087, loss_ce: 0.088203\niteration 430 : loss : 0.303711, loss_ce: 0.056446\niteration 431 : loss : 0.325265, loss_ce: 0.079343\niteration 432 : loss : 0.307585, loss_ce: 0.050476\niteration 433 : loss : 0.351941, loss_ce: 0.126363\niteration 434 : loss : 0.316663, loss_ce: 0.085384\niteration 435 : loss : 0.314907, loss_ce: 0.054684\niteration 436 : loss : 0.330325, loss_ce: 0.095819\niteration 437 : loss : 0.335712, loss_ce: 0.101637\niteration 438 : loss : 0.305354, loss_ce: 0.074969\niteration 439 : loss : 0.303721, loss_ce: 0.078744\niteration 440 : loss : 0.309816, loss_ce: 0.058645\niteration 441 : loss : 0.307338, loss_ce: 0.084901\niteration 442 : loss : 0.298108, loss_ce: 0.064907\niteration 443 : loss : 0.300365, loss_ce: 0.071331\niteration 444 : loss : 0.296659, loss_ce: 0.061567\niteration 445 : loss : 0.307434, loss_ce: 0.047024\niteration 446 : loss : 0.292005, loss_ce: 0.059454\niteration 447 : loss : 0.308375, loss_ce: 0.087330\niteration 448 : loss : 0.292057, loss_ce: 0.055422\niteration 449 : loss : 0.298198, loss_ce: 0.064307\niteration 450 : loss : 0.312658, loss_ce: 0.089823\niteration 451 : loss : 0.301893, loss_ce: 0.065541\niteration 452 : loss : 0.324834, loss_ce: 0.086203\niteration 453 : loss : 0.299956, loss_ce: 0.077817\niteration 454 : loss : 0.309023, loss_ce: 0.090647\niteration 455 : loss : 0.296440, loss_ce: 0.061676\niteration 456 : loss : 0.311555, loss_ce: 0.084082\niteration 457 : loss : 0.308418, loss_ce: 0.064018\niteration 458 : loss : 0.296846, loss_ce: 0.079423\niteration 459 : loss : 0.321224, loss_ce: 0.098720\niteration 460 : loss : 0.316313, loss_ce: 0.081174\niteration 461 : loss : 0.295960, loss_ce: 0.063973\niteration 462 : loss : 0.315502, loss_ce: 0.067618\niteration 463 : loss : 0.289129, loss_ce: 0.064747\niteration 464 : loss : 0.277793, loss_ce: 0.047219\niteration 465 : loss : 0.491488, loss_ce: 0.092877\n","output_type":"stream"},{"name":"stderr","text":"  3%|█                              | 5/150 [08:07<3:53:20, 96.55s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 466 : loss : 0.298718, loss_ce: 0.072332\niteration 467 : loss : 0.327732, loss_ce: 0.049180\niteration 468 : loss : 0.297094, loss_ce: 0.064514\niteration 469 : loss : 0.314202, loss_ce: 0.085347\niteration 470 : loss : 0.311862, loss_ce: 0.080131\niteration 471 : loss : 0.289414, loss_ce: 0.052794\niteration 472 : loss : 0.299854, loss_ce: 0.061849\niteration 473 : loss : 0.293117, loss_ce: 0.082401\niteration 474 : loss : 0.293343, loss_ce: 0.074622\niteration 475 : loss : 0.282914, loss_ce: 0.047835\niteration 476 : loss : 0.276918, loss_ce: 0.061736\niteration 477 : loss : 0.292783, loss_ce: 0.064982\niteration 478 : loss : 0.287594, loss_ce: 0.058565\niteration 479 : loss : 0.268009, loss_ce: 0.051952\niteration 480 : loss : 0.280794, loss_ce: 0.062854\niteration 481 : loss : 0.288139, loss_ce: 0.056934\niteration 482 : loss : 0.297809, loss_ce: 0.081442\niteration 483 : loss : 0.286821, loss_ce: 0.068024\niteration 484 : loss : 0.272201, loss_ce: 0.049586\niteration 485 : loss : 0.287101, loss_ce: 0.071995\niteration 486 : loss : 0.280279, loss_ce: 0.049862\niteration 487 : loss : 0.281241, loss_ce: 0.068679\niteration 488 : loss : 0.266245, loss_ce: 0.045854\niteration 489 : loss : 0.278356, loss_ce: 0.064264\niteration 490 : loss : 0.278885, loss_ce: 0.065880\niteration 491 : loss : 0.268621, loss_ce: 0.052865\niteration 492 : loss : 0.272309, loss_ce: 0.057724\niteration 493 : loss : 0.306359, loss_ce: 0.080418\niteration 494 : loss : 0.273378, loss_ce: 0.060607\niteration 495 : loss : 0.266366, loss_ce: 0.068260\niteration 496 : loss : 0.252754, loss_ce: 0.039099\niteration 497 : loss : 0.273004, loss_ce: 0.059380\niteration 498 : loss : 0.268067, loss_ce: 0.060749\niteration 499 : loss : 0.260100, loss_ce: 0.039613\niteration 500 : loss : 0.301032, loss_ce: 0.071304\niteration 501 : loss : 0.288087, loss_ce: 0.065046\niteration 502 : loss : 0.265733, loss_ce: 0.048753\niteration 503 : loss : 0.269874, loss_ce: 0.064345\niteration 504 : loss : 0.292921, loss_ce: 0.094199\niteration 505 : loss : 0.271276, loss_ce: 0.059661\niteration 506 : loss : 0.281661, loss_ce: 0.067978\niteration 507 : loss : 0.296058, loss_ce: 0.046721\niteration 508 : loss : 0.291726, loss_ce: 0.087496\niteration 509 : loss : 0.256949, loss_ce: 0.056496\niteration 510 : loss : 0.270556, loss_ce: 0.066982\niteration 511 : loss : 0.265265, loss_ce: 0.052295\niteration 512 : loss : 0.260254, loss_ce: 0.052293\niteration 513 : loss : 0.265316, loss_ce: 0.059410\niteration 514 : loss : 0.321143, loss_ce: 0.061915\niteration 515 : loss : 0.237082, loss_ce: 0.037412\niteration 516 : loss : 0.256950, loss_ce: 0.053730\niteration 517 : loss : 0.245752, loss_ce: 0.042161\niteration 518 : loss : 0.277426, loss_ce: 0.055324\niteration 519 : loss : 0.266140, loss_ce: 0.080606\niteration 520 : loss : 0.289615, loss_ce: 0.081372\niteration 521 : loss : 0.253143, loss_ce: 0.053954\niteration 522 : loss : 0.295835, loss_ce: 0.089846\niteration 523 : loss : 0.250690, loss_ce: 0.056297\niteration 524 : loss : 0.273393, loss_ce: 0.069018\niteration 525 : loss : 0.249878, loss_ce: 0.054194\niteration 526 : loss : 0.271485, loss_ce: 0.063530\niteration 527 : loss : 0.283218, loss_ce: 0.076907\niteration 528 : loss : 0.293081, loss_ce: 0.042762\niteration 529 : loss : 0.251539, loss_ce: 0.042175\niteration 530 : loss : 0.244855, loss_ce: 0.040334\niteration 531 : loss : 0.267463, loss_ce: 0.070993\niteration 532 : loss : 0.251063, loss_ce: 0.049714\niteration 533 : loss : 0.259323, loss_ce: 0.052012\niteration 534 : loss : 0.253830, loss_ce: 0.046404\niteration 535 : loss : 0.254240, loss_ce: 0.050270\niteration 536 : loss : 0.254735, loss_ce: 0.053806\niteration 537 : loss : 0.256403, loss_ce: 0.059986\niteration 538 : loss : 0.264534, loss_ce: 0.064955\niteration 539 : loss : 0.259571, loss_ce: 0.063434\niteration 540 : loss : 0.268431, loss_ce: 0.083069\niteration 541 : loss : 0.240973, loss_ce: 0.054523\niteration 542 : loss : 0.235440, loss_ce: 0.037314\niteration 543 : loss : 0.229918, loss_ce: 0.037031\niteration 544 : loss : 0.262928, loss_ce: 0.061327\niteration 545 : loss : 0.256825, loss_ce: 0.057185\niteration 546 : loss : 0.260809, loss_ce: 0.073457\niteration 547 : loss : 0.257593, loss_ce: 0.075561\niteration 548 : loss : 0.254257, loss_ce: 0.059651\niteration 549 : loss : 0.251542, loss_ce: 0.058688\niteration 550 : loss : 0.285296, loss_ce: 0.082086\niteration 551 : loss : 0.260107, loss_ce: 0.062315\niteration 552 : loss : 0.250116, loss_ce: 0.064027\niteration 553 : loss : 0.231638, loss_ce: 0.048556\niteration 554 : loss : 0.252171, loss_ce: 0.065470\niteration 555 : loss : 0.239216, loss_ce: 0.045917\niteration 556 : loss : 0.249270, loss_ce: 0.060929\niteration 557 : loss : 0.219540, loss_ce: 0.035537\niteration 558 : loss : 0.330270, loss_ce: 0.092108\n","output_type":"stream"},{"name":"stderr","text":"  4%|█▏                             | 6/150 [09:43<3:50:50, 96.19s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 559 : loss : 0.247243, loss_ce: 0.067074\niteration 560 : loss : 0.267779, loss_ce: 0.065390\niteration 561 : loss : 0.243630, loss_ce: 0.058322\niteration 562 : loss : 0.254623, loss_ce: 0.056641\niteration 563 : loss : 0.265136, loss_ce: 0.049975\niteration 564 : loss : 0.267868, loss_ce: 0.048796\niteration 565 : loss : 0.236212, loss_ce: 0.049046\niteration 566 : loss : 0.283395, loss_ce: 0.096409\niteration 567 : loss : 0.235787, loss_ce: 0.040994\niteration 568 : loss : 0.235365, loss_ce: 0.039079\niteration 569 : loss : 0.247123, loss_ce: 0.045040\niteration 570 : loss : 0.260516, loss_ce: 0.045942\niteration 571 : loss : 0.273871, loss_ce: 0.072833\niteration 572 : loss : 0.280900, loss_ce: 0.057221\niteration 573 : loss : 0.261342, loss_ce: 0.067684\niteration 574 : loss : 0.273373, loss_ce: 0.073992\niteration 575 : loss : 0.266021, loss_ce: 0.065220\niteration 576 : loss : 0.242173, loss_ce: 0.058028\niteration 577 : loss : 0.250042, loss_ce: 0.075135\niteration 578 : loss : 0.234736, loss_ce: 0.057726\niteration 579 : loss : 0.250568, loss_ce: 0.065296\niteration 580 : loss : 0.214894, loss_ce: 0.036257\niteration 581 : loss : 0.278297, loss_ce: 0.084085\niteration 582 : loss : 0.247405, loss_ce: 0.043667\niteration 583 : loss : 0.245949, loss_ce: 0.071741\niteration 584 : loss : 0.247680, loss_ce: 0.051162\niteration 585 : loss : 0.247331, loss_ce: 0.051598\niteration 586 : loss : 0.243955, loss_ce: 0.059127\niteration 587 : loss : 0.248198, loss_ce: 0.049740\niteration 588 : loss : 0.260804, loss_ce: 0.055364\niteration 589 : loss : 0.241203, loss_ce: 0.064301\niteration 590 : loss : 0.246829, loss_ce: 0.050326\niteration 591 : loss : 0.257094, loss_ce: 0.069624\niteration 592 : loss : 0.248531, loss_ce: 0.046176\niteration 593 : loss : 0.254721, loss_ce: 0.079439\niteration 594 : loss : 0.234274, loss_ce: 0.048412\niteration 595 : loss : 0.248515, loss_ce: 0.057688\niteration 596 : loss : 0.226574, loss_ce: 0.045411\niteration 597 : loss : 0.242087, loss_ce: 0.060262\niteration 598 : loss : 0.260759, loss_ce: 0.049521\niteration 599 : loss : 0.254691, loss_ce: 0.051633\niteration 600 : loss : 0.236439, loss_ce: 0.046601\niteration 601 : loss : 0.247816, loss_ce: 0.066591\niteration 602 : loss : 0.229468, loss_ce: 0.045028\niteration 603 : loss : 0.234156, loss_ce: 0.065057\niteration 604 : loss : 0.230802, loss_ce: 0.060586\niteration 605 : loss : 0.239502, loss_ce: 0.076763\niteration 606 : loss : 0.236105, loss_ce: 0.056691\niteration 607 : loss : 0.263195, loss_ce: 0.077641\niteration 608 : loss : 0.231810, loss_ce: 0.045001\niteration 609 : loss : 0.236188, loss_ce: 0.058831\niteration 610 : loss : 0.251866, loss_ce: 0.044053\niteration 611 : loss : 0.226885, loss_ce: 0.042323\niteration 612 : loss : 0.248947, loss_ce: 0.056852\niteration 613 : loss : 0.208613, loss_ce: 0.021529\niteration 614 : loss : 0.258356, loss_ce: 0.061157\niteration 615 : loss : 0.239632, loss_ce: 0.049071\niteration 616 : loss : 0.241171, loss_ce: 0.052839\niteration 617 : loss : 0.250558, loss_ce: 0.079233\niteration 618 : loss : 0.248882, loss_ce: 0.051013\niteration 619 : loss : 0.230643, loss_ce: 0.044539\niteration 620 : loss : 0.231460, loss_ce: 0.035311\niteration 621 : loss : 0.248751, loss_ce: 0.051479\niteration 622 : loss : 0.254330, loss_ce: 0.042505\niteration 623 : loss : 0.235872, loss_ce: 0.047890\niteration 624 : loss : 0.224806, loss_ce: 0.044820\niteration 625 : loss : 0.236067, loss_ce: 0.059988\niteration 626 : loss : 0.222084, loss_ce: 0.054472\niteration 627 : loss : 0.250714, loss_ce: 0.043476\niteration 628 : loss : 0.219576, loss_ce: 0.038995\niteration 629 : loss : 0.237705, loss_ce: 0.056684\niteration 630 : loss : 0.251530, loss_ce: 0.065136\niteration 631 : loss : 0.234672, loss_ce: 0.047749\niteration 632 : loss : 0.225126, loss_ce: 0.051375\niteration 633 : loss : 0.231212, loss_ce: 0.060873\niteration 634 : loss : 0.226093, loss_ce: 0.049248\niteration 635 : loss : 0.230876, loss_ce: 0.046782\niteration 636 : loss : 0.246338, loss_ce: 0.049606\niteration 637 : loss : 0.234084, loss_ce: 0.057576\niteration 638 : loss : 0.229661, loss_ce: 0.052464\niteration 639 : loss : 0.244378, loss_ce: 0.078966\niteration 640 : loss : 0.235599, loss_ce: 0.057136\niteration 641 : loss : 0.234900, loss_ce: 0.042236\niteration 642 : loss : 0.226575, loss_ce: 0.052394\niteration 643 : loss : 0.242101, loss_ce: 0.054500\niteration 644 : loss : 0.218375, loss_ce: 0.049964\niteration 645 : loss : 0.220455, loss_ce: 0.057032\niteration 646 : loss : 0.215657, loss_ce: 0.039580\niteration 647 : loss : 0.235346, loss_ce: 0.040940\niteration 648 : loss : 0.239162, loss_ce: 0.030247\niteration 649 : loss : 0.214653, loss_ce: 0.038964\niteration 650 : loss : 0.287890, loss_ce: 0.035139\niteration 651 : loss : 0.453923, loss_ce: 0.018691\n","output_type":"stream"},{"name":"stderr","text":"  5%|█▍                             | 7/150 [11:19<3:48:55, 96.05s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 652 : loss : 0.289898, loss_ce: 0.120891\niteration 653 : loss : 0.233799, loss_ce: 0.059756\niteration 654 : loss : 0.214454, loss_ce: 0.037038\niteration 655 : loss : 0.208990, loss_ce: 0.048102\niteration 656 : loss : 0.217366, loss_ce: 0.056812\niteration 657 : loss : 0.214848, loss_ce: 0.049997\niteration 658 : loss : 0.229440, loss_ce: 0.047637\niteration 659 : loss : 0.249118, loss_ce: 0.052089\niteration 660 : loss : 0.220485, loss_ce: 0.041347\niteration 661 : loss : 0.224570, loss_ce: 0.050734\niteration 662 : loss : 0.214859, loss_ce: 0.039605\niteration 663 : loss : 0.214625, loss_ce: 0.046822\niteration 664 : loss : 0.216692, loss_ce: 0.031490\niteration 665 : loss : 0.221816, loss_ce: 0.044078\niteration 666 : loss : 0.235630, loss_ce: 0.069855\niteration 667 : loss : 0.214727, loss_ce: 0.048994\niteration 668 : loss : 0.233695, loss_ce: 0.057422\niteration 669 : loss : 0.221502, loss_ce: 0.067021\niteration 670 : loss : 0.225344, loss_ce: 0.039669\niteration 671 : loss : 0.207172, loss_ce: 0.061217\niteration 672 : loss : 0.204803, loss_ce: 0.039248\niteration 673 : loss : 0.246579, loss_ce: 0.044994\niteration 674 : loss : 0.253727, loss_ce: 0.086014\niteration 675 : loss : 0.215592, loss_ce: 0.048170\niteration 676 : loss : 0.230940, loss_ce: 0.039141\niteration 677 : loss : 0.212074, loss_ce: 0.046304\niteration 678 : loss : 0.233471, loss_ce: 0.044145\niteration 679 : loss : 0.228673, loss_ce: 0.051653\niteration 680 : loss : 0.231206, loss_ce: 0.034278\niteration 681 : loss : 0.192219, loss_ce: 0.035340\niteration 682 : loss : 0.228466, loss_ce: 0.032361\niteration 683 : loss : 0.196802, loss_ce: 0.044585\niteration 684 : loss : 0.213126, loss_ce: 0.057858\niteration 685 : loss : 0.195137, loss_ce: 0.044503\niteration 686 : loss : 0.226068, loss_ce: 0.033238\niteration 687 : loss : 0.214419, loss_ce: 0.033108\niteration 688 : loss : 0.240056, loss_ce: 0.059908\niteration 689 : loss : 0.208906, loss_ce: 0.028672\niteration 690 : loss : 0.221973, loss_ce: 0.038435\niteration 691 : loss : 0.191613, loss_ce: 0.030871\niteration 692 : loss : 0.235317, loss_ce: 0.043458\niteration 693 : loss : 0.240994, loss_ce: 0.080477\niteration 694 : loss : 0.230719, loss_ce: 0.044943\niteration 695 : loss : 0.207069, loss_ce: 0.041390\niteration 696 : loss : 0.237337, loss_ce: 0.058667\niteration 697 : loss : 0.200093, loss_ce: 0.046470\niteration 698 : loss : 0.229610, loss_ce: 0.045751\niteration 699 : loss : 0.207865, loss_ce: 0.052640\niteration 700 : loss : 0.244223, loss_ce: 0.074155\niteration 701 : loss : 0.190017, loss_ce: 0.049761\niteration 702 : loss : 0.224266, loss_ce: 0.052740\niteration 703 : loss : 0.186030, loss_ce: 0.055470\niteration 704 : loss : 0.189754, loss_ce: 0.046567\niteration 705 : loss : 0.216290, loss_ce: 0.044708\niteration 706 : loss : 0.184977, loss_ce: 0.044066\niteration 707 : loss : 0.187782, loss_ce: 0.034285\niteration 708 : loss : 0.191456, loss_ce: 0.041649\niteration 709 : loss : 0.173720, loss_ce: 0.039215\niteration 710 : loss : 0.237646, loss_ce: 0.085742\niteration 711 : loss : 0.190018, loss_ce: 0.048558\niteration 712 : loss : 0.187856, loss_ce: 0.048939\niteration 713 : loss : 0.223256, loss_ce: 0.035906\niteration 714 : loss : 0.217077, loss_ce: 0.062619\niteration 715 : loss : 0.202276, loss_ce: 0.057247\niteration 716 : loss : 0.188594, loss_ce: 0.042146\niteration 717 : loss : 0.194202, loss_ce: 0.040845\niteration 718 : loss : 0.200521, loss_ce: 0.043799\niteration 719 : loss : 0.202108, loss_ce: 0.049780\niteration 720 : loss : 0.234663, loss_ce: 0.041342\niteration 721 : loss : 0.200225, loss_ce: 0.048856\niteration 722 : loss : 0.205149, loss_ce: 0.054662\niteration 723 : loss : 0.202657, loss_ce: 0.055566\niteration 724 : loss : 0.198859, loss_ce: 0.043934\niteration 725 : loss : 0.181172, loss_ce: 0.044309\niteration 726 : loss : 0.191186, loss_ce: 0.037329\niteration 727 : loss : 0.174675, loss_ce: 0.042469\niteration 728 : loss : 0.175285, loss_ce: 0.031449\niteration 729 : loss : 0.182001, loss_ce: 0.035051\niteration 730 : loss : 0.193825, loss_ce: 0.071994\niteration 731 : loss : 0.208206, loss_ce: 0.048234\niteration 732 : loss : 0.176868, loss_ce: 0.038061\niteration 733 : loss : 0.178305, loss_ce: 0.046919\niteration 734 : loss : 0.170722, loss_ce: 0.031886\niteration 735 : loss : 0.191690, loss_ce: 0.052133\niteration 736 : loss : 0.194277, loss_ce: 0.075391\niteration 737 : loss : 0.173902, loss_ce: 0.039767\niteration 738 : loss : 0.167731, loss_ce: 0.043984\niteration 739 : loss : 0.148274, loss_ce: 0.042714\niteration 740 : loss : 0.149615, loss_ce: 0.055627\niteration 741 : loss : 0.166004, loss_ce: 0.059123\niteration 742 : loss : 0.137108, loss_ce: 0.041948\niteration 743 : loss : 0.140399, loss_ce: 0.030291\niteration 744 : loss : 0.367009, loss_ce: 0.044959\n","output_type":"stream"},{"name":"stderr","text":"  5%|█▋                             | 8/150 [12:54<3:47:08, 95.97s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 745 : loss : 0.136947, loss_ce: 0.037339\niteration 746 : loss : 0.165859, loss_ce: 0.037140\niteration 747 : loss : 0.175823, loss_ce: 0.058395\niteration 748 : loss : 0.161958, loss_ce: 0.065267\niteration 749 : loss : 0.151902, loss_ce: 0.051874\niteration 750 : loss : 0.145967, loss_ce: 0.041744\niteration 751 : loss : 0.157721, loss_ce: 0.043110\niteration 752 : loss : 0.185112, loss_ce: 0.031293\niteration 753 : loss : 0.173586, loss_ce: 0.049500\niteration 754 : loss : 0.169083, loss_ce: 0.030269\niteration 755 : loss : 0.122936, loss_ce: 0.029870\niteration 756 : loss : 0.156818, loss_ce: 0.057091\niteration 757 : loss : 0.190972, loss_ce: 0.051263\niteration 758 : loss : 0.188787, loss_ce: 0.045348\niteration 759 : loss : 0.205322, loss_ce: 0.042543\niteration 760 : loss : 0.161432, loss_ce: 0.033113\niteration 761 : loss : 0.179855, loss_ce: 0.057883\niteration 762 : loss : 0.194425, loss_ce: 0.045479\niteration 763 : loss : 0.172082, loss_ce: 0.049720\niteration 764 : loss : 0.138065, loss_ce: 0.042291\niteration 765 : loss : 0.155199, loss_ce: 0.042819\niteration 766 : loss : 0.150847, loss_ce: 0.041578\niteration 767 : loss : 0.195438, loss_ce: 0.054335\niteration 768 : loss : 0.177824, loss_ce: 0.042276\niteration 769 : loss : 0.173243, loss_ce: 0.035751\niteration 770 : loss : 0.147754, loss_ce: 0.037286\niteration 771 : loss : 0.159922, loss_ce: 0.033870\niteration 772 : loss : 0.189058, loss_ce: 0.053474\niteration 773 : loss : 0.179256, loss_ce: 0.027725\niteration 774 : loss : 0.164424, loss_ce: 0.048131\niteration 775 : loss : 0.181260, loss_ce: 0.045673\niteration 776 : loss : 0.199164, loss_ce: 0.052933\niteration 777 : loss : 0.171231, loss_ce: 0.070162\niteration 778 : loss : 0.144764, loss_ce: 0.046462\niteration 779 : loss : 0.159001, loss_ce: 0.050736\niteration 780 : loss : 0.209925, loss_ce: 0.070640\niteration 781 : loss : 0.207670, loss_ce: 0.060054\niteration 782 : loss : 0.162520, loss_ce: 0.057748\niteration 783 : loss : 0.155906, loss_ce: 0.040634\niteration 784 : loss : 0.137624, loss_ce: 0.038431\niteration 785 : loss : 0.128172, loss_ce: 0.033633\niteration 786 : loss : 0.139063, loss_ce: 0.039206\niteration 787 : loss : 0.148581, loss_ce: 0.024930\niteration 788 : loss : 0.147234, loss_ce: 0.052602\niteration 789 : loss : 0.199645, loss_ce: 0.036057\niteration 790 : loss : 0.175132, loss_ce: 0.045948\niteration 791 : loss : 0.148507, loss_ce: 0.039046\niteration 792 : loss : 0.135499, loss_ce: 0.049734\niteration 793 : loss : 0.167205, loss_ce: 0.049600\niteration 794 : loss : 0.154063, loss_ce: 0.065088\niteration 795 : loss : 0.193629, loss_ce: 0.047460\niteration 796 : loss : 0.153622, loss_ce: 0.041730\niteration 797 : loss : 0.159802, loss_ce: 0.053648\niteration 798 : loss : 0.146046, loss_ce: 0.055727\niteration 799 : loss : 0.126095, loss_ce: 0.046471\niteration 800 : loss : 0.120350, loss_ce: 0.040185\niteration 801 : loss : 0.193669, loss_ce: 0.048582\niteration 802 : loss : 0.128250, loss_ce: 0.028597\niteration 803 : loss : 0.190324, loss_ce: 0.037062\niteration 804 : loss : 0.129442, loss_ce: 0.035806\niteration 805 : loss : 0.140989, loss_ce: 0.051173\niteration 806 : loss : 0.165203, loss_ce: 0.030364\niteration 807 : loss : 0.155501, loss_ce: 0.031541\niteration 808 : loss : 0.178185, loss_ce: 0.035114\niteration 809 : loss : 0.142637, loss_ce: 0.034523\niteration 810 : loss : 0.173693, loss_ce: 0.054691\niteration 811 : loss : 0.170785, loss_ce: 0.053471\niteration 812 : loss : 0.147837, loss_ce: 0.036188\niteration 813 : loss : 0.171032, loss_ce: 0.072109\niteration 814 : loss : 0.175095, loss_ce: 0.040072\niteration 815 : loss : 0.118986, loss_ce: 0.040701\niteration 816 : loss : 0.180589, loss_ce: 0.058424\niteration 817 : loss : 0.147927, loss_ce: 0.045950\niteration 818 : loss : 0.160738, loss_ce: 0.034196\niteration 819 : loss : 0.138846, loss_ce: 0.049550\niteration 820 : loss : 0.149035, loss_ce: 0.065215\niteration 821 : loss : 0.141753, loss_ce: 0.049819\niteration 822 : loss : 0.135203, loss_ce: 0.043060\niteration 823 : loss : 0.191025, loss_ce: 0.070635\niteration 824 : loss : 0.179029, loss_ce: 0.042981\niteration 825 : loss : 0.165923, loss_ce: 0.042214\niteration 826 : loss : 0.168157, loss_ce: 0.051566\niteration 827 : loss : 0.098800, loss_ce: 0.040291\niteration 828 : loss : 0.110456, loss_ce: 0.046396\niteration 829 : loss : 0.154530, loss_ce: 0.061065\niteration 830 : loss : 0.176028, loss_ce: 0.043434\niteration 831 : loss : 0.163921, loss_ce: 0.037621\niteration 832 : loss : 0.156995, loss_ce: 0.042968\niteration 833 : loss : 0.217500, loss_ce: 0.034972\niteration 834 : loss : 0.171295, loss_ce: 0.055410\niteration 835 : loss : 0.169320, loss_ce: 0.064440\niteration 836 : loss : 0.146353, loss_ce: 0.032479\niteration 837 : loss : 0.227743, loss_ce: 0.033002\n","output_type":"stream"},{"name":"stderr","text":"  6%|█▊                             | 9/150 [14:30<3:45:24, 95.92s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 838 : loss : 0.150106, loss_ce: 0.043382\niteration 839 : loss : 0.160754, loss_ce: 0.057168\niteration 840 : loss : 0.155642, loss_ce: 0.057531\niteration 841 : loss : 0.174059, loss_ce: 0.048791\niteration 842 : loss : 0.158332, loss_ce: 0.050528\niteration 843 : loss : 0.176003, loss_ce: 0.071675\niteration 844 : loss : 0.154225, loss_ce: 0.036970\niteration 845 : loss : 0.170693, loss_ce: 0.051797\niteration 846 : loss : 0.149064, loss_ce: 0.056765\niteration 847 : loss : 0.145370, loss_ce: 0.029788\niteration 848 : loss : 0.140260, loss_ce: 0.044194\niteration 849 : loss : 0.171099, loss_ce: 0.036013\niteration 850 : loss : 0.159342, loss_ce: 0.043670\niteration 851 : loss : 0.134262, loss_ce: 0.041276\niteration 852 : loss : 0.154572, loss_ce: 0.064002\niteration 853 : loss : 0.154968, loss_ce: 0.031629\niteration 854 : loss : 0.157786, loss_ce: 0.038925\niteration 855 : loss : 0.140039, loss_ce: 0.051927\niteration 856 : loss : 0.145900, loss_ce: 0.047305\niteration 857 : loss : 0.132208, loss_ce: 0.030758\niteration 858 : loss : 0.096359, loss_ce: 0.026786\niteration 859 : loss : 0.119132, loss_ce: 0.037540\niteration 860 : loss : 0.117274, loss_ce: 0.030284\niteration 861 : loss : 0.164931, loss_ce: 0.054889\niteration 862 : loss : 0.092108, loss_ce: 0.024969\niteration 863 : loss : 0.104176, loss_ce: 0.038144\niteration 864 : loss : 0.114158, loss_ce: 0.040753\niteration 865 : loss : 0.124818, loss_ce: 0.046161\niteration 866 : loss : 0.111145, loss_ce: 0.039735\niteration 867 : loss : 0.158950, loss_ce: 0.036688\niteration 868 : loss : 0.161314, loss_ce: 0.044164\niteration 869 : loss : 0.108657, loss_ce: 0.039432\niteration 870 : loss : 0.098797, loss_ce: 0.034301\niteration 871 : loss : 0.166088, loss_ce: 0.036567\niteration 872 : loss : 0.179582, loss_ce: 0.022460\niteration 873 : loss : 0.121349, loss_ce: 0.032079\niteration 874 : loss : 0.137154, loss_ce: 0.031945\niteration 875 : loss : 0.113178, loss_ce: 0.030462\niteration 876 : loss : 0.116807, loss_ce: 0.035174\niteration 877 : loss : 0.136882, loss_ce: 0.042077\niteration 878 : loss : 0.127320, loss_ce: 0.050174\niteration 879 : loss : 0.114869, loss_ce: 0.040049\niteration 880 : loss : 0.200978, loss_ce: 0.038376\niteration 881 : loss : 0.109407, loss_ce: 0.035989\niteration 882 : loss : 0.131716, loss_ce: 0.037679\niteration 883 : loss : 0.143648, loss_ce: 0.030509\niteration 884 : loss : 0.133850, loss_ce: 0.044750\niteration 885 : loss : 0.094636, loss_ce: 0.024346\niteration 886 : loss : 0.130562, loss_ce: 0.022811\niteration 887 : loss : 0.130450, loss_ce: 0.040582\niteration 888 : loss : 0.144100, loss_ce: 0.033425\niteration 889 : loss : 0.116731, loss_ce: 0.044549\niteration 890 : loss : 0.132820, loss_ce: 0.048412\niteration 891 : loss : 0.147184, loss_ce: 0.039055\niteration 892 : loss : 0.102344, loss_ce: 0.025979\niteration 893 : loss : 0.157396, loss_ce: 0.042607\niteration 894 : loss : 0.120495, loss_ce: 0.038047\niteration 895 : loss : 0.181586, loss_ce: 0.051589\niteration 896 : loss : 0.133465, loss_ce: 0.046836\niteration 897 : loss : 0.150084, loss_ce: 0.038085\niteration 898 : loss : 0.129819, loss_ce: 0.028867\niteration 899 : loss : 0.114765, loss_ce: 0.045262\niteration 900 : loss : 0.127579, loss_ce: 0.038630\niteration 901 : loss : 0.137416, loss_ce: 0.042853\niteration 902 : loss : 0.133255, loss_ce: 0.037078\niteration 903 : loss : 0.158360, loss_ce: 0.035789\niteration 904 : loss : 0.115805, loss_ce: 0.040768\niteration 905 : loss : 0.113986, loss_ce: 0.039938\niteration 906 : loss : 0.141516, loss_ce: 0.050261\niteration 907 : loss : 0.109280, loss_ce: 0.037434\niteration 908 : loss : 0.138371, loss_ce: 0.025120\niteration 909 : loss : 0.129850, loss_ce: 0.049154\niteration 910 : loss : 0.109800, loss_ce: 0.027886\niteration 911 : loss : 0.125358, loss_ce: 0.051998\niteration 912 : loss : 0.089714, loss_ce: 0.026182\niteration 913 : loss : 0.109508, loss_ce: 0.032302\niteration 914 : loss : 0.103021, loss_ce: 0.025113\niteration 915 : loss : 0.128129, loss_ce: 0.028691\niteration 916 : loss : 0.148882, loss_ce: 0.042734\niteration 917 : loss : 0.162893, loss_ce: 0.030939\niteration 918 : loss : 0.119614, loss_ce: 0.037716\niteration 919 : loss : 0.143485, loss_ce: 0.028698\niteration 920 : loss : 0.094762, loss_ce: 0.019897\niteration 921 : loss : 0.144483, loss_ce: 0.032646\niteration 922 : loss : 0.169727, loss_ce: 0.040689\niteration 923 : loss : 0.151663, loss_ce: 0.046714\niteration 924 : loss : 0.202032, loss_ce: 0.038728\niteration 925 : loss : 0.154903, loss_ce: 0.030056\niteration 926 : loss : 0.105008, loss_ce: 0.034559\niteration 927 : loss : 0.143846, loss_ce: 0.039081\niteration 928 : loss : 0.107275, loss_ce: 0.025524\niteration 929 : loss : 0.139937, loss_ce: 0.033256\niteration 930 : loss : 0.190477, loss_ce: 0.032459\n","output_type":"stream"},{"name":"stderr","text":"  7%|██                            | 10/150 [16:07<3:44:05, 96.04s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 931 : loss : 0.129982, loss_ce: 0.034973\niteration 932 : loss : 0.159042, loss_ce: 0.055697\niteration 933 : loss : 0.142265, loss_ce: 0.038513\niteration 934 : loss : 0.108449, loss_ce: 0.031982\niteration 935 : loss : 0.119048, loss_ce: 0.027852\niteration 936 : loss : 0.146627, loss_ce: 0.039568\niteration 937 : loss : 0.147146, loss_ce: 0.030540\niteration 938 : loss : 0.097318, loss_ce: 0.035147\niteration 939 : loss : 0.155298, loss_ce: 0.021423\niteration 940 : loss : 0.117329, loss_ce: 0.033254\niteration 941 : loss : 0.121452, loss_ce: 0.050458\niteration 942 : loss : 0.162096, loss_ce: 0.050741\niteration 943 : loss : 0.095902, loss_ce: 0.026904\niteration 944 : loss : 0.180988, loss_ce: 0.033557\niteration 945 : loss : 0.124683, loss_ce: 0.042000\niteration 946 : loss : 0.137409, loss_ce: 0.053822\niteration 947 : loss : 0.126215, loss_ce: 0.030552\niteration 948 : loss : 0.134381, loss_ce: 0.055941\niteration 949 : loss : 0.096583, loss_ce: 0.023587\niteration 950 : loss : 0.144010, loss_ce: 0.032635\niteration 951 : loss : 0.100777, loss_ce: 0.039370\niteration 952 : loss : 0.145865, loss_ce: 0.027907\niteration 953 : loss : 0.134754, loss_ce: 0.036100\niteration 954 : loss : 0.169412, loss_ce: 0.036371\niteration 955 : loss : 0.159326, loss_ce: 0.035076\niteration 956 : loss : 0.135634, loss_ce: 0.037768\niteration 957 : loss : 0.115092, loss_ce: 0.044669\niteration 958 : loss : 0.127650, loss_ce: 0.033756\niteration 959 : loss : 0.119426, loss_ce: 0.047306\niteration 960 : loss : 0.154794, loss_ce: 0.026435\niteration 961 : loss : 0.096707, loss_ce: 0.025335\niteration 962 : loss : 0.132021, loss_ce: 0.023757\niteration 963 : loss : 0.137226, loss_ce: 0.029064\niteration 964 : loss : 0.085303, loss_ce: 0.027934\niteration 965 : loss : 0.143208, loss_ce: 0.044437\niteration 966 : loss : 0.122385, loss_ce: 0.039553\niteration 967 : loss : 0.095292, loss_ce: 0.039449\niteration 968 : loss : 0.141837, loss_ce: 0.038447\niteration 969 : loss : 0.122036, loss_ce: 0.038622\niteration 970 : loss : 0.114786, loss_ce: 0.027754\niteration 971 : loss : 0.124739, loss_ce: 0.035292\niteration 972 : loss : 0.092326, loss_ce: 0.027671\niteration 973 : loss : 0.104927, loss_ce: 0.031114\niteration 974 : loss : 0.090735, loss_ce: 0.026576\niteration 975 : loss : 0.128412, loss_ce: 0.031893\niteration 976 : loss : 0.118899, loss_ce: 0.038323\niteration 977 : loss : 0.142511, loss_ce: 0.035822\niteration 978 : loss : 0.100435, loss_ce: 0.044552\niteration 979 : loss : 0.157082, loss_ce: 0.036362\niteration 980 : loss : 0.121053, loss_ce: 0.040385\niteration 981 : loss : 0.110453, loss_ce: 0.038316\niteration 982 : loss : 0.086725, loss_ce: 0.027701\niteration 983 : loss : 0.117446, loss_ce: 0.030096\niteration 984 : loss : 0.110947, loss_ce: 0.024820\niteration 985 : loss : 0.099826, loss_ce: 0.023553\niteration 986 : loss : 0.110108, loss_ce: 0.046498\niteration 987 : loss : 0.153068, loss_ce: 0.027491\niteration 988 : loss : 0.134565, loss_ce: 0.038429\niteration 989 : loss : 0.120242, loss_ce: 0.022171\niteration 990 : loss : 0.108780, loss_ce: 0.040844\niteration 991 : loss : 0.138348, loss_ce: 0.024070\niteration 992 : loss : 0.112126, loss_ce: 0.051072\niteration 993 : loss : 0.132149, loss_ce: 0.036927\niteration 994 : loss : 0.108630, loss_ce: 0.028612\niteration 995 : loss : 0.141418, loss_ce: 0.024947\niteration 996 : loss : 0.122991, loss_ce: 0.033516\niteration 997 : loss : 0.125188, loss_ce: 0.046296\niteration 998 : loss : 0.128441, loss_ce: 0.039514\niteration 999 : loss : 0.094445, loss_ce: 0.036006\niteration 1000 : loss : 0.086452, loss_ce: 0.023419\niteration 1001 : loss : 0.095348, loss_ce: 0.036935\niteration 1002 : loss : 0.084578, loss_ce: 0.019124\niteration 1003 : loss : 0.097231, loss_ce: 0.040566\niteration 1004 : loss : 0.085451, loss_ce: 0.020074\niteration 1005 : loss : 0.136749, loss_ce: 0.027128\niteration 1006 : loss : 0.121283, loss_ce: 0.031972\niteration 1007 : loss : 0.113033, loss_ce: 0.036521\niteration 1008 : loss : 0.107500, loss_ce: 0.054202\niteration 1009 : loss : 0.159632, loss_ce: 0.032234\niteration 1010 : loss : 0.097479, loss_ce: 0.035908\niteration 1011 : loss : 0.122993, loss_ce: 0.038783\niteration 1012 : loss : 0.070777, loss_ce: 0.024931\niteration 1013 : loss : 0.154115, loss_ce: 0.039698\niteration 1014 : loss : 0.131826, loss_ce: 0.036256\niteration 1015 : loss : 0.126919, loss_ce: 0.030385\niteration 1016 : loss : 0.080983, loss_ce: 0.037713\niteration 1017 : loss : 0.128452, loss_ce: 0.035630\niteration 1018 : loss : 0.126563, loss_ce: 0.023951\niteration 1019 : loss : 0.116603, loss_ce: 0.028693\niteration 1020 : loss : 0.137604, loss_ce: 0.032497\niteration 1021 : loss : 0.110993, loss_ce: 0.028404\niteration 1022 : loss : 0.091008, loss_ce: 0.018055\niteration 1023 : loss : 0.349987, loss_ce: 0.022789\n","output_type":"stream"},{"name":"stderr","text":"  7%|██▏                           | 11/150 [17:42<3:42:24, 96.01s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 1024 : loss : 0.094427, loss_ce: 0.023612\niteration 1025 : loss : 0.111138, loss_ce: 0.037855\niteration 1026 : loss : 0.113290, loss_ce: 0.051798\niteration 1027 : loss : 0.118202, loss_ce: 0.027201\niteration 1028 : loss : 0.070143, loss_ce: 0.023847\niteration 1029 : loss : 0.108493, loss_ce: 0.036196\niteration 1030 : loss : 0.136916, loss_ce: 0.026895\niteration 1031 : loss : 0.104486, loss_ce: 0.033020\niteration 1032 : loss : 0.100553, loss_ce: 0.033135\niteration 1033 : loss : 0.094701, loss_ce: 0.031588\niteration 1034 : loss : 0.129411, loss_ce: 0.029433\niteration 1035 : loss : 0.114551, loss_ce: 0.029965\niteration 1036 : loss : 0.156917, loss_ce: 0.033662\niteration 1037 : loss : 0.106527, loss_ce: 0.023721\niteration 1038 : loss : 0.102790, loss_ce: 0.025489\niteration 1039 : loss : 0.090171, loss_ce: 0.025433\niteration 1040 : loss : 0.166927, loss_ce: 0.047841\niteration 1041 : loss : 0.113174, loss_ce: 0.036694\niteration 1042 : loss : 0.114447, loss_ce: 0.025223\niteration 1043 : loss : 0.111370, loss_ce: 0.036685\niteration 1044 : loss : 0.110987, loss_ce: 0.027785\niteration 1045 : loss : 0.088342, loss_ce: 0.026241\niteration 1046 : loss : 0.104102, loss_ce: 0.037665\niteration 1047 : loss : 0.140037, loss_ce: 0.043161\niteration 1048 : loss : 0.105534, loss_ce: 0.036276\niteration 1049 : loss : 0.096970, loss_ce: 0.031333\niteration 1050 : loss : 0.108821, loss_ce: 0.028331\niteration 1051 : loss : 0.097224, loss_ce: 0.037157\niteration 1052 : loss : 0.115227, loss_ce: 0.029762\niteration 1053 : loss : 0.107938, loss_ce: 0.039851\niteration 1054 : loss : 0.122024, loss_ce: 0.029756\niteration 1055 : loss : 0.158054, loss_ce: 0.088863\niteration 1056 : loss : 0.096519, loss_ce: 0.023835\niteration 1057 : loss : 0.117459, loss_ce: 0.028855\niteration 1058 : loss : 0.104094, loss_ce: 0.039777\niteration 1059 : loss : 0.091104, loss_ce: 0.031414\niteration 1060 : loss : 0.107272, loss_ce: 0.025270\niteration 1061 : loss : 0.081673, loss_ce: 0.018565\niteration 1062 : loss : 0.083895, loss_ce: 0.029776\niteration 1063 : loss : 0.126272, loss_ce: 0.031082\niteration 1064 : loss : 0.110861, loss_ce: 0.037279\niteration 1065 : loss : 0.103716, loss_ce: 0.035423\niteration 1066 : loss : 0.126055, loss_ce: 0.027902\niteration 1067 : loss : 0.134334, loss_ce: 0.039533\niteration 1068 : loss : 0.132336, loss_ce: 0.027311\niteration 1069 : loss : 0.174244, loss_ce: 0.021144\niteration 1070 : loss : 0.140186, loss_ce: 0.032907\niteration 1071 : loss : 0.091964, loss_ce: 0.033881\niteration 1072 : loss : 0.138381, loss_ce: 0.034428\niteration 1073 : loss : 0.106444, loss_ce: 0.041579\niteration 1074 : loss : 0.086605, loss_ce: 0.018409\niteration 1075 : loss : 0.096924, loss_ce: 0.022271\niteration 1076 : loss : 0.107381, loss_ce: 0.032530\niteration 1077 : loss : 0.117045, loss_ce: 0.040192\niteration 1078 : loss : 0.139107, loss_ce: 0.039954\niteration 1079 : loss : 0.131091, loss_ce: 0.040182\niteration 1080 : loss : 0.132296, loss_ce: 0.027350\niteration 1081 : loss : 0.101420, loss_ce: 0.023583\niteration 1082 : loss : 0.115252, loss_ce: 0.040870\niteration 1083 : loss : 0.132149, loss_ce: 0.030956\niteration 1084 : loss : 0.099792, loss_ce: 0.034179\niteration 1085 : loss : 0.100117, loss_ce: 0.037527\niteration 1086 : loss : 0.107004, loss_ce: 0.029384\niteration 1087 : loss : 0.095973, loss_ce: 0.044501\niteration 1088 : loss : 0.113285, loss_ce: 0.029209\niteration 1089 : loss : 0.111452, loss_ce: 0.031961\niteration 1090 : loss : 0.116275, loss_ce: 0.038313\niteration 1091 : loss : 0.105484, loss_ce: 0.037110\niteration 1092 : loss : 0.120212, loss_ce: 0.028213\niteration 1093 : loss : 0.105150, loss_ce: 0.034350\niteration 1094 : loss : 0.089672, loss_ce: 0.028426\niteration 1095 : loss : 0.083253, loss_ce: 0.025285\niteration 1096 : loss : 0.095232, loss_ce: 0.031488\niteration 1097 : loss : 0.106571, loss_ce: 0.024767\niteration 1098 : loss : 0.074513, loss_ce: 0.016956\niteration 1099 : loss : 0.109568, loss_ce: 0.026582\niteration 1100 : loss : 0.093662, loss_ce: 0.030033\niteration 1101 : loss : 0.117453, loss_ce: 0.028546\niteration 1102 : loss : 0.089695, loss_ce: 0.026325\niteration 1103 : loss : 0.128064, loss_ce: 0.027564\niteration 1104 : loss : 0.149131, loss_ce: 0.042229\niteration 1105 : loss : 0.105472, loss_ce: 0.025001\niteration 1106 : loss : 0.126750, loss_ce: 0.031552\niteration 1107 : loss : 0.126112, loss_ce: 0.037312\niteration 1108 : loss : 0.134832, loss_ce: 0.032810\niteration 1109 : loss : 0.113666, loss_ce: 0.030186\niteration 1110 : loss : 0.138343, loss_ce: 0.055487\niteration 1111 : loss : 0.112922, loss_ce: 0.037118\niteration 1112 : loss : 0.094887, loss_ce: 0.034655\niteration 1113 : loss : 0.136155, loss_ce: 0.037735\niteration 1114 : loss : 0.139576, loss_ce: 0.025248\niteration 1115 : loss : 0.065336, loss_ce: 0.020991\niteration 1116 : loss : 0.453894, loss_ce: 0.018665\n","output_type":"stream"},{"name":"stderr","text":"  8%|██▍                           | 12/150 [19:18<3:40:42, 95.96s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 1117 : loss : 0.116285, loss_ce: 0.041596\niteration 1118 : loss : 0.131549, loss_ce: 0.040366\niteration 1119 : loss : 0.129753, loss_ce: 0.032787\niteration 1120 : loss : 0.155048, loss_ce: 0.039121\niteration 1121 : loss : 0.121392, loss_ce: 0.022028\niteration 1122 : loss : 0.179943, loss_ce: 0.034212\niteration 1123 : loss : 0.138460, loss_ce: 0.027057\niteration 1124 : loss : 0.143652, loss_ce: 0.041380\niteration 1125 : loss : 0.113522, loss_ce: 0.017312\niteration 1126 : loss : 0.118989, loss_ce: 0.044269\niteration 1127 : loss : 0.089205, loss_ce: 0.030562\niteration 1128 : loss : 0.123458, loss_ce: 0.050698\niteration 1129 : loss : 0.133706, loss_ce: 0.039880\niteration 1130 : loss : 0.105876, loss_ce: 0.028307\niteration 1131 : loss : 0.110957, loss_ce: 0.033634\niteration 1132 : loss : 0.130270, loss_ce: 0.050809\niteration 1133 : loss : 0.097018, loss_ce: 0.033324\niteration 1134 : loss : 0.134028, loss_ce: 0.042237\niteration 1135 : loss : 0.111591, loss_ce: 0.026681\niteration 1136 : loss : 0.116574, loss_ce: 0.021551\niteration 1137 : loss : 0.104341, loss_ce: 0.029416\niteration 1138 : loss : 0.118234, loss_ce: 0.033191\niteration 1139 : loss : 0.146765, loss_ce: 0.041574\niteration 1140 : loss : 0.126965, loss_ce: 0.027063\niteration 1141 : loss : 0.115521, loss_ce: 0.030514\niteration 1142 : loss : 0.114556, loss_ce: 0.025751\niteration 1143 : loss : 0.080445, loss_ce: 0.022063\niteration 1144 : loss : 0.099999, loss_ce: 0.028431\niteration 1145 : loss : 0.132913, loss_ce: 0.030031\niteration 1146 : loss : 0.115668, loss_ce: 0.039828\niteration 1147 : loss : 0.091052, loss_ce: 0.029818\niteration 1148 : loss : 0.114632, loss_ce: 0.015232\niteration 1149 : loss : 0.088714, loss_ce: 0.024636\niteration 1150 : loss : 0.129982, loss_ce: 0.039050\niteration 1151 : loss : 0.135267, loss_ce: 0.027460\niteration 1152 : loss : 0.071199, loss_ce: 0.028859\niteration 1153 : loss : 0.129252, loss_ce: 0.037254\niteration 1154 : loss : 0.129125, loss_ce: 0.020139\niteration 1155 : loss : 0.102323, loss_ce: 0.035945\niteration 1156 : loss : 0.117583, loss_ce: 0.030239\niteration 1157 : loss : 0.083292, loss_ce: 0.019602\niteration 1158 : loss : 0.111845, loss_ce: 0.030663\niteration 1159 : loss : 0.103922, loss_ce: 0.042512\niteration 1160 : loss : 0.096819, loss_ce: 0.026488\niteration 1161 : loss : 0.079460, loss_ce: 0.036071\niteration 1162 : loss : 0.111691, loss_ce: 0.029055\niteration 1163 : loss : 0.109957, loss_ce: 0.040650\niteration 1164 : loss : 0.134581, loss_ce: 0.026650\niteration 1165 : loss : 0.118242, loss_ce: 0.021532\niteration 1166 : loss : 0.100320, loss_ce: 0.054484\niteration 1167 : loss : 0.124064, loss_ce: 0.030993\niteration 1168 : loss : 0.125873, loss_ce: 0.028600\niteration 1169 : loss : 0.098297, loss_ce: 0.039098\niteration 1170 : loss : 0.094437, loss_ce: 0.026065\niteration 1171 : loss : 0.093416, loss_ce: 0.034295\niteration 1172 : loss : 0.091940, loss_ce: 0.036320\niteration 1173 : loss : 0.114286, loss_ce: 0.047421\niteration 1174 : loss : 0.130998, loss_ce: 0.018692\niteration 1175 : loss : 0.128991, loss_ce: 0.031046\niteration 1176 : loss : 0.077153, loss_ce: 0.028170\niteration 1177 : loss : 0.108623, loss_ce: 0.018701\niteration 1178 : loss : 0.109813, loss_ce: 0.036106\niteration 1179 : loss : 0.086072, loss_ce: 0.027925\niteration 1180 : loss : 0.102362, loss_ce: 0.017196\niteration 1181 : loss : 0.083775, loss_ce: 0.021025\niteration 1182 : loss : 0.103159, loss_ce: 0.032394\niteration 1183 : loss : 0.099035, loss_ce: 0.026364\niteration 1184 : loss : 0.109150, loss_ce: 0.023481\niteration 1185 : loss : 0.122720, loss_ce: 0.054551\niteration 1186 : loss : 0.092744, loss_ce: 0.022317\niteration 1187 : loss : 0.102458, loss_ce: 0.029299\niteration 1188 : loss : 0.097306, loss_ce: 0.039021\niteration 1189 : loss : 0.127815, loss_ce: 0.039170\niteration 1190 : loss : 0.101562, loss_ce: 0.028362\niteration 1191 : loss : 0.106280, loss_ce: 0.024947\niteration 1192 : loss : 0.085615, loss_ce: 0.034844\niteration 1193 : loss : 0.093963, loss_ce: 0.037408\niteration 1194 : loss : 0.097160, loss_ce: 0.035231\niteration 1195 : loss : 0.059900, loss_ce: 0.025460\niteration 1196 : loss : 0.104294, loss_ce: 0.031753\niteration 1197 : loss : 0.066122, loss_ce: 0.021973\niteration 1198 : loss : 0.102313, loss_ce: 0.020315\niteration 1199 : loss : 0.081556, loss_ce: 0.037132\niteration 1200 : loss : 0.111964, loss_ce: 0.024131\niteration 1201 : loss : 0.094324, loss_ce: 0.025009\niteration 1202 : loss : 0.110342, loss_ce: 0.031950\niteration 1203 : loss : 0.085050, loss_ce: 0.032566\niteration 1204 : loss : 0.104122, loss_ce: 0.021485\niteration 1205 : loss : 0.091143, loss_ce: 0.023738\niteration 1206 : loss : 0.073053, loss_ce: 0.022762\niteration 1207 : loss : 0.132602, loss_ce: 0.030230\niteration 1208 : loss : 0.126351, loss_ce: 0.032645\niteration 1209 : loss : 0.386739, loss_ce: 0.013548\n","output_type":"stream"},{"name":"stderr","text":"  9%|██▌                           | 13/150 [20:54<3:39:09, 95.98s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 1210 : loss : 0.136330, loss_ce: 0.015177\niteration 1211 : loss : 0.107868, loss_ce: 0.039116\niteration 1212 : loss : 0.129249, loss_ce: 0.056281\niteration 1213 : loss : 0.110687, loss_ce: 0.029282\niteration 1214 : loss : 0.122437, loss_ce: 0.045592\niteration 1215 : loss : 0.124289, loss_ce: 0.033464\niteration 1216 : loss : 0.105416, loss_ce: 0.040415\niteration 1217 : loss : 0.163735, loss_ce: 0.068099\niteration 1218 : loss : 0.144643, loss_ce: 0.022441\niteration 1219 : loss : 0.105799, loss_ce: 0.041058\niteration 1220 : loss : 0.114757, loss_ce: 0.030290\niteration 1221 : loss : 0.117269, loss_ce: 0.029787\niteration 1222 : loss : 0.078191, loss_ce: 0.021466\niteration 1223 : loss : 0.105320, loss_ce: 0.033413\niteration 1224 : loss : 0.200185, loss_ce: 0.047339\niteration 1225 : loss : 0.109826, loss_ce: 0.030741\niteration 1226 : loss : 0.125776, loss_ce: 0.028867\niteration 1227 : loss : 0.121304, loss_ce: 0.036170\niteration 1228 : loss : 0.073944, loss_ce: 0.028209\niteration 1229 : loss : 0.083323, loss_ce: 0.031287\niteration 1230 : loss : 0.087166, loss_ce: 0.021732\niteration 1231 : loss : 0.099924, loss_ce: 0.036402\niteration 1232 : loss : 0.148581, loss_ce: 0.019193\niteration 1233 : loss : 0.082104, loss_ce: 0.027458\niteration 1234 : loss : 0.089882, loss_ce: 0.022111\niteration 1235 : loss : 0.074064, loss_ce: 0.016348\niteration 1236 : loss : 0.173517, loss_ce: 0.020506\niteration 1237 : loss : 0.087421, loss_ce: 0.023054\niteration 1238 : loss : 0.111002, loss_ce: 0.039175\niteration 1239 : loss : 0.074103, loss_ce: 0.020229\niteration 1240 : loss : 0.103161, loss_ce: 0.028722\niteration 1241 : loss : 0.076739, loss_ce: 0.033469\niteration 1242 : loss : 0.083668, loss_ce: 0.026416\niteration 1243 : loss : 0.087868, loss_ce: 0.023876\niteration 1244 : loss : 0.137408, loss_ce: 0.022052\niteration 1245 : loss : 0.123172, loss_ce: 0.038894\niteration 1246 : loss : 0.088666, loss_ce: 0.024980\niteration 1247 : loss : 0.113931, loss_ce: 0.029553\niteration 1248 : loss : 0.089442, loss_ce: 0.028410\niteration 1249 : loss : 0.101683, loss_ce: 0.025719\niteration 1250 : loss : 0.100099, loss_ce: 0.033457\niteration 1251 : loss : 0.088801, loss_ce: 0.025133\niteration 1252 : loss : 0.083722, loss_ce: 0.021823\niteration 1253 : loss : 0.099457, loss_ce: 0.023100\niteration 1254 : loss : 0.159234, loss_ce: 0.018456\niteration 1255 : loss : 0.079520, loss_ce: 0.031919\niteration 1256 : loss : 0.077843, loss_ce: 0.017786\niteration 1257 : loss : 0.091326, loss_ce: 0.030194\niteration 1258 : loss : 0.062575, loss_ce: 0.016784\niteration 1259 : loss : 0.114122, loss_ce: 0.026541\niteration 1260 : loss : 0.116189, loss_ce: 0.058107\niteration 1261 : loss : 0.116413, loss_ce: 0.032923\niteration 1262 : loss : 0.120418, loss_ce: 0.024520\niteration 1263 : loss : 0.101112, loss_ce: 0.029310\niteration 1264 : loss : 0.089180, loss_ce: 0.038952\niteration 1265 : loss : 0.126867, loss_ce: 0.035411\niteration 1266 : loss : 0.096303, loss_ce: 0.029384\niteration 1267 : loss : 0.080070, loss_ce: 0.027051\niteration 1268 : loss : 0.095349, loss_ce: 0.024862\niteration 1269 : loss : 0.095322, loss_ce: 0.026964\niteration 1270 : loss : 0.115503, loss_ce: 0.038745\niteration 1271 : loss : 0.103850, loss_ce: 0.031125\niteration 1272 : loss : 0.106231, loss_ce: 0.036455\niteration 1273 : loss : 0.078599, loss_ce: 0.024622\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### test.py\n使用测试集测试训练后的模型准确度","metadata":{}},{"cell_type":"code","source":"import argparse\nimport logging\nimport os\nimport random\nimport sys\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n# from datasets.dataset_synapse import Synapse_dataset\nfrom utils import test_single_volume\nfrom networks.vit_seg_modeling import VisionTransformer as ViT_seg\nfrom networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--volume_path', type=str,\n                    default='/kaggle/input/transeunet-synapse/data/Synapse/test_vol_h5', help='root dir for validation volume data')  # for acdc volume_path=root_dir\nparser.add_argument('--dataset', type=str,\n                    default='Synapse', help='experiment_name')\nparser.add_argument('--num_classes', type=int,\n                    default=4, help='output channel of network')\nparser.add_argument('--list_dir', type=str,\n                    default='/kaggle/input/transunet-lists/lists/lists_Synapse', help='list dir')\n\nparser.add_argument('--max_iterations', type=int,default=20000, help='maximum epoch number to train')\nparser.add_argument('--max_epochs', type=int, default=30, help='maximum epoch number to train')\nparser.add_argument('--batch_size', type=int, default=24,\n                    help='batch_size per gpu')\nparser.add_argument('--img_size', type=int, default=224, help='input patch size of network input')\nparser.add_argument('--is_savenii', action=\"store_true\", help='whether to save results during inference')\n\nparser.add_argument('--n_skip', type=int, default=3, help='using number of skip-connect, default is num')\nparser.add_argument('--vit_name', type=str, default='ViT-B_16', help='select one vit model')\n\nparser.add_argument('--test_save_dir', type=str, default='../predictions', help='saving prediction as nii!')\nparser.add_argument('--deterministic', type=int,  default=1, help='whether use deterministic training')\nparser.add_argument('--base_lr', type=float,  default=0.01, help='segmentation network learning rate')\nparser.add_argument('--seed', type=int, default=1234, help='random seed')\nparser.add_argument('--vit_patches_size', type=int, default=16, help='vit_patches_size, default is 16')\nargs = parser.parse_args()\n\n\ndef inference(args, model, test_save_path=None):\n    db_test = args.Dataset(base_dir=args.volume_path, split=\"test_vol\", list_dir=args.list_dir)\n    testloader = DataLoader(db_test, batch_size=1, shuffle=False, num_workers=1)\n    logging.info(\"{} test iterations per epoch\".format(len(testloader)))\n    model.eval()\n    metric_list = 0.0\n    for i_batch, sampled_batch in tqdm(enumerate(testloader)):\n        h, w = sampled_batch[\"image\"].size()[2:]\n        image, label, case_name = sampled_batch[\"image\"], sampled_batch[\"label\"], sampled_batch['case_name'][0]\n        metric_i = test_single_volume(image, label, model, classes=args.num_classes, patch_size=[args.img_size, args.img_size],\n                                      test_save_path=test_save_path, case=case_name, z_spacing=args.z_spacing)\n        metric_list += np.array(metric_i)\n        logging.info('idx %d case %s mean_dice %f mean_hd95 %f' % (i_batch, case_name, np.mean(metric_i, axis=0)[0], np.mean(metric_i, axis=0)[1]))\n    metric_list = metric_list / len(db_test)\n    for i in range(1, args.num_classes):\n        logging.info('Mean class %d mean_dice %f mean_hd95 %f' % (i, metric_list[i-1][0], metric_list[i-1][1]))\n    performance = np.mean(metric_list, axis=0)[0]\n    mean_hd95 = np.mean(metric_list, axis=0)[1]\n    logging.info('Testing performance in best val model: mean_dice : %f mean_hd95 : %f' % (performance, mean_hd95))\n    return \"Testing Finished!\"\n\n\nif __name__ == \"__main__\":\n\n    if not args.deterministic:\n        cudnn.benchmark = True\n        cudnn.deterministic = False\n    else:\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n\n    dataset_config = {\n        'Synapse': {\n            'Dataset': Synapse_dataset,\n            'volume_path': '../data/Synapse/test_vol_h5',\n            'list_dir': './lists/lists_Synapse',\n            'num_classes': 9,\n            'z_spacing': 1,\n        },\n    }\n    dataset_name = args.dataset\n    args.num_classes = dataset_config[dataset_name]['num_classes']\n    args.volume_path = dataset_config[dataset_name]['volume_path']\n    args.Dataset = dataset_config[dataset_name]['Dataset']\n    args.list_dir = dataset_config[dataset_name]['list_dir']\n    args.z_spacing = dataset_config[dataset_name]['z_spacing']\n    args.is_pretrain = True\n\n    # name the same snapshot defined in train script!\n    args.exp = 'TU_' + dataset_name + str(args.img_size)\n    snapshot_path = \"../model/{}/{}\".format(args.exp, 'TU')\n    snapshot_path = snapshot_path + '_pretrain' if args.is_pretrain else snapshot_path\n    snapshot_path += '_' + args.vit_name\n    snapshot_path = snapshot_path + '_skip' + str(args.n_skip)\n    snapshot_path = snapshot_path + '_vitpatch' + str(args.vit_patches_size) if args.vit_patches_size!=16 else snapshot_path\n    snapshot_path = snapshot_path + '_epo' + str(args.max_epochs) if args.max_epochs != 30 else snapshot_path\n    if dataset_name == 'ACDC':  # using max_epoch instead of iteration to control training duration\n        snapshot_path = snapshot_path + '_' + str(args.max_iterations)[0:2] + 'k' if args.max_iterations != 30000 else snapshot_path\n    snapshot_path = snapshot_path+'_bs'+str(args.batch_size)\n    snapshot_path = snapshot_path + '_lr' + str(args.base_lr) if args.base_lr != 0.01 else snapshot_path\n    snapshot_path = snapshot_path + '_'+str(args.img_size)\n    snapshot_path = snapshot_path + '_s'+str(args.seed) if args.seed!=1234 else snapshot_path\n\n    config_vit = CONFIGS_ViT_seg[args.vit_name]\n    config_vit.n_classes = args.num_classes\n    config_vit.n_skip = args.n_skip\n    config_vit.patches.size = (args.vit_patches_size, args.vit_patches_size)\n    if args.vit_name.find('R50') !=-1:\n        config_vit.patches.grid = (int(args.img_size/args.vit_patches_size), int(args.img_size/args.vit_patches_size))\n    net = ViT_seg(config_vit, img_size=args.img_size, num_classes=config_vit.n_classes).cuda()\n\n    snapshot = os.path.join(snapshot_path, 'best_model.pth')\n    if not os.path.exists(snapshot): snapshot = snapshot.replace('best_model', 'epoch_'+str(args.max_epochs-1))\n    net.load_state_dict(torch.load(snapshot))\n    snapshot_name = snapshot_path.split('/')[-1]\n\n    log_folder = './test_log/test_log_' + args.exp\n    os.makedirs(log_folder, exist_ok=True)\n    logging.basicConfig(filename=log_folder + '/'+snapshot_name+\".txt\", level=logging.INFO, format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.info(str(args))\n    logging.info(snapshot_name)\n\n    if args.is_savenii:\n        args.test_save_dir = '../predictions'\n        test_save_path = os.path.join(args.test_save_dir, args.exp, snapshot_name)\n        os.makedirs(test_save_path, exist_ok=True)\n    else:\n        test_save_path = None\n    inference(args, net, test_save_path)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}