{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TransUNet\n\nhttps://github.com/Beckschen/TransUNet","metadata":{}},{"cell_type":"markdown","source":"## 准备依赖环境","metadata":{}},{"cell_type":"markdown","source":"## 准备数据集\n参考 https://github.com/Beckschen/TransUNet/tree/main#2-prepare-data\n- transunet-synapse\n- transunet-lists\n","metadata":{}},{"cell_type":"markdown","source":"### transunet-lists","metadata":{}},{"cell_type":"code","source":"!mkdir -p lists/lists_Synapse/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile lists/lists_Synapse/all.lst\ncase0031.npy.h5\ncase0007.npy.h5\ncase0009.npy.h5\ncase0005.npy.h5\ncase0026.npy.h5\ncase0039.npy.h5\ncase0024.npy.h5\ncase0034.npy.h5\ncase0033.npy.h5\ncase0030.npy.h5\ncase0023.npy.h5\ncase0040.npy.h5\ncase0010.npy.h5\ncase0021.npy.h5\ncase0006.npy.h5\ncase0027.npy.h5\ncase0028.npy.h5\ncase0037.npy.h5\ncase0008.npy.h5\ncase0022.npy.h5\ncase0038.npy.h5\ncase0036.npy.h5\ncase0032.npy.h5\ncase0002.npy.h5\ncase0029.npy.h5\ncase0003.npy.h5\ncase0001.npy.h5\ncase0004.npy.h5\ncase0025.npy.h5\ncase0035.npy.h5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile lists/lists_Synapse/test_vol.txt\ncase0008\ncase0022\ncase0038\ncase0036\ncase0032\ncase0002\ncase0029\ncase0003\ncase0001\ncase0004\ncase0025\ncase0035","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile lists/lists_Synapse/train.txt\ncase0031_slice000\ncase0031_slice001\ncase0031_slice002\ncase0031_slice003\ncase0031_slice004\ncase0031_slice005\ncase0031_slice006\ncase0031_slice007\ncase0031_slice008\ncase0031_slice009\ncase0031_slice010\ncase0031_slice011\ncase0031_slice012\ncase0031_slice013\ncase0031_slice014\ncase0031_slice015\ncase0031_slice016\ncase0031_slice017\ncase0031_slice018\ncase0031_slice019\ncase0031_slice020\ncase0031_slice021\ncase0031_slice022\ncase0031_slice023\ncase0031_slice024\ncase0031_slice025\ncase0031_slice026\ncase0031_slice027\ncase0031_slice028\ncase0031_slice029\ncase0031_slice030\ncase0031_slice031\ncase0031_slice032\ncase0031_slice033\ncase0031_slice034\ncase0031_slice035\ncase0031_slice036\ncase0031_slice037\ncase0031_slice038\ncase0031_slice039\ncase0031_slice040\ncase0031_slice041\ncase0031_slice042\ncase0031_slice043\ncase0031_slice044\ncase0031_slice045\ncase0031_slice046\ncase0031_slice047\ncase0031_slice048\ncase0031_slice049\ncase0031_slice050\ncase0031_slice051\ncase0031_slice052\ncase0031_slice053\ncase0031_slice054\ncase0031_slice055\ncase0031_slice056\ncase0031_slice057\ncase0031_slice058\ncase0031_slice059\ncase0031_slice060\ncase0031_slice061\ncase0031_slice062\ncase0031_slice063\ncase0031_slice064\ncase0031_slice065\ncase0031_slice066\ncase0031_slice067\ncase0031_slice068\ncase0031_slice069\ncase0031_slice070\ncase0031_slice071\ncase0031_slice072\ncase0031_slice073\ncase0031_slice074\ncase0031_slice075\ncase0031_slice076\ncase0031_slice077\ncase0031_slice078\ncase0031_slice079\ncase0031_slice080\ncase0031_slice081\ncase0031_slice082\ncase0031_slice083\ncase0031_slice084\ncase0031_slice085\ncase0031_slice086\ncase0031_slice087\ncase0031_slice088\ncase0031_slice089\ncase0031_slice090\ncase0031_slice091\ncase0031_slice092\ncase0007_slice000\ncase0007_slice001\ncase0007_slice002\ncase0007_slice003\ncase0007_slice004\ncase0007_slice005\ncase0007_slice006\ncase0007_slice007\ncase0007_slice008\ncase0007_slice009\ncase0007_slice010\ncase0007_slice011\ncase0007_slice012\ncase0007_slice013\ncase0007_slice014\ncase0007_slice015\ncase0007_slice016\ncase0007_slice017\ncase0007_slice018\ncase0007_slice019\ncase0007_slice020\ncase0007_slice021\ncase0007_slice022\ncase0007_slice023\ncase0007_slice024\ncase0007_slice025\ncase0007_slice026\ncase0007_slice027\ncase0007_slice028\ncase0007_slice029\ncase0007_slice030\ncase0007_slice031\ncase0007_slice032\ncase0007_slice033\ncase0007_slice034\ncase0007_slice035\ncase0007_slice036\ncase0007_slice037\ncase0007_slice038\ncase0007_slice039\ncase0007_slice040\ncase0007_slice041\ncase0007_slice042\ncase0007_slice043\ncase0007_slice044\ncase0007_slice045\ncase0007_slice046\ncase0007_slice047\ncase0007_slice048\ncase0007_slice049\ncase0007_slice050\ncase0007_slice051\ncase0007_slice052\ncase0007_slice053\ncase0007_slice054\ncase0007_slice055\ncase0007_slice056\ncase0007_slice057\ncase0007_slice058\ncase0007_slice059\ncase0007_slice060\ncase0007_slice061\ncase0007_slice062\ncase0007_slice063\ncase0007_slice064\ncase0007_slice065\ncase0007_slice066\ncase0007_slice067\ncase0007_slice068\ncase0007_slice069\ncase0007_slice070\ncase0007_slice071\ncase0007_slice072\ncase0007_slice073\ncase0007_slice074\ncase0007_slice075\ncase0007_slice076\ncase0007_slice077\ncase0007_slice078\ncase0007_slice079\ncase0007_slice080\ncase0007_slice081\ncase0007_slice082\ncase0007_slice083\ncase0007_slice084\ncase0007_slice085\ncase0007_slice086\ncase0007_slice087\ncase0007_slice088\ncase0007_slice089\ncase0007_slice090\ncase0007_slice091\ncase0007_slice092\ncase0007_slice093\ncase0007_slice094\ncase0007_slice095\ncase0007_slice096\ncase0007_slice097\ncase0007_slice098\ncase0007_slice099\ncase0007_slice100\ncase0007_slice101\ncase0007_slice102\ncase0007_slice103\ncase0007_slice104\ncase0007_slice105\ncase0007_slice106\ncase0007_slice107\ncase0007_slice108\ncase0007_slice109\ncase0007_slice110\ncase0007_slice111\ncase0007_slice112\ncase0007_slice113\ncase0007_slice114\ncase0007_slice115\ncase0007_slice116\ncase0007_slice117\ncase0007_slice118\ncase0007_slice119\ncase0007_slice120\ncase0007_slice121\ncase0007_slice122\ncase0007_slice123\ncase0007_slice124\ncase0007_slice125\ncase0007_slice126\ncase0007_slice127\ncase0007_slice128\ncase0007_slice129\ncase0007_slice130\ncase0007_slice131\ncase0007_slice132\ncase0007_slice133\ncase0007_slice134\ncase0007_slice135\ncase0007_slice136\ncase0007_slice137\ncase0007_slice138\ncase0007_slice139\ncase0007_slice140\ncase0007_slice141\ncase0007_slice142\ncase0007_slice143\ncase0007_slice144\ncase0007_slice145\ncase0007_slice146\ncase0007_slice147\ncase0007_slice148\ncase0007_slice149\ncase0007_slice150\ncase0007_slice151\ncase0007_slice152\ncase0007_slice153\ncase0007_slice154\ncase0007_slice155\ncase0007_slice156\ncase0007_slice157\ncase0007_slice158\ncase0007_slice159\ncase0007_slice160\ncase0007_slice161\ncase0007_slice162\ncase0009_slice000\ncase0009_slice001\ncase0009_slice002\ncase0009_slice003\ncase0009_slice004\ncase0009_slice005\ncase0009_slice006\ncase0009_slice007\ncase0009_slice008\ncase0009_slice009\ncase0009_slice010\ncase0009_slice011\ncase0009_slice012\ncase0009_slice013\ncase0009_slice014\ncase0009_slice015\ncase0009_slice016\ncase0009_slice017\ncase0009_slice018\ncase0009_slice019\ncase0009_slice020\ncase0009_slice021\ncase0009_slice022\ncase0009_slice023\ncase0009_slice024\ncase0009_slice025\ncase0009_slice026\ncase0009_slice027\ncase0009_slice028\ncase0009_slice029\ncase0009_slice030\ncase0009_slice031\ncase0009_slice032\ncase0009_slice033\ncase0009_slice034\ncase0009_slice035\ncase0009_slice036\ncase0009_slice037\ncase0009_slice038\ncase0009_slice039\ncase0009_slice040\ncase0009_slice041\ncase0009_slice042\ncase0009_slice043\ncase0009_slice044\ncase0009_slice045\ncase0009_slice046\ncase0009_slice047\ncase0009_slice048\ncase0009_slice049\ncase0009_slice050\ncase0009_slice051\ncase0009_slice052\ncase0009_slice053\ncase0009_slice054\ncase0009_slice055\ncase0009_slice056\ncase0009_slice057\ncase0009_slice058\ncase0009_slice059\ncase0009_slice060\ncase0009_slice061\ncase0009_slice062\ncase0009_slice063\ncase0009_slice064\ncase0009_slice065\ncase0009_slice066\ncase0009_slice067\ncase0009_slice068\ncase0009_slice069\ncase0009_slice070\ncase0009_slice071\ncase0009_slice072\ncase0009_slice073\ncase0009_slice074\ncase0009_slice075\ncase0009_slice076\ncase0009_slice077\ncase0009_slice078\ncase0009_slice079\ncase0009_slice080\ncase0009_slice081\ncase0009_slice082\ncase0009_slice083\ncase0009_slice084\ncase0009_slice085\ncase0009_slice086\ncase0009_slice087\ncase0009_slice088\ncase0009_slice089\ncase0009_slice090\ncase0009_slice091\ncase0009_slice092\ncase0009_slice093\ncase0009_slice094\ncase0009_slice095\ncase0009_slice096\ncase0009_slice097\ncase0009_slice098\ncase0009_slice099\ncase0009_slice100\ncase0009_slice101\ncase0009_slice102\ncase0009_slice103\ncase0009_slice104\ncase0009_slice105\ncase0009_slice106\ncase0009_slice107\ncase0009_slice108\ncase0009_slice109\ncase0009_slice110\ncase0009_slice111\ncase0009_slice112\ncase0009_slice113\ncase0009_slice114\ncase0009_slice115\ncase0009_slice116\ncase0009_slice117\ncase0009_slice118\ncase0009_slice119\ncase0009_slice120\ncase0009_slice121\ncase0009_slice122\ncase0009_slice123\ncase0009_slice124\ncase0009_slice125\ncase0009_slice126\ncase0009_slice127\ncase0009_slice128\ncase0009_slice129\ncase0009_slice130\ncase0009_slice131\ncase0009_slice132\ncase0009_slice133\ncase0009_slice134\ncase0009_slice135\ncase0009_slice136\ncase0009_slice137\ncase0009_slice138\ncase0009_slice139\ncase0009_slice140\ncase0009_slice141\ncase0009_slice142\ncase0009_slice143\ncase0009_slice144\ncase0009_slice145\ncase0009_slice146\ncase0009_slice147\ncase0009_slice148\ncase0005_slice000\ncase0005_slice001\ncase0005_slice002\ncase0005_slice003\ncase0005_slice004\ncase0005_slice005\ncase0005_slice006\ncase0005_slice007\ncase0005_slice008\ncase0005_slice009\ncase0005_slice010\ncase0005_slice011\ncase0005_slice012\ncase0005_slice013\ncase0005_slice014\ncase0005_slice015\ncase0005_slice016\ncase0005_slice017\ncase0005_slice018\ncase0005_slice019\ncase0005_slice020\ncase0005_slice021\ncase0005_slice022\ncase0005_slice023\ncase0005_slice024\ncase0005_slice025\ncase0005_slice026\ncase0005_slice027\ncase0005_slice028\ncase0005_slice029\ncase0005_slice030\ncase0005_slice031\ncase0005_slice032\ncase0005_slice033\ncase0005_slice034\ncase0005_slice035\ncase0005_slice036\ncase0005_slice037\ncase0005_slice038\ncase0005_slice039\ncase0005_slice040\ncase0005_slice041\ncase0005_slice042\ncase0005_slice043\ncase0005_slice044\ncase0005_slice045\ncase0005_slice046\ncase0005_slice047\ncase0005_slice048\ncase0005_slice049\ncase0005_slice050\ncase0005_slice051\ncase0005_slice052\ncase0005_slice053\ncase0005_slice054\ncase0005_slice055\ncase0005_slice056\ncase0005_slice057\ncase0005_slice058\ncase0005_slice059\ncase0005_slice060\ncase0005_slice061\ncase0005_slice062\ncase0005_slice063\ncase0005_slice064\ncase0005_slice065\ncase0005_slice066\ncase0005_slice067\ncase0005_slice068\ncase0005_slice069\ncase0005_slice070\ncase0005_slice071\ncase0005_slice072\ncase0005_slice073\ncase0005_slice074\ncase0005_slice075\ncase0005_slice076\ncase0005_slice077\ncase0005_slice078\ncase0005_slice079\ncase0005_slice080\ncase0005_slice081\ncase0005_slice082\ncase0005_slice083\ncase0005_slice084\ncase0005_slice085\ncase0005_slice086\ncase0005_slice087\ncase0005_slice088\ncase0005_slice089\ncase0005_slice090\ncase0005_slice091\ncase0005_slice092\ncase0005_slice093\ncase0005_slice094\ncase0005_slice095\ncase0005_slice096\ncase0005_slice097\ncase0005_slice098\ncase0005_slice099\ncase0005_slice100\ncase0005_slice101\ncase0005_slice102\ncase0005_slice103\ncase0005_slice104\ncase0005_slice105\ncase0005_slice106\ncase0005_slice107\ncase0005_slice108\ncase0005_slice109\ncase0005_slice110\ncase0005_slice111\ncase0005_slice112\ncase0005_slice113\ncase0005_slice114\ncase0005_slice115\ncase0005_slice116\ncase0026_slice000\ncase0026_slice001\ncase0026_slice002\ncase0026_slice003\ncase0026_slice004\ncase0026_slice005\ncase0026_slice006\ncase0026_slice007\ncase0026_slice008\ncase0026_slice009\ncase0026_slice010\ncase0026_slice011\ncase0026_slice012\ncase0026_slice013\ncase0026_slice014\ncase0026_slice015\ncase0026_slice016\ncase0026_slice017\ncase0026_slice018\ncase0026_slice019\ncase0026_slice020\ncase0026_slice021\ncase0026_slice022\ncase0026_slice023\ncase0026_slice024\ncase0026_slice025\ncase0026_slice026\ncase0026_slice027\ncase0026_slice028\ncase0026_slice029\ncase0026_slice030\ncase0026_slice031\ncase0026_slice032\ncase0026_slice033\ncase0026_slice034\ncase0026_slice035\ncase0026_slice036\ncase0026_slice037\ncase0026_slice038\ncase0026_slice039\ncase0026_slice040\ncase0026_slice041\ncase0026_slice042\ncase0026_slice043\ncase0026_slice044\ncase0026_slice045\ncase0026_slice046\ncase0026_slice047\ncase0026_slice048\ncase0026_slice049\ncase0026_slice050\ncase0026_slice051\ncase0026_slice052\ncase0026_slice053\ncase0026_slice054\ncase0026_slice055\ncase0026_slice056\ncase0026_slice057\ncase0026_slice058\ncase0026_slice059\ncase0026_slice060\ncase0026_slice061\ncase0026_slice062\ncase0026_slice063\ncase0026_slice064\ncase0026_slice065\ncase0026_slice066\ncase0026_slice067\ncase0026_slice068\ncase0026_slice069\ncase0026_slice070\ncase0026_slice071\ncase0026_slice072\ncase0026_slice073\ncase0026_slice074\ncase0026_slice075\ncase0026_slice076\ncase0026_slice077\ncase0026_slice078\ncase0026_slice079\ncase0026_slice080\ncase0026_slice081\ncase0026_slice082\ncase0026_slice083\ncase0026_slice084\ncase0026_slice085\ncase0026_slice086\ncase0026_slice087\ncase0026_slice088\ncase0026_slice089\ncase0026_slice090\ncase0026_slice091\ncase0026_slice092\ncase0026_slice093\ncase0026_slice094\ncase0026_slice095\ncase0026_slice096\ncase0026_slice097\ncase0026_slice098\ncase0026_slice099\ncase0026_slice100\ncase0026_slice101\ncase0026_slice102\ncase0026_slice103\ncase0026_slice104\ncase0026_slice105\ncase0026_slice106\ncase0026_slice107\ncase0026_slice108\ncase0026_slice109\ncase0026_slice110\ncase0026_slice111\ncase0026_slice112\ncase0026_slice113\ncase0026_slice114\ncase0026_slice115\ncase0026_slice116\ncase0026_slice117\ncase0026_slice118\ncase0026_slice119\ncase0026_slice120\ncase0026_slice121\ncase0026_slice122\ncase0026_slice123\ncase0026_slice124\ncase0026_slice125\ncase0026_slice126\ncase0026_slice127\ncase0026_slice128\ncase0026_slice129\ncase0026_slice130\ncase0039_slice000\ncase0039_slice001\ncase0039_slice002\ncase0039_slice003\ncase0039_slice004\ncase0039_slice005\ncase0039_slice006\ncase0039_slice007\ncase0039_slice008\ncase0039_slice009\ncase0039_slice010\ncase0039_slice011\ncase0039_slice012\ncase0039_slice013\ncase0039_slice014\ncase0039_slice015\ncase0039_slice016\ncase0039_slice017\ncase0039_slice018\ncase0039_slice019\ncase0039_slice020\ncase0039_slice021\ncase0039_slice022\ncase0039_slice023\ncase0039_slice024\ncase0039_slice025\ncase0039_slice026\ncase0039_slice027\ncase0039_slice028\ncase0039_slice029\ncase0039_slice030\ncase0039_slice031\ncase0039_slice032\ncase0039_slice033\ncase0039_slice034\ncase0039_slice035\ncase0039_slice036\ncase0039_slice037\ncase0039_slice038\ncase0039_slice039\ncase0039_slice040\ncase0039_slice041\ncase0039_slice042\ncase0039_slice043\ncase0039_slice044\ncase0039_slice045\ncase0039_slice046\ncase0039_slice047\ncase0039_slice048\ncase0039_slice049\ncase0039_slice050\ncase0039_slice051\ncase0039_slice052\ncase0039_slice053\ncase0039_slice054\ncase0039_slice055\ncase0039_slice056\ncase0039_slice057\ncase0039_slice058\ncase0039_slice059\ncase0039_slice060\ncase0039_slice061\ncase0039_slice062\ncase0039_slice063\ncase0039_slice064\ncase0039_slice065\ncase0039_slice066\ncase0039_slice067\ncase0039_slice068\ncase0039_slice069\ncase0039_slice070\ncase0039_slice071\ncase0039_slice072\ncase0039_slice073\ncase0039_slice074\ncase0039_slice075\ncase0039_slice076\ncase0039_slice077\ncase0039_slice078\ncase0039_slice079\ncase0039_slice080\ncase0039_slice081\ncase0039_slice082\ncase0039_slice083\ncase0039_slice084\ncase0039_slice085\ncase0039_slice086\ncase0039_slice087\ncase0039_slice088\ncase0039_slice089\ncase0024_slice000\ncase0024_slice001\ncase0024_slice002\ncase0024_slice003\ncase0024_slice004\ncase0024_slice005\ncase0024_slice006\ncase0024_slice007\ncase0024_slice008\ncase0024_slice009\ncase0024_slice010\ncase0024_slice011\ncase0024_slice012\ncase0024_slice013\ncase0024_slice014\ncase0024_slice015\ncase0024_slice016\ncase0024_slice017\ncase0024_slice018\ncase0024_slice019\ncase0024_slice020\ncase0024_slice021\ncase0024_slice022\ncase0024_slice023\ncase0024_slice024\ncase0024_slice025\ncase0024_slice026\ncase0024_slice027\ncase0024_slice028\ncase0024_slice029\ncase0024_slice030\ncase0024_slice031\ncase0024_slice032\ncase0024_slice033\ncase0024_slice034\ncase0024_slice035\ncase0024_slice036\ncase0024_slice037\ncase0024_slice038\ncase0024_slice039\ncase0024_slice040\ncase0024_slice041\ncase0024_slice042\ncase0024_slice043\ncase0024_slice044\ncase0024_slice045\ncase0024_slice046\ncase0024_slice047\ncase0024_slice048\ncase0024_slice049\ncase0024_slice050\ncase0024_slice051\ncase0024_slice052\ncase0024_slice053\ncase0024_slice054\ncase0024_slice055\ncase0024_slice056\ncase0024_slice057\ncase0024_slice058\ncase0024_slice059\ncase0024_slice060\ncase0024_slice061\ncase0024_slice062\ncase0024_slice063\ncase0024_slice064\ncase0024_slice065\ncase0024_slice066\ncase0024_slice067\ncase0024_slice068\ncase0024_slice069\ncase0024_slice070\ncase0024_slice071\ncase0024_slice072\ncase0024_slice073\ncase0024_slice074\ncase0024_slice075\ncase0024_slice076\ncase0024_slice077\ncase0024_slice078\ncase0024_slice079\ncase0024_slice080\ncase0024_slice081\ncase0024_slice082\ncase0024_slice083\ncase0024_slice084\ncase0024_slice085\ncase0024_slice086\ncase0024_slice087\ncase0024_slice088\ncase0024_slice089\ncase0024_slice090\ncase0024_slice091\ncase0024_slice092\ncase0024_slice093\ncase0024_slice094\ncase0024_slice095\ncase0024_slice096\ncase0024_slice097\ncase0024_slice098\ncase0024_slice099\ncase0024_slice100\ncase0024_slice101\ncase0024_slice102\ncase0024_slice103\ncase0024_slice104\ncase0024_slice105\ncase0024_slice106\ncase0024_slice107\ncase0024_slice108\ncase0024_slice109\ncase0024_slice110\ncase0024_slice111\ncase0024_slice112\ncase0024_slice113\ncase0024_slice114\ncase0024_slice115\ncase0024_slice116\ncase0024_slice117\ncase0024_slice118\ncase0024_slice119\ncase0024_slice120\ncase0024_slice121\ncase0024_slice122\ncase0024_slice123\ncase0034_slice000\ncase0034_slice001\ncase0034_slice002\ncase0034_slice003\ncase0034_slice004\ncase0034_slice005\ncase0034_slice006\ncase0034_slice007\ncase0034_slice008\ncase0034_slice009\ncase0034_slice010\ncase0034_slice011\ncase0034_slice012\ncase0034_slice013\ncase0034_slice014\ncase0034_slice015\ncase0034_slice016\ncase0034_slice017\ncase0034_slice018\ncase0034_slice019\ncase0034_slice020\ncase0034_slice021\ncase0034_slice022\ncase0034_slice023\ncase0034_slice024\ncase0034_slice025\ncase0034_slice026\ncase0034_slice027\ncase0034_slice028\ncase0034_slice029\ncase0034_slice030\ncase0034_slice031\ncase0034_slice032\ncase0034_slice033\ncase0034_slice034\ncase0034_slice035\ncase0034_slice036\ncase0034_slice037\ncase0034_slice038\ncase0034_slice039\ncase0034_slice040\ncase0034_slice041\ncase0034_slice042\ncase0034_slice043\ncase0034_slice044\ncase0034_slice045\ncase0034_slice046\ncase0034_slice047\ncase0034_slice048\ncase0034_slice049\ncase0034_slice050\ncase0034_slice051\ncase0034_slice052\ncase0034_slice053\ncase0034_slice054\ncase0034_slice055\ncase0034_slice056\ncase0034_slice057\ncase0034_slice058\ncase0034_slice059\ncase0034_slice060\ncase0034_slice061\ncase0034_slice062\ncase0034_slice063\ncase0034_slice064\ncase0034_slice065\ncase0034_slice066\ncase0034_slice067\ncase0034_slice068\ncase0034_slice069\ncase0034_slice070\ncase0034_slice071\ncase0034_slice072\ncase0034_slice073\ncase0034_slice074\ncase0034_slice075\ncase0034_slice076\ncase0034_slice077\ncase0034_slice078\ncase0034_slice079\ncase0034_slice080\ncase0034_slice081\ncase0034_slice082\ncase0034_slice083\ncase0034_slice084\ncase0034_slice085\ncase0034_slice086\ncase0034_slice087\ncase0034_slice088\ncase0034_slice089\ncase0034_slice090\ncase0034_slice091\ncase0034_slice092\ncase0034_slice093\ncase0034_slice094\ncase0034_slice095\ncase0034_slice096\ncase0034_slice097\ncase0033_slice000\ncase0033_slice001\ncase0033_slice002\ncase0033_slice003\ncase0033_slice004\ncase0033_slice005\ncase0033_slice006\ncase0033_slice007\ncase0033_slice008\ncase0033_slice009\ncase0033_slice010\ncase0033_slice011\ncase0033_slice012\ncase0033_slice013\ncase0033_slice014\ncase0033_slice015\ncase0033_slice016\ncase0033_slice017\ncase0033_slice018\ncase0033_slice019\ncase0033_slice020\ncase0033_slice021\ncase0033_slice022\ncase0033_slice023\ncase0033_slice024\ncase0033_slice025\ncase0033_slice026\ncase0033_slice027\ncase0033_slice028\ncase0033_slice029\ncase0033_slice030\ncase0033_slice031\ncase0033_slice032\ncase0033_slice033\ncase0033_slice034\ncase0033_slice035\ncase0033_slice036\ncase0033_slice037\ncase0033_slice038\ncase0033_slice039\ncase0033_slice040\ncase0033_slice041\ncase0033_slice042\ncase0033_slice043\ncase0033_slice044\ncase0033_slice045\ncase0033_slice046\ncase0033_slice047\ncase0033_slice048\ncase0033_slice049\ncase0033_slice050\ncase0033_slice051\ncase0033_slice052\ncase0033_slice053\ncase0033_slice054\ncase0033_slice055\ncase0033_slice056\ncase0033_slice057\ncase0033_slice058\ncase0033_slice059\ncase0033_slice060\ncase0033_slice061\ncase0033_slice062\ncase0033_slice063\ncase0033_slice064\ncase0033_slice065\ncase0033_slice066\ncase0033_slice067\ncase0033_slice068\ncase0033_slice069\ncase0033_slice070\ncase0033_slice071\ncase0033_slice072\ncase0033_slice073\ncase0033_slice074\ncase0033_slice075\ncase0033_slice076\ncase0033_slice077\ncase0033_slice078\ncase0033_slice079\ncase0033_slice080\ncase0033_slice081\ncase0033_slice082\ncase0033_slice083\ncase0033_slice084\ncase0033_slice085\ncase0033_slice086\ncase0033_slice087\ncase0033_slice088\ncase0033_slice089\ncase0033_slice090\ncase0033_slice091\ncase0033_slice092\ncase0033_slice093\ncase0033_slice094\ncase0033_slice095\ncase0033_slice096\ncase0033_slice097\ncase0033_slice098\ncase0033_slice099\ncase0033_slice100\ncase0033_slice101\ncase0033_slice102\ncase0033_slice103\ncase0030_slice000\ncase0030_slice001\ncase0030_slice002\ncase0030_slice003\ncase0030_slice004\ncase0030_slice005\ncase0030_slice006\ncase0030_slice007\ncase0030_slice008\ncase0030_slice009\ncase0030_slice010\ncase0030_slice011\ncase0030_slice012\ncase0030_slice013\ncase0030_slice014\ncase0030_slice015\ncase0030_slice016\ncase0030_slice017\ncase0030_slice018\ncase0030_slice019\ncase0030_slice020\ncase0030_slice021\ncase0030_slice022\ncase0030_slice023\ncase0030_slice024\ncase0030_slice025\ncase0030_slice026\ncase0030_slice027\ncase0030_slice028\ncase0030_slice029\ncase0030_slice030\ncase0030_slice031\ncase0030_slice032\ncase0030_slice033\ncase0030_slice034\ncase0030_slice035\ncase0030_slice036\ncase0030_slice037\ncase0030_slice038\ncase0030_slice039\ncase0030_slice040\ncase0030_slice041\ncase0030_slice042\ncase0030_slice043\ncase0030_slice044\ncase0030_slice045\ncase0030_slice046\ncase0030_slice047\ncase0030_slice048\ncase0030_slice049\ncase0030_slice050\ncase0030_slice051\ncase0030_slice052\ncase0030_slice053\ncase0030_slice054\ncase0030_slice055\ncase0030_slice056\ncase0030_slice057\ncase0030_slice058\ncase0030_slice059\ncase0030_slice060\ncase0030_slice061\ncase0030_slice062\ncase0030_slice063\ncase0030_slice064\ncase0030_slice065\ncase0030_slice066\ncase0030_slice067\ncase0030_slice068\ncase0030_slice069\ncase0030_slice070\ncase0030_slice071\ncase0030_slice072\ncase0030_slice073\ncase0030_slice074\ncase0030_slice075\ncase0030_slice076\ncase0030_slice077\ncase0030_slice078\ncase0030_slice079\ncase0030_slice080\ncase0030_slice081\ncase0030_slice082\ncase0030_slice083\ncase0030_slice084\ncase0030_slice085\ncase0030_slice086\ncase0030_slice087\ncase0030_slice088\ncase0030_slice089\ncase0030_slice090\ncase0030_slice091\ncase0030_slice092\ncase0030_slice093\ncase0030_slice094\ncase0030_slice095\ncase0030_slice096\ncase0030_slice097\ncase0030_slice098\ncase0030_slice099\ncase0030_slice100\ncase0030_slice101\ncase0030_slice102\ncase0030_slice103\ncase0030_slice104\ncase0030_slice105\ncase0030_slice106\ncase0030_slice107\ncase0030_slice108\ncase0030_slice109\ncase0030_slice110\ncase0030_slice111\ncase0030_slice112\ncase0030_slice113\ncase0030_slice114\ncase0030_slice115\ncase0030_slice116\ncase0030_slice117\ncase0030_slice118\ncase0030_slice119\ncase0030_slice120\ncase0030_slice121\ncase0030_slice122\ncase0030_slice123\ncase0030_slice124\ncase0030_slice125\ncase0030_slice126\ncase0030_slice127\ncase0030_slice128\ncase0030_slice129\ncase0030_slice130\ncase0030_slice131\ncase0030_slice132\ncase0030_slice133\ncase0030_slice134\ncase0030_slice135\ncase0030_slice136\ncase0030_slice137\ncase0030_slice138\ncase0030_slice139\ncase0030_slice140\ncase0030_slice141\ncase0030_slice142\ncase0030_slice143\ncase0030_slice144\ncase0030_slice145\ncase0030_slice146\ncase0030_slice147\ncase0030_slice148\ncase0030_slice149\ncase0030_slice150\ncase0030_slice151\ncase0030_slice152\ncase0023_slice000\ncase0023_slice001\ncase0023_slice002\ncase0023_slice003\ncase0023_slice004\ncase0023_slice005\ncase0023_slice006\ncase0023_slice007\ncase0023_slice008\ncase0023_slice009\ncase0023_slice010\ncase0023_slice011\ncase0023_slice012\ncase0023_slice013\ncase0023_slice014\ncase0023_slice015\ncase0023_slice016\ncase0023_slice017\ncase0023_slice018\ncase0023_slice019\ncase0023_slice020\ncase0023_slice021\ncase0023_slice022\ncase0023_slice023\ncase0023_slice024\ncase0023_slice025\ncase0023_slice026\ncase0023_slice027\ncase0023_slice028\ncase0023_slice029\ncase0023_slice030\ncase0023_slice031\ncase0023_slice032\ncase0023_slice033\ncase0023_slice034\ncase0023_slice035\ncase0023_slice036\ncase0023_slice037\ncase0023_slice038\ncase0023_slice039\ncase0023_slice040\ncase0023_slice041\ncase0023_slice042\ncase0023_slice043\ncase0023_slice044\ncase0023_slice045\ncase0023_slice046\ncase0023_slice047\ncase0023_slice048\ncase0023_slice049\ncase0023_slice050\ncase0023_slice051\ncase0023_slice052\ncase0023_slice053\ncase0023_slice054\ncase0023_slice055\ncase0023_slice056\ncase0023_slice057\ncase0023_slice058\ncase0023_slice059\ncase0023_slice060\ncase0023_slice061\ncase0023_slice062\ncase0023_slice063\ncase0023_slice064\ncase0023_slice065\ncase0023_slice066\ncase0023_slice067\ncase0023_slice068\ncase0023_slice069\ncase0023_slice070\ncase0023_slice071\ncase0023_slice072\ncase0023_slice073\ncase0023_slice074\ncase0023_slice075\ncase0023_slice076\ncase0023_slice077\ncase0023_slice078\ncase0023_slice079\ncase0023_slice080\ncase0023_slice081\ncase0023_slice082\ncase0023_slice083\ncase0023_slice084\ncase0023_slice085\ncase0023_slice086\ncase0023_slice087\ncase0023_slice088\ncase0023_slice089\ncase0023_slice090\ncase0023_slice091\ncase0023_slice092\ncase0023_slice093\ncase0023_slice094\ncase0023_slice095\ncase0040_slice000\ncase0040_slice001\ncase0040_slice002\ncase0040_slice003\ncase0040_slice004\ncase0040_slice005\ncase0040_slice006\ncase0040_slice007\ncase0040_slice008\ncase0040_slice009\ncase0040_slice010\ncase0040_slice011\ncase0040_slice012\ncase0040_slice013\ncase0040_slice014\ncase0040_slice015\ncase0040_slice016\ncase0040_slice017\ncase0040_slice018\ncase0040_slice019\ncase0040_slice020\ncase0040_slice021\ncase0040_slice022\ncase0040_slice023\ncase0040_slice024\ncase0040_slice025\ncase0040_slice026\ncase0040_slice027\ncase0040_slice028\ncase0040_slice029\ncase0040_slice030\ncase0040_slice031\ncase0040_slice032\ncase0040_slice033\ncase0040_slice034\ncase0040_slice035\ncase0040_slice036\ncase0040_slice037\ncase0040_slice038\ncase0040_slice039\ncase0040_slice040\ncase0040_slice041\ncase0040_slice042\ncase0040_slice043\ncase0040_slice044\ncase0040_slice045\ncase0040_slice046\ncase0040_slice047\ncase0040_slice048\ncase0040_slice049\ncase0040_slice050\ncase0040_slice051\ncase0040_slice052\ncase0040_slice053\ncase0040_slice054\ncase0040_slice055\ncase0040_slice056\ncase0040_slice057\ncase0040_slice058\ncase0040_slice059\ncase0040_slice060\ncase0040_slice061\ncase0040_slice062\ncase0040_slice063\ncase0040_slice064\ncase0040_slice065\ncase0040_slice066\ncase0040_slice067\ncase0040_slice068\ncase0040_slice069\ncase0040_slice070\ncase0040_slice071\ncase0040_slice072\ncase0040_slice073\ncase0040_slice074\ncase0040_slice075\ncase0040_slice076\ncase0040_slice077\ncase0040_slice078\ncase0040_slice079\ncase0040_slice080\ncase0040_slice081\ncase0040_slice082\ncase0040_slice083\ncase0040_slice084\ncase0040_slice085\ncase0040_slice086\ncase0040_slice087\ncase0040_slice088\ncase0040_slice089\ncase0040_slice090\ncase0040_slice091\ncase0040_slice092\ncase0040_slice093\ncase0040_slice094\ncase0040_slice095\ncase0040_slice096\ncase0040_slice097\ncase0040_slice098\ncase0040_slice099\ncase0040_slice100\ncase0040_slice101\ncase0040_slice102\ncase0040_slice103\ncase0040_slice104\ncase0040_slice105\ncase0040_slice106\ncase0040_slice107\ncase0040_slice108\ncase0040_slice109\ncase0040_slice110\ncase0040_slice111\ncase0040_slice112\ncase0040_slice113\ncase0040_slice114\ncase0040_slice115\ncase0040_slice116\ncase0040_slice117\ncase0040_slice118\ncase0040_slice119\ncase0040_slice120\ncase0040_slice121\ncase0040_slice122\ncase0040_slice123\ncase0040_slice124\ncase0040_slice125\ncase0040_slice126\ncase0040_slice127\ncase0040_slice128\ncase0040_slice129\ncase0040_slice130\ncase0040_slice131\ncase0040_slice132\ncase0040_slice133\ncase0040_slice134\ncase0040_slice135\ncase0040_slice136\ncase0040_slice137\ncase0040_slice138\ncase0040_slice139\ncase0040_slice140\ncase0040_slice141\ncase0040_slice142\ncase0040_slice143\ncase0040_slice144\ncase0040_slice145\ncase0040_slice146\ncase0040_slice147\ncase0040_slice148\ncase0040_slice149\ncase0040_slice150\ncase0040_slice151\ncase0040_slice152\ncase0040_slice153\ncase0040_slice154\ncase0040_slice155\ncase0040_slice156\ncase0040_slice157\ncase0040_slice158\ncase0040_slice159\ncase0040_slice160\ncase0040_slice161\ncase0040_slice162\ncase0040_slice163\ncase0040_slice164\ncase0040_slice165\ncase0040_slice166\ncase0040_slice167\ncase0040_slice168\ncase0040_slice169\ncase0040_slice170\ncase0040_slice171\ncase0040_slice172\ncase0040_slice173\ncase0040_slice174\ncase0040_slice175\ncase0040_slice176\ncase0040_slice177\ncase0040_slice178\ncase0040_slice179\ncase0040_slice180\ncase0040_slice181\ncase0040_slice182\ncase0040_slice183\ncase0040_slice184\ncase0040_slice185\ncase0040_slice186\ncase0040_slice187\ncase0040_slice188\ncase0040_slice189\ncase0040_slice190\ncase0040_slice191\ncase0040_slice192\ncase0040_slice193\ncase0040_slice194\ncase0010_slice000\ncase0010_slice001\ncase0010_slice002\ncase0010_slice003\ncase0010_slice004\ncase0010_slice005\ncase0010_slice006\ncase0010_slice007\ncase0010_slice008\ncase0010_slice009\ncase0010_slice010\ncase0010_slice011\ncase0010_slice012\ncase0010_slice013\ncase0010_slice014\ncase0010_slice015\ncase0010_slice016\ncase0010_slice017\ncase0010_slice018\ncase0010_slice019\ncase0010_slice020\ncase0010_slice021\ncase0010_slice022\ncase0010_slice023\ncase0010_slice024\ncase0010_slice025\ncase0010_slice026\ncase0010_slice027\ncase0010_slice028\ncase0010_slice029\ncase0010_slice030\ncase0010_slice031\ncase0010_slice032\ncase0010_slice033\ncase0010_slice034\ncase0010_slice035\ncase0010_slice036\ncase0010_slice037\ncase0010_slice038\ncase0010_slice039\ncase0010_slice040\ncase0010_slice041\ncase0010_slice042\ncase0010_slice043\ncase0010_slice044\ncase0010_slice045\ncase0010_slice046\ncase0010_slice047\ncase0010_slice048\ncase0010_slice049\ncase0010_slice050\ncase0010_slice051\ncase0010_slice052\ncase0010_slice053\ncase0010_slice054\ncase0010_slice055\ncase0010_slice056\ncase0010_slice057\ncase0010_slice058\ncase0010_slice059\ncase0010_slice060\ncase0010_slice061\ncase0010_slice062\ncase0010_slice063\ncase0010_slice064\ncase0010_slice065\ncase0010_slice066\ncase0010_slice067\ncase0010_slice068\ncase0010_slice069\ncase0010_slice070\ncase0010_slice071\ncase0010_slice072\ncase0010_slice073\ncase0010_slice074\ncase0010_slice075\ncase0010_slice076\ncase0010_slice077\ncase0010_slice078\ncase0010_slice079\ncase0010_slice080\ncase0010_slice081\ncase0010_slice082\ncase0010_slice083\ncase0010_slice084\ncase0010_slice085\ncase0010_slice086\ncase0010_slice087\ncase0010_slice088\ncase0010_slice089\ncase0010_slice090\ncase0010_slice091\ncase0010_slice092\ncase0010_slice093\ncase0010_slice094\ncase0010_slice095\ncase0010_slice096\ncase0010_slice097\ncase0010_slice098\ncase0010_slice099\ncase0010_slice100\ncase0010_slice101\ncase0010_slice102\ncase0010_slice103\ncase0010_slice104\ncase0010_slice105\ncase0010_slice106\ncase0010_slice107\ncase0010_slice108\ncase0010_slice109\ncase0010_slice110\ncase0010_slice111\ncase0010_slice112\ncase0010_slice113\ncase0010_slice114\ncase0010_slice115\ncase0010_slice116\ncase0010_slice117\ncase0010_slice118\ncase0010_slice119\ncase0010_slice120\ncase0010_slice121\ncase0010_slice122\ncase0010_slice123\ncase0010_slice124\ncase0010_slice125\ncase0010_slice126\ncase0010_slice127\ncase0010_slice128\ncase0010_slice129\ncase0010_slice130\ncase0010_slice131\ncase0010_slice132\ncase0010_slice133\ncase0010_slice134\ncase0010_slice135\ncase0010_slice136\ncase0010_slice137\ncase0010_slice138\ncase0010_slice139\ncase0010_slice140\ncase0010_slice141\ncase0010_slice142\ncase0010_slice143\ncase0010_slice144\ncase0010_slice145\ncase0010_slice146\ncase0010_slice147\ncase0021_slice000\ncase0021_slice001\ncase0021_slice002\ncase0021_slice003\ncase0021_slice004\ncase0021_slice005\ncase0021_slice006\ncase0021_slice007\ncase0021_slice008\ncase0021_slice009\ncase0021_slice010\ncase0021_slice011\ncase0021_slice012\ncase0021_slice013\ncase0021_slice014\ncase0021_slice015\ncase0021_slice016\ncase0021_slice017\ncase0021_slice018\ncase0021_slice019\ncase0021_slice020\ncase0021_slice021\ncase0021_slice022\ncase0021_slice023\ncase0021_slice024\ncase0021_slice025\ncase0021_slice026\ncase0021_slice027\ncase0021_slice028\ncase0021_slice029\ncase0021_slice030\ncase0021_slice031\ncase0021_slice032\ncase0021_slice033\ncase0021_slice034\ncase0021_slice035\ncase0021_slice036\ncase0021_slice037\ncase0021_slice038\ncase0021_slice039\ncase0021_slice040\ncase0021_slice041\ncase0021_slice042\ncase0021_slice043\ncase0021_slice044\ncase0021_slice045\ncase0021_slice046\ncase0021_slice047\ncase0021_slice048\ncase0021_slice049\ncase0021_slice050\ncase0021_slice051\ncase0021_slice052\ncase0021_slice053\ncase0021_slice054\ncase0021_slice055\ncase0021_slice056\ncase0021_slice057\ncase0021_slice058\ncase0021_slice059\ncase0021_slice060\ncase0021_slice061\ncase0021_slice062\ncase0021_slice063\ncase0021_slice064\ncase0021_slice065\ncase0021_slice066\ncase0021_slice067\ncase0021_slice068\ncase0021_slice069\ncase0021_slice070\ncase0021_slice071\ncase0021_slice072\ncase0021_slice073\ncase0021_slice074\ncase0021_slice075\ncase0021_slice076\ncase0021_slice077\ncase0021_slice078\ncase0021_slice079\ncase0021_slice080\ncase0021_slice081\ncase0021_slice082\ncase0021_slice083\ncase0021_slice084\ncase0021_slice085\ncase0021_slice086\ncase0021_slice087\ncase0021_slice088\ncase0021_slice089\ncase0021_slice090\ncase0021_slice091\ncase0021_slice092\ncase0021_slice093\ncase0021_slice094\ncase0021_slice095\ncase0021_slice096\ncase0021_slice097\ncase0021_slice098\ncase0021_slice099\ncase0021_slice100\ncase0021_slice101\ncase0021_slice102\ncase0021_slice103\ncase0021_slice104\ncase0021_slice105\ncase0021_slice106\ncase0021_slice107\ncase0021_slice108\ncase0021_slice109\ncase0021_slice110\ncase0021_slice111\ncase0021_slice112\ncase0021_slice113\ncase0021_slice114\ncase0021_slice115\ncase0021_slice116\ncase0021_slice117\ncase0021_slice118\ncase0021_slice119\ncase0021_slice120\ncase0021_slice121\ncase0021_slice122\ncase0021_slice123\ncase0021_slice124\ncase0021_slice125\ncase0021_slice126\ncase0021_slice127\ncase0021_slice128\ncase0021_slice129\ncase0021_slice130\ncase0021_slice131\ncase0021_slice132\ncase0021_slice133\ncase0021_slice134\ncase0021_slice135\ncase0021_slice136\ncase0021_slice137\ncase0021_slice138\ncase0021_slice139\ncase0021_slice140\ncase0021_slice141\ncase0021_slice142\ncase0006_slice000\ncase0006_slice001\ncase0006_slice002\ncase0006_slice003\ncase0006_slice004\ncase0006_slice005\ncase0006_slice006\ncase0006_slice007\ncase0006_slice008\ncase0006_slice009\ncase0006_slice010\ncase0006_slice011\ncase0006_slice012\ncase0006_slice013\ncase0006_slice014\ncase0006_slice015\ncase0006_slice016\ncase0006_slice017\ncase0006_slice018\ncase0006_slice019\ncase0006_slice020\ncase0006_slice021\ncase0006_slice022\ncase0006_slice023\ncase0006_slice024\ncase0006_slice025\ncase0006_slice026\ncase0006_slice027\ncase0006_slice028\ncase0006_slice029\ncase0006_slice030\ncase0006_slice031\ncase0006_slice032\ncase0006_slice033\ncase0006_slice034\ncase0006_slice035\ncase0006_slice036\ncase0006_slice037\ncase0006_slice038\ncase0006_slice039\ncase0006_slice040\ncase0006_slice041\ncase0006_slice042\ncase0006_slice043\ncase0006_slice044\ncase0006_slice045\ncase0006_slice046\ncase0006_slice047\ncase0006_slice048\ncase0006_slice049\ncase0006_slice050\ncase0006_slice051\ncase0006_slice052\ncase0006_slice053\ncase0006_slice054\ncase0006_slice055\ncase0006_slice056\ncase0006_slice057\ncase0006_slice058\ncase0006_slice059\ncase0006_slice060\ncase0006_slice061\ncase0006_slice062\ncase0006_slice063\ncase0006_slice064\ncase0006_slice065\ncase0006_slice066\ncase0006_slice067\ncase0006_slice068\ncase0006_slice069\ncase0006_slice070\ncase0006_slice071\ncase0006_slice072\ncase0006_slice073\ncase0006_slice074\ncase0006_slice075\ncase0006_slice076\ncase0006_slice077\ncase0006_slice078\ncase0006_slice079\ncase0006_slice080\ncase0006_slice081\ncase0006_slice082\ncase0006_slice083\ncase0006_slice084\ncase0006_slice085\ncase0006_slice086\ncase0006_slice087\ncase0006_slice088\ncase0006_slice089\ncase0006_slice090\ncase0006_slice091\ncase0006_slice092\ncase0006_slice093\ncase0006_slice094\ncase0006_slice095\ncase0006_slice096\ncase0006_slice097\ncase0006_slice098\ncase0006_slice099\ncase0006_slice100\ncase0006_slice101\ncase0006_slice102\ncase0006_slice103\ncase0006_slice104\ncase0006_slice105\ncase0006_slice106\ncase0006_slice107\ncase0006_slice108\ncase0006_slice109\ncase0006_slice110\ncase0006_slice111\ncase0006_slice112\ncase0006_slice113\ncase0006_slice114\ncase0006_slice115\ncase0006_slice116\ncase0006_slice117\ncase0006_slice118\ncase0006_slice119\ncase0006_slice120\ncase0006_slice121\ncase0006_slice122\ncase0006_slice123\ncase0006_slice124\ncase0006_slice125\ncase0006_slice126\ncase0006_slice127\ncase0006_slice128\ncase0006_slice129\ncase0006_slice130\ncase0027_slice000\ncase0027_slice001\ncase0027_slice002\ncase0027_slice003\ncase0027_slice004\ncase0027_slice005\ncase0027_slice006\ncase0027_slice007\ncase0027_slice008\ncase0027_slice009\ncase0027_slice010\ncase0027_slice011\ncase0027_slice012\ncase0027_slice013\ncase0027_slice014\ncase0027_slice015\ncase0027_slice016\ncase0027_slice017\ncase0027_slice018\ncase0027_slice019\ncase0027_slice020\ncase0027_slice021\ncase0027_slice022\ncase0027_slice023\ncase0027_slice024\ncase0027_slice025\ncase0027_slice026\ncase0027_slice027\ncase0027_slice028\ncase0027_slice029\ncase0027_slice030\ncase0027_slice031\ncase0027_slice032\ncase0027_slice033\ncase0027_slice034\ncase0027_slice035\ncase0027_slice036\ncase0027_slice037\ncase0027_slice038\ncase0027_slice039\ncase0027_slice040\ncase0027_slice041\ncase0027_slice042\ncase0027_slice043\ncase0027_slice044\ncase0027_slice045\ncase0027_slice046\ncase0027_slice047\ncase0027_slice048\ncase0027_slice049\ncase0027_slice050\ncase0027_slice051\ncase0027_slice052\ncase0027_slice053\ncase0027_slice054\ncase0027_slice055\ncase0027_slice056\ncase0027_slice057\ncase0027_slice058\ncase0027_slice059\ncase0027_slice060\ncase0027_slice061\ncase0027_slice062\ncase0027_slice063\ncase0027_slice064\ncase0027_slice065\ncase0027_slice066\ncase0027_slice067\ncase0027_slice068\ncase0027_slice069\ncase0027_slice070\ncase0027_slice071\ncase0027_slice072\ncase0027_slice073\ncase0027_slice074\ncase0027_slice075\ncase0027_slice076\ncase0027_slice077\ncase0027_slice078\ncase0027_slice079\ncase0027_slice080\ncase0027_slice081\ncase0027_slice082\ncase0027_slice083\ncase0027_slice084\ncase0027_slice085\ncase0027_slice086\ncase0027_slice087\ncase0028_slice000\ncase0028_slice001\ncase0028_slice002\ncase0028_slice003\ncase0028_slice004\ncase0028_slice005\ncase0028_slice006\ncase0028_slice007\ncase0028_slice008\ncase0028_slice009\ncase0028_slice010\ncase0028_slice011\ncase0028_slice012\ncase0028_slice013\ncase0028_slice014\ncase0028_slice015\ncase0028_slice016\ncase0028_slice017\ncase0028_slice018\ncase0028_slice019\ncase0028_slice020\ncase0028_slice021\ncase0028_slice022\ncase0028_slice023\ncase0028_slice024\ncase0028_slice025\ncase0028_slice026\ncase0028_slice027\ncase0028_slice028\ncase0028_slice029\ncase0028_slice030\ncase0028_slice031\ncase0028_slice032\ncase0028_slice033\ncase0028_slice034\ncase0028_slice035\ncase0028_slice036\ncase0028_slice037\ncase0028_slice038\ncase0028_slice039\ncase0028_slice040\ncase0028_slice041\ncase0028_slice042\ncase0028_slice043\ncase0028_slice044\ncase0028_slice045\ncase0028_slice046\ncase0028_slice047\ncase0028_slice048\ncase0028_slice049\ncase0028_slice050\ncase0028_slice051\ncase0028_slice052\ncase0028_slice053\ncase0028_slice054\ncase0028_slice055\ncase0028_slice056\ncase0028_slice057\ncase0028_slice058\ncase0028_slice059\ncase0028_slice060\ncase0028_slice061\ncase0028_slice062\ncase0028_slice063\ncase0028_slice064\ncase0028_slice065\ncase0028_slice066\ncase0028_slice067\ncase0028_slice068\ncase0028_slice069\ncase0028_slice070\ncase0028_slice071\ncase0028_slice072\ncase0028_slice073\ncase0028_slice074\ncase0028_slice075\ncase0028_slice076\ncase0028_slice077\ncase0028_slice078\ncase0028_slice079\ncase0028_slice080\ncase0028_slice081\ncase0028_slice082\ncase0028_slice083\ncase0028_slice084\ncase0028_slice085\ncase0028_slice086\ncase0028_slice087\ncase0028_slice088\ncase0037_slice000\ncase0037_slice001\ncase0037_slice002\ncase0037_slice003\ncase0037_slice004\ncase0037_slice005\ncase0037_slice006\ncase0037_slice007\ncase0037_slice008\ncase0037_slice009\ncase0037_slice010\ncase0037_slice011\ncase0037_slice012\ncase0037_slice013\ncase0037_slice014\ncase0037_slice015\ncase0037_slice016\ncase0037_slice017\ncase0037_slice018\ncase0037_slice019\ncase0037_slice020\ncase0037_slice021\ncase0037_slice022\ncase0037_slice023\ncase0037_slice024\ncase0037_slice025\ncase0037_slice026\ncase0037_slice027\ncase0037_slice028\ncase0037_slice029\ncase0037_slice030\ncase0037_slice031\ncase0037_slice032\ncase0037_slice033\ncase0037_slice034\ncase0037_slice035\ncase0037_slice036\ncase0037_slice037\ncase0037_slice038\ncase0037_slice039\ncase0037_slice040\ncase0037_slice041\ncase0037_slice042\ncase0037_slice043\ncase0037_slice044\ncase0037_slice045\ncase0037_slice046\ncase0037_slice047\ncase0037_slice048\ncase0037_slice049\ncase0037_slice050\ncase0037_slice051\ncase0037_slice052\ncase0037_slice053\ncase0037_slice054\ncase0037_slice055\ncase0037_slice056\ncase0037_slice057\ncase0037_slice058\ncase0037_slice059\ncase0037_slice060\ncase0037_slice061\ncase0037_slice062\ncase0037_slice063\ncase0037_slice064\ncase0037_slice065\ncase0037_slice066\ncase0037_slice067\ncase0037_slice068\ncase0037_slice069\ncase0037_slice070\ncase0037_slice071\ncase0037_slice072\ncase0037_slice073\ncase0037_slice074\ncase0037_slice075\ncase0037_slice076\ncase0037_slice077\ncase0037_slice078\ncase0037_slice079\ncase0037_slice080\ncase0037_slice081\ncase0037_slice082\ncase0037_slice083\ncase0037_slice084\ncase0037_slice085\ncase0037_slice086\ncase0037_slice087\ncase0037_slice088\ncase0037_slice089\ncase0037_slice090\ncase0037_slice091\ncase0037_slice092\ncase0037_slice093\ncase0037_slice094\ncase0037_slice095\ncase0037_slice096\ncase0037_slice097\ncase0037_slice098","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch torchvision numpy scipy tqdm tensorboard tensorboardX ml-collections medpy SimpleITK h5py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 下载Google imagenet 21预训练模型\n\n关于ViT预训练模型简介：\n- https://github.com/google-research/vision_transformer\n- https://zhuanlan.zhihu.com/p/445122996","metadata":{}},{"cell_type":"code","source":"# !mkdir model\n# !gsutil -m cp \\\n#   \"gs://vit_models/imagenet21k/R26+ViT-B_32.npz\" \\\n#   \"gs://vit_models/imagenet21k/R50+ViT-B_16.npz\" \\\n#   \"gs://vit_models/imagenet21k/R50+ViT-L_32.npz\" \\\n#   \"gs://vit_models/imagenet21k/ViT-B_16.npz\" \\\n#   \"gs://vit_models/imagenet21k/ViT-B_32.npz\" \\\n#   \"gs://vit_models/imagenet21k/ViT-B_8.npz\" \\\n#   \"gs://vit_models/imagenet21k/ViT-H_14.npz\" \\\n#   \"gs://vit_models/imagenet21k/ViT-L_16.npz\" \\\n#   \"gs://vit_models/imagenet21k/ViT-L_32.npz\" \\\n#   ./model\n\n# 下载一次就够了","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### dataset.py\n数据集处理工具，实现pytorch的DataSet类，用于加载训练集、标签和测试数据","metadata":{}},{"cell_type":"code","source":"!mkdir -p dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile dataset/dataset_synapse.py\nimport os\nimport random\nimport h5py\nimport numpy as np\nimport torch\nfrom scipy import ndimage\nfrom scipy.ndimage.interpolation import zoom\nfrom torch.utils.data import Dataset\n\n\ndef random_rot_flip(image, label):\n    k = np.random.randint(0, 4)\n    image = np.rot90(image, k)\n    label = np.rot90(label, k)\n    axis = np.random.randint(0, 2)\n    image = np.flip(image, axis=axis).copy()\n    label = np.flip(label, axis=axis).copy()\n    return image, label\n\n\ndef random_rotate(image, label):\n    angle = np.random.randint(-20, 20)\n    image = ndimage.rotate(image, angle, order=0, reshape=False)\n    label = ndimage.rotate(label, angle, order=0, reshape=False)\n    return image, label\n\n\nclass RandomGenerator(object):\n    def __init__(self, output_size):\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['label']\n\n        if random.random() > 0.5:\n            image, label = random_rot_flip(image, label)\n        elif random.random() > 0.5:\n            image, label = random_rotate(image, label)\n        x, y = image.shape\n        if x != self.output_size[0] or y != self.output_size[1]:\n            image = zoom(image, (self.output_size[0] / x, self.output_size[1] / y), order=3)  # why not 3?\n            label = zoom(label, (self.output_size[0] / x, self.output_size[1] / y), order=0)\n        image = torch.from_numpy(image.astype(np.float32)).unsqueeze(0)\n        label = torch.from_numpy(label.astype(np.float32))\n        sample = {'image': image, 'label': label.long()}\n        return sample\n\n\nclass Synapse_dataset(Dataset):\n    def __init__(self, base_dir, list_dir, split, transform=None):\n        self.transform = transform  # using transform in torch!\n        self.split = split\n        self.sample_list = open(os.path.join(list_dir, self.split+'.txt')).readlines()\n        self.data_dir = base_dir\n\n    def __len__(self):\n        return len(self.sample_list)\n\n    def __getitem__(self, idx):\n        if self.split == \"train\":\n            slice_name = self.sample_list[idx].strip('\\n')\n            data_path = os.path.join(self.data_dir, slice_name+'.npz')\n            data = np.load(data_path)\n            image, label = data['image'], data['label']\n        else:\n            vol_name = self.sample_list[idx].strip('\\n')\n            filepath = self.data_dir + \"/{}.npy.h5\".format(vol_name)\n            data = h5py.File(filepath)\n            image, label = data['image'][:], data['label'][:]\n\n        sample = {'image': image, 'label': label}\n        if self.transform:\n            sample = self.transform(sample)\n        sample['case_name'] = self.sample_list[idx].strip('\\n')\n        return sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### utils.py\n\n一些工具方法\n- DiceLoss 损失函数： https://zhuanlan.zhihu.com/p/86704421\n","metadata":{}},{"cell_type":"code","source":"%%writefile utils.py\nimport numpy as np\nimport torch\nfrom medpy import metric\nfrom scipy.ndimage import zoom\nimport torch.nn as nn\nimport SimpleITK as sitk\n\n\nclass DiceLoss(nn.Module):\n    def __init__(self, n_classes):\n        super(DiceLoss, self).__init__()\n        self.n_classes = n_classes\n\n    def _one_hot_encoder(self, input_tensor):\n        tensor_list = []\n        for i in range(self.n_classes):\n            temp_prob = input_tensor == i  # * torch.ones_like(input_tensor)\n            tensor_list.append(temp_prob.unsqueeze(1))\n        output_tensor = torch.cat(tensor_list, dim=1)\n        return output_tensor.float()\n\n    def _dice_loss(self, score, target):\n        target = target.float()\n        smooth = 1e-5\n        intersect = torch.sum(score * target)\n        y_sum = torch.sum(target * target)\n        z_sum = torch.sum(score * score)\n        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n        loss = 1 - loss\n        return loss\n\n    def forward(self, inputs, target, weight=None, softmax=False):\n        if softmax:\n            inputs = torch.softmax(inputs, dim=1)\n        target = self._one_hot_encoder(target)\n        if weight is None:\n            weight = [1] * self.n_classes\n        assert inputs.size() == target.size(), 'predict {} & target {} shape do not match'.format(inputs.size(), target.size())\n        class_wise_dice = []\n        loss = 0.0\n        for i in range(0, self.n_classes):\n            dice = self._dice_loss(inputs[:, i], target[:, i])\n            class_wise_dice.append(1.0 - dice.item())\n            loss += dice * weight[i]\n        return loss / self.n_classes\n\n\ndef calculate_metric_percase(pred, gt):\n    pred[pred > 0] = 1\n    gt[gt > 0] = 1\n    if pred.sum() > 0 and gt.sum()>0:\n        dice = metric.binary.dc(pred, gt)\n        hd95 = metric.binary.hd95(pred, gt)\n        return dice, hd95\n    elif pred.sum() > 0 and gt.sum()==0:\n        return 1, 0\n    else:\n        return 0, 0\n\n\ndef test_single_volume(image, label, net, classes, patch_size=[256, 256], test_save_path=None, case=None, z_spacing=1):\n    image, label = image.squeeze(0).cpu().detach().numpy(), label.squeeze(0).cpu().detach().numpy()\n    if len(image.shape) == 3:\n        prediction = np.zeros_like(label)\n        for ind in range(image.shape[0]):\n            slice = image[ind, :, :]\n            x, y = slice.shape[0], slice.shape[1]\n            if x != patch_size[0] or y != patch_size[1]:\n                slice = zoom(slice, (patch_size[0] / x, patch_size[1] / y), order=3)  # previous using 0\n            input = torch.from_numpy(slice).unsqueeze(0).unsqueeze(0).float().cuda()\n            net.eval()\n            with torch.no_grad():\n                outputs = net(input)\n                out = torch.argmax(torch.softmax(outputs, dim=1), dim=1).squeeze(0)\n                out = out.cpu().detach().numpy()\n                if x != patch_size[0] or y != patch_size[1]:\n                    pred = zoom(out, (x / patch_size[0], y / patch_size[1]), order=0)\n                else:\n                    pred = out\n                prediction[ind] = pred\n    else:\n        input = torch.from_numpy(image).unsqueeze(\n            0).unsqueeze(0).float().cuda()\n        net.eval()\n        with torch.no_grad():\n            out = torch.argmax(torch.softmax(net(input), dim=1), dim=1).squeeze(0)\n            prediction = out.cpu().detach().numpy()\n    metric_list = []\n    for i in range(1, classes):\n        metric_list.append(calculate_metric_percase(prediction == i, label == i))\n\n    if test_save_path is not None:\n        img_itk = sitk.GetImageFromArray(image.astype(np.float32))\n        prd_itk = sitk.GetImageFromArray(prediction.astype(np.float32))\n        lab_itk = sitk.GetImageFromArray(label.astype(np.float32))\n        img_itk.SetSpacing((1, 1, z_spacing))\n        prd_itk.SetSpacing((1, 1, z_spacing))\n        lab_itk.SetSpacing((1, 1, z_spacing))\n        sitk.WriteImage(prd_itk, test_save_path + '/'+case + \"_pred.nii.gz\")\n        sitk.WriteImage(img_itk, test_save_path + '/'+ case + \"_img.nii.gz\")\n        sitk.WriteImage(lab_itk, test_save_path + '/'+ case + \"_gt.nii.gz\")\n    return metric_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## networks\n获取预训练模型的配置，用于在训练之前预先加载该模型","metadata":{}},{"cell_type":"markdown","source":"### vit_seg_configs.py","metadata":{}},{"cell_type":"code","source":"!mkdir networks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile networks/vit_seg_configs.py\nimport ml_collections\n\ndef get_b16_config():\n    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 768\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 3072\n    config.transformer.num_heads = 12\n    config.transformer.num_layers = 12\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n\n    config.classifier = 'seg'\n    config.representation_size = None\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '/kaggle/working/model/ViT-B_16.npz'\n    config.patch_size = 16\n\n    config.decoder_channels = (256, 128, 64, 16)\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_testing():\n    \"\"\"Returns a minimal configuration for testing.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 1\n    config.transformer.num_heads = 1\n    config.transformer.num_layers = 1\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config\n\ndef get_r50_b16_config():\n    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.pretrained_path = '/kaggle/working/model/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 2\n    config.n_skip = 3\n    config.activation = 'softmax'\n\n    return config\n\n\ndef get_b32_config():\n    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.size = (32, 32)\n    config.pretrained_path = '/kaggle/working/model/ViT-B_32.npz'\n    return config\n\n\ndef get_l16_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1024\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 4096\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 24\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.representation_size = None\n\n    # custom\n    config.classifier = 'seg'\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '/kaggle/working/model/ViT-L_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_r50_l16_config():\n    \"\"\"Returns the Resnet50 + ViT-L/16 configuration. customized \"\"\"\n    config = get_l16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.resnet_pretrained_path = '/kaggle/working/model/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_l32_config():\n    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n    config = get_l16_config()\n    config.patches.size = (32, 32)\n    return config\n\n\ndef get_h14_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n    config.hidden_size = 1280\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 5120\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 32\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n\n    return config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### vit_seg_modeling.py\n\n网络模型定义","metadata":{}},{"cell_type":"code","source":"%%writefile networks/vit_seg_modeling.py\n# coding=utf-8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\nimport math\n\nfrom os.path import join as pjoin\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nfrom torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\nfrom torch.nn.modules.utils import _pair\nfrom scipy import ndimage\nfrom . import vit_seg_configs as configs\nfrom .vit_seg_modeling_resnet_skip import ResNetV2\n\n\nlogger = logging.getLogger(__name__)\n\n\nATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\nATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\nATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\nATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\nFC_0 = \"MlpBlock_3/Dense_0\"\nFC_1 = \"MlpBlock_3/Dense_1\"\nATTENTION_NORM = \"LayerNorm_0\"\nMLP_NORM = \"LayerNorm_2\"\n\n\ndef np2th(weights, conv=False):\n    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n    if conv:\n        weights = weights.transpose([3, 2, 0, 1])\n    return torch.from_numpy(weights)\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n\n\nclass Attention(nn.Module):\n    def __init__(self, config, vis):\n        super(Attention, self).__init__()\n        self.vis = vis\n        self.num_attention_heads = config.transformer[\"num_heads\"]\n        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = Linear(config.hidden_size, self.all_head_size)\n        self.key = Linear(config.hidden_size, self.all_head_size)\n        self.value = Linear(config.hidden_size, self.all_head_size)\n\n        self.out = Linear(config.hidden_size, config.hidden_size)\n        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n\n        self.softmax = Softmax(dim=-1)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        attention_probs = self.softmax(attention_scores)\n        weights = attention_probs if self.vis else None\n        attention_probs = self.attn_dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        attention_output = self.out(context_layer)\n        attention_output = self.proj_dropout(attention_output)\n        return attention_output, weights\n\n\nclass Mlp(nn.Module):\n    def __init__(self, config):\n        super(Mlp, self).__init__()\n        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n        self.act_fn = ACT2FN[\"gelu\"]\n        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n\n        self._init_weights()\n\n    def _init_weights(self):\n        nn.init.xavier_uniform_(self.fc1.weight)\n        nn.init.xavier_uniform_(self.fc2.weight)\n        nn.init.normal_(self.fc1.bias, std=1e-6)\n        nn.init.normal_(self.fc2.bias, std=1e-6)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act_fn(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Embeddings(nn.Module):\n    \"\"\"Construct the embeddings from patch, position embeddings.\n    \"\"\"\n    def __init__(self, config, img_size, in_channels=3):\n        super(Embeddings, self).__init__()\n        self.hybrid = None\n        self.config = config\n        img_size = _pair(img_size)\n\n        if config.patches.get(\"grid\") is not None:   # ResNet\n            grid_size = config.patches[\"grid\"]\n            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n            patch_size_real = (patch_size[0] * 16, patch_size[1] * 16)\n            n_patches = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1])  \n            self.hybrid = True\n        else:\n            patch_size = _pair(config.patches[\"size\"])\n            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n            self.hybrid = False\n\n        if self.hybrid:\n            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers, width_factor=config.resnet.width_factor)\n            in_channels = self.hybrid_model.width * 16\n        self.patch_embeddings = Conv2d(in_channels=in_channels,\n                                       out_channels=config.hidden_size,\n                                       kernel_size=patch_size,\n                                       stride=patch_size)\n        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, config.hidden_size))\n\n        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n\n\n    def forward(self, x):\n        if self.hybrid:\n            x, features = self.hybrid_model(x)\n        else:\n            features = None\n        x = self.patch_embeddings(x)  # (B, hidden. n_patches^(1/2), n_patches^(1/2))\n        x = x.flatten(2)\n        x = x.transpose(-1, -2)  # (B, n_patches, hidden)\n\n        embeddings = x + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n        return embeddings, features\n\n\nclass Block(nn.Module):\n    def __init__(self, config, vis):\n        super(Block, self).__init__()\n        self.hidden_size = config.hidden_size\n        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        self.ffn = Mlp(config)\n        self.attn = Attention(config, vis)\n\n    def forward(self, x):\n        h = x\n        x = self.attention_norm(x)\n        x, weights = self.attn(x)\n        x = x + h\n\n        h = x\n        x = self.ffn_norm(x)\n        x = self.ffn(x)\n        x = x + h\n        return x, weights\n\n    def load_from(self, weights, n_block):\n        ROOT = f\"Transformer/encoderblock_{n_block}\"\n        with torch.no_grad():\n            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n\n            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n\n            self.attn.query.weight.copy_(query_weight)\n            self.attn.key.weight.copy_(key_weight)\n            self.attn.value.weight.copy_(value_weight)\n            self.attn.out.weight.copy_(out_weight)\n            self.attn.query.bias.copy_(query_bias)\n            self.attn.key.bias.copy_(key_bias)\n            self.attn.value.bias.copy_(value_bias)\n            self.attn.out.bias.copy_(out_bias)\n\n            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n\n            self.ffn.fc1.weight.copy_(mlp_weight_0)\n            self.ffn.fc2.weight.copy_(mlp_weight_1)\n            self.ffn.fc1.bias.copy_(mlp_bias_0)\n            self.ffn.fc2.bias.copy_(mlp_bias_1)\n\n            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n\n\nclass Encoder(nn.Module):\n    def __init__(self, config, vis):\n        super(Encoder, self).__init__()\n        self.vis = vis\n        self.layer = nn.ModuleList()\n        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        for _ in range(config.transformer[\"num_layers\"]):\n            layer = Block(config, vis)\n            self.layer.append(copy.deepcopy(layer))\n\n    def forward(self, hidden_states):\n        attn_weights = []\n        for layer_block in self.layer:\n            hidden_states, weights = layer_block(hidden_states)\n            if self.vis:\n                attn_weights.append(weights)\n        encoded = self.encoder_norm(hidden_states)\n        return encoded, attn_weights\n\n\nclass Transformer(nn.Module):\n    def __init__(self, config, img_size, vis):\n        super(Transformer, self).__init__()\n        self.embeddings = Embeddings(config, img_size=img_size)\n        self.encoder = Encoder(config, vis)\n\n    def forward(self, input_ids):\n        embedding_output, features = self.embeddings(input_ids)\n        encoded, attn_weights = self.encoder(embedding_output)  # (B, n_patch, hidden)\n        return encoded, attn_weights, features\n\n\nclass Conv2dReLU(nn.Sequential):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding=0,\n            stride=1,\n            use_batchnorm=True,\n    ):\n        conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=not (use_batchnorm),\n        )\n        relu = nn.ReLU(inplace=True)\n\n        bn = nn.BatchNorm2d(out_channels)\n\n        super(Conv2dReLU, self).__init__(conv, bn, relu)\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            skip_channels=0,\n            use_batchnorm=True,\n    ):\n        super().__init__()\n        self.conv1 = Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.conv2 = Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n    def forward(self, x, skip=None):\n        x = self.up(x)\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\n\nclass SegmentationHead(nn.Sequential):\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n        super().__init__(conv2d, upsampling)\n\n\nclass DecoderCup(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        head_channels = 512\n        self.conv_more = Conv2dReLU(\n            config.hidden_size,\n            head_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=True,\n        )\n        decoder_channels = config.decoder_channels\n        in_channels = [head_channels] + list(decoder_channels[:-1])\n        out_channels = decoder_channels\n\n        if self.config.n_skip != 0:\n            skip_channels = self.config.skip_channels\n            for i in range(4-self.config.n_skip):  # re-select the skip channels according to n_skip\n                skip_channels[3-i]=0\n\n        else:\n            skip_channels=[0,0,0,0]\n\n        blocks = [\n            DecoderBlock(in_ch, out_ch, sk_ch) for in_ch, out_ch, sk_ch in zip(in_channels, out_channels, skip_channels)\n        ]\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, hidden_states, features=None):\n        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n        x = hidden_states.permute(0, 2, 1)\n        x = x.contiguous().view(B, hidden, h, w)\n        x = self.conv_more(x)\n        for i, decoder_block in enumerate(self.blocks):\n            if features is not None:\n                skip = features[i] if (i < self.config.n_skip) else None\n            else:\n                skip = None\n            x = decoder_block(x, skip=skip)\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n        super(VisionTransformer, self).__init__()\n        self.num_classes = num_classes\n        self.zero_head = zero_head\n        self.classifier = config.classifier\n        self.transformer = Transformer(config, img_size, vis)\n        self.decoder = DecoderCup(config)\n        self.segmentation_head = SegmentationHead(\n            in_channels=config['decoder_channels'][-1],\n            out_channels=config['n_classes'],\n            kernel_size=3,\n        )\n        self.config = config\n\n    def forward(self, x):\n        if x.size()[1] == 1:\n            x = x.repeat(1,3,1,1)\n        x, attn_weights, features = self.transformer(x)  # (B, n_patch, hidden)\n        x = self.decoder(x, features)\n        logits = self.segmentation_head(x)\n        return logits\n\n    def load_from(self, weights):\n        with torch.no_grad():\n\n            res_weight = weights\n            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n\n            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n\n            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n\n            posemb_new = self.transformer.embeddings.position_embeddings\n            if posemb.size() == posemb_new.size():\n                self.transformer.embeddings.position_embeddings.copy_(posemb)\n            elif posemb.size()[1]-1 == posemb_new.size()[1]:\n                posemb = posemb[:, 1:]\n                self.transformer.embeddings.position_embeddings.copy_(posemb)\n            else:\n                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n                ntok_new = posemb_new.size(1)\n                if self.classifier == \"seg\":\n                    _, posemb_grid = posemb[:, :1], posemb[0, 1:]\n                gs_old = int(np.sqrt(len(posemb_grid)))\n                gs_new = int(np.sqrt(ntok_new))\n                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)  # th2np\n                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n                posemb = posemb_grid\n                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n\n            # Encoder whole\n            for bname, block in self.transformer.encoder.named_children():\n                for uname, unit in block.named_children():\n                    unit.load_from(weights, n_block=uname)\n\n            if self.transformer.embeddings.hybrid:\n                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(res_weight[\"conv_root/kernel\"], conv=True))\n                gn_weight = np2th(res_weight[\"gn_root/scale\"]).view(-1)\n                gn_bias = np2th(res_weight[\"gn_root/bias\"]).view(-1)\n                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n\n                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n                    for uname, unit in block.named_children():\n                        unit.load_from(res_weight, n_block=bname, n_unit=uname)\n                        \nCONFIGS = {\n    'ViT-B_16': configs.get_b16_config(),\n    'ViT-B_32': configs.get_b32_config(),\n    'ViT-L_16': configs.get_l16_config(),\n    'ViT-L_32': configs.get_l32_config(),\n    'ViT-H_14': configs.get_h14_config(),\n    'R50-ViT-B_16': configs.get_r50_b16_config(),\n    'R50-ViT-L_16': configs.get_r50_l16_config(),\n    'testing': configs.get_testing(),\n}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### vit_seg_modeling_resnet_skip.py\n\n残差网络ResNet模型","metadata":{}},{"cell_type":"code","source":"%%writefile networks/vit_seg_modeling_resnet_skip.py\nimport math\n\nfrom os.path import join as pjoin\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef np2th(weights, conv=False):\n    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n    if conv:\n        weights = weights.transpose([3, 2, 0, 1])\n    return torch.from_numpy(weights)\n\n\nclass StdConv2d(nn.Conv2d):\n\n    def forward(self, x):\n        w = self.weight\n        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n        w = (w - m) / torch.sqrt(v + 1e-5)\n        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n                        self.dilation, self.groups)\n\n\ndef conv3x3(cin, cout, stride=1, groups=1, bias=False):\n    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n                     padding=1, bias=bias, groups=groups)\n\n\ndef conv1x1(cin, cout, stride=1, bias=False):\n    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n                     padding=0, bias=bias)\n\n\nclass PreActBottleneck(nn.Module):\n    \"\"\"Pre-activation (v2) bottleneck block.\n    \"\"\"\n\n    def __init__(self, cin, cout=None, cmid=None, stride=1):\n        super().__init__()\n        cout = cout or cin\n        cmid = cmid or cout//4\n\n        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n        self.conv1 = conv1x1(cin, cmid, bias=False)\n        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n        self.conv3 = conv1x1(cmid, cout, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n\n        if (stride != 1 or cin != cout):\n            # Projection also with pre-activation according to paper.\n            self.downsample = conv1x1(cin, cout, stride, bias=False)\n            self.gn_proj = nn.GroupNorm(cout, cout)\n\n    def forward(self, x):\n\n        # Residual branch\n        residual = x\n        if hasattr(self, 'downsample'):\n            residual = self.downsample(x)\n            residual = self.gn_proj(residual)\n\n        # Unit's branch\n        y = self.relu(self.gn1(self.conv1(x)))\n        y = self.relu(self.gn2(self.conv2(y)))\n        y = self.gn3(self.conv3(y))\n\n        y = self.relu(residual + y)\n        return y\n\n    def load_from(self, weights, n_block, n_unit):\n        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n\n        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n\n        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n\n        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n\n        self.conv1.weight.copy_(conv1_weight)\n        self.conv2.weight.copy_(conv2_weight)\n        self.conv3.weight.copy_(conv3_weight)\n\n        self.gn1.weight.copy_(gn1_weight.view(-1))\n        self.gn1.bias.copy_(gn1_bias.view(-1))\n\n        self.gn2.weight.copy_(gn2_weight.view(-1))\n        self.gn2.bias.copy_(gn2_bias.view(-1))\n\n        self.gn3.weight.copy_(gn3_weight.view(-1))\n        self.gn3.bias.copy_(gn3_bias.view(-1))\n\n        if hasattr(self, 'downsample'):\n            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n\n            self.downsample.weight.copy_(proj_conv_weight)\n            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))\n\nclass ResNetV2(nn.Module):\n    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n\n    def __init__(self, block_units, width_factor):\n        super().__init__()\n        width = int(64 * width_factor)\n        self.width = width\n\n        self.root = nn.Sequential(OrderedDict([\n            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n            ('relu', nn.ReLU(inplace=True)),\n            # ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n        ]))\n\n        self.body = nn.Sequential(OrderedDict([\n            ('block1', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n                ))),\n            ('block2', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n                ))),\n            ('block3', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n                ))),\n        ]))\n\n    def forward(self, x):\n        features = []\n        b, c, in_size, _ = x.size()\n        x = self.root(x)\n        features.append(x)\n        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)(x)\n        for i in range(len(self.body)-1):\n            x = self.body[i](x)\n            right_size = int(in_size / 4 / (i+1))\n            if x.size()[2] != right_size:\n                pad = right_size - x.size()[2]\n                assert pad < 3 and pad > 0, \"x {} should {}\".format(x.size(), right_size)\n                feat = torch.zeros((b, x.size()[1], right_size, right_size), device=x.device)\n                feat[:, :, 0:x.size()[2], 0:x.size()[3]] = x[:]\n            else:\n                feat = x\n            features.append(feat)\n        x = self.body[-1](x)\n        return x, features[::-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train","metadata":{}},{"cell_type":"markdown","source":"### trainer.py\n\n用函数定义训练过程","metadata":{}},{"cell_type":"code","source":"%%writefile trainer.py\nimport argparse\nimport logging\nimport os\nimport random\nimport sys\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tensorboardX import SummaryWriter\nfrom torch.nn.modules.loss import CrossEntropyLoss\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom utils import DiceLoss\nfrom torchvision import transforms\n\n\ndef trainer_synapse(args, model, snapshot_path):\n    from dataset.dataset_synapse import Synapse_dataset, RandomGenerator\n    logging.basicConfig(filename=snapshot_path + \"/log.txt\", level=logging.INFO,\n                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.info(str(args))\n    base_lr = args.base_lr\n    num_classes = args.num_classes\n    batch_size = args.batch_size * args.n_gpu\n    # max_iterations = args.max_iterations\n    db_train = Synapse_dataset(base_dir=args.root_path, list_dir=args.list_dir, split=\"train\",\n                               transform=transforms.Compose(\n                                   [RandomGenerator(output_size=[args.img_size, args.img_size])]))\n    print(\"The length of train set is: {}\".format(len(db_train)))\n\n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n\n    trainloader = DataLoader(db_train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True,\n                             worker_init_fn=worker_init_fn)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    model.train()\n    ce_loss = CrossEntropyLoss()\n    dice_loss = DiceLoss(num_classes)\n    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n    writer = SummaryWriter(snapshot_path + '/log')\n    iter_num = 0\n    max_epoch = args.max_epochs\n    max_iterations = args.max_epochs * len(trainloader)  # max_epoch = max_iterations // len(trainloader) + 1\n    logging.info(\"{} iterations per epoch. {} max iterations \".format(len(trainloader), max_iterations))\n    best_performance = 0.0\n    iterator = tqdm(range(max_epoch), ncols=70)\n    for epoch_num in iterator:\n        for i_batch, sampled_batch in enumerate(trainloader):\n            image_batch, label_batch = sampled_batch['image'], sampled_batch['label']\n            image_batch, label_batch = image_batch.cuda(), label_batch.cuda()\n            outputs = model(image_batch)\n            loss_ce = ce_loss(outputs, label_batch[:].long())\n            loss_dice = dice_loss(outputs, label_batch, softmax=True)\n            loss = 0.5 * loss_ce + 0.5 * loss_dice\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            lr_ = base_lr * (1.0 - iter_num / max_iterations) ** 0.9\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr_\n\n            iter_num = iter_num + 1\n            writer.add_scalar('info/lr', lr_, iter_num)\n            writer.add_scalar('info/total_loss', loss, iter_num)\n            writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n\n            if iter_num % 100 == 0: \n                logging.info('iteration %d : loss : %f, loss_ce: %f' % (iter_num, loss.item(), loss_ce.item()))\n\n            if iter_num % 20 == 0:\n                image = image_batch[1, 0:1, :, :]\n                image = (image - image.min()) / (image.max() - image.min())\n                writer.add_image('train/Image', image, iter_num)\n                outputs = torch.argmax(torch.softmax(outputs, dim=1), dim=1, keepdim=True)\n                writer.add_image('train/Prediction', outputs[1, ...] * 50, iter_num)\n                labs = label_batch[1, ...].unsqueeze(0) * 50\n                writer.add_image('train/GroundTruth', labs, iter_num)\n\n        save_interval = 50  # int(max_epoch/6)\n        if epoch_num > int(max_epoch / 2) and (epoch_num + 1) % save_interval == 0:\n            save_mode_path = os.path.join(snapshot_path, 'epoch_' + str(epoch_num) + '.pth')\n            torch.save(model.state_dict(), save_mode_path)\n            logging.info(\"save model to {}\".format(save_mode_path))\n\n        if epoch_num >= max_epoch - 1:\n            save_mode_path = os.path.join(snapshot_path, 'epoch_' + str(epoch_num) + '.pth')\n            torch.save(model.state_dict(), save_mode_path)\n            logging.info(\"save model to {}\".format(save_mode_path))\n            iterator.close()\n            break\n\n    writer.close()\n    return \"Training Finished!\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### train.py\n\n训练过程入口文件","metadata":{}},{"cell_type":"code","source":"%%writefile train.py\nimport argparse\nimport logging\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom networks.vit_seg_modeling import VisionTransformer as ViT_seg\nfrom networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\nfrom trainer import trainer_synapse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--root_path', type=str,\n                    default='/kaggle/input/transeunet-synapse/data/Synapse/train_npz', help='root dir for data')\nparser.add_argument('--dataset', type=str,\n                    default='Synapse', help='experiment_name')\nparser.add_argument('--list_dir', type=str,\n                    default='/kaggle/working/lists/lists_Synapse', help='list dir')\nparser.add_argument('--num_classes', type=int,\n                    default=9, help='output channel of network')\nparser.add_argument('--max_iterations', type=int,\n                    default=30000, help='maximum epoch number to train')\nparser.add_argument('--max_epochs', type=int,\n                    default=150, help='maximum epoch number to train')\nparser.add_argument('--batch_size', type=int,\n                    default=24, help='batch_size per gpu')\nparser.add_argument('--n_gpu', type=int, default=1, help='total gpu')\nparser.add_argument('--deterministic', type=int,  default=1,\n                    help='whether use deterministic training')\nparser.add_argument('--base_lr', type=float,  default=0.01,\n                    help='segmentation network learning rate')\nparser.add_argument('--img_size', type=int,\n                    default=224, help='input patch size of network input')\nparser.add_argument('--seed', type=int,\n                    default=1234, help='random seed')\nparser.add_argument('--n_skip', type=int,\n                    default=3, help='using number of skip-connect, default is num')\nparser.add_argument('--vit_name', type=str,\n                    default='R50-ViT-B_16', help='select one vit model')\nparser.add_argument('--vit_patches_size', type=int,\n                    default=16, help='vit_patches_size, default is 16')\nparser.add_argument('-f', type=str,\n                    default='', help='unkonw')\nargs = parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    if not args.deterministic:\n        cudnn.benchmark = True\n        cudnn.deterministic = False\n    else:\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    dataset_name = args.dataset\n    dataset_config = {\n        'Synapse': {\n            'root_path': '/kaggle/input/transeunet-synapse/data/Synapse/train_npz',\n            'list_dir': '/kaggle/working/lists/lists_Synapse',\n            'num_classes': 9,\n        },\n    }\n    args.num_classes = dataset_config[dataset_name]['num_classes']\n    args.root_path = dataset_config[dataset_name]['root_path']\n    args.list_dir = dataset_config[dataset_name]['list_dir']\n    args.is_pretrain = True\n    args.exp = 'TU_' + dataset_name + str(args.img_size)\n    snapshot_path = \"./model/{}/{}\".format(args.exp, 'TU')\n    snapshot_path = snapshot_path + '_pretrain' if args.is_pretrain else snapshot_path\n    snapshot_path += '_' + args.vit_name\n    snapshot_path = snapshot_path + '_skip' + str(args.n_skip)\n    snapshot_path = snapshot_path + '_vitpatch' + str(args.vit_patches_size) if args.vit_patches_size!=16 else snapshot_path\n    snapshot_path = snapshot_path+'_'+str(args.max_iterations)[0:2]+'k' if args.max_iterations != 30000 else snapshot_path\n    snapshot_path = snapshot_path + '_epo' +str(args.max_epochs) if args.max_epochs != 30 else snapshot_path\n    snapshot_path = snapshot_path+'_bs'+str(args.batch_size)\n    snapshot_path = snapshot_path + '_lr' + str(args.base_lr) if args.base_lr != 0.01 else snapshot_path\n    snapshot_path = snapshot_path + '_'+str(args.img_size)\n    snapshot_path = snapshot_path + '_s'+str(args.seed) if args.seed!=1234 else snapshot_path\n\n    if not os.path.exists(snapshot_path):\n        os.makedirs(snapshot_path)\n    config_vit = CONFIGS_ViT_seg[args.vit_name]\n    config_vit.n_classes = args.num_classes\n    config_vit.n_skip = args.n_skip\n    if args.vit_name.find('R50') != -1:\n        config_vit.patches.grid = (int(args.img_size / args.vit_patches_size), int(args.img_size / args.vit_patches_size))\n    net = ViT_seg(config_vit, img_size=args.img_size, num_classes=config_vit.n_classes).cuda()\n    net.load_from(weights=np.load(config_vit.pretrained_path))\n\n    trainer = {'Synapse': trainer_synapse,}\n    trainer[dataset_name](args, net, snapshot_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### test.py\n使用测试集测试训练后的模型准确度","metadata":{}},{"cell_type":"code","source":"%%writefile test.py\nimport argparse\nimport logging\nimport os\nimport random\nimport sys\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom dataset.dataset_synapse import Synapse_dataset\nfrom utils import test_single_volume\nfrom networks.vit_seg_modeling import VisionTransformer as ViT_seg\nfrom networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--volume_path', type=str,\n                    default='/kaggle/input/transeunet-synapse/data/Synapse/test_vol_h5', help='root dir for validation volume data')  # for acdc volume_path=root_dir\nparser.add_argument('--dataset', type=str,\n                    default='Synapse', help='experiment_name')\nparser.add_argument('--num_classes', type=int,\n                    default=4, help='output channel of network')\nparser.add_argument('--list_dir', type=str,\n                    default='/kaggle/input/transunet-lists/lists/lists_Synapse', help='list dir')\n\nparser.add_argument('--max_iterations', type=int,default=20000, help='maximum epoch number to train')\nparser.add_argument('--max_epochs', type=int, default=30, help='maximum epoch number to train')\nparser.add_argument('--batch_size', type=int, default=24,\n                    help='batch_size per gpu')\nparser.add_argument('--img_size', type=int, default=224, help='input patch size of network input')\nparser.add_argument('--is_savenii', action=\"store_true\", help='whether to save results during inference')\n\nparser.add_argument('--n_skip', type=int, default=3, help='using number of skip-connect, default is num')\nparser.add_argument('--vit_name', type=str, default='ViT-B_16', help='select one vit model')\n\nparser.add_argument('--test_save_dir', type=str, default='../predictions', help='saving prediction as nii!')\nparser.add_argument('--deterministic', type=int,  default=1, help='whether use deterministic training')\nparser.add_argument('--base_lr', type=float,  default=0.01, help='segmentation network learning rate')\nparser.add_argument('--seed', type=int, default=1234, help='random seed')\nparser.add_argument('--vit_patches_size', type=int, default=16, help='vit_patches_size, default is 16')\nargs = parser.parse_args()\n\n\ndef inference(args, model, test_save_path=None):\n    from dataset.dataset_synapse import Synapse_dataset, RandomGenerator\n    db_test = args.Dataset(base_dir=args.volume_path, split=\"test_vol\", list_dir=args.list_dir)\n    testloader = DataLoader(db_test, batch_size=1, shuffle=False, num_workers=1)\n    logging.info(\"{} test iterations per epoch\".format(len(testloader)))\n    model.eval()\n    metric_list = 0.0\n    for i_batch, sampled_batch in tqdm(enumerate(testloader)):\n        h, w = sampled_batch[\"image\"].size()[2:]\n        image, label, case_name = sampled_batch[\"image\"], sampled_batch[\"label\"], sampled_batch['case_name'][0]\n        metric_i = test_single_volume(image, label, model, classes=args.num_classes, patch_size=[args.img_size, args.img_size],\n                                      test_save_path=test_save_path, case=case_name, z_spacing=args.z_spacing)\n        metric_list += np.array(metric_i)\n        logging.info('idx %d case %s mean_dice %f mean_hd95 %f' % (i_batch, case_name, np.mean(metric_i, axis=0)[0], np.mean(metric_i, axis=0)[1]))\n    metric_list = metric_list / len(db_test)\n    for i in range(1, args.num_classes):\n        logging.info('Mean class %d mean_dice %f mean_hd95 %f' % (i, metric_list[i-1][0], metric_list[i-1][1]))\n    performance = np.mean(metric_list, axis=0)[0]\n    mean_hd95 = np.mean(metric_list, axis=0)[1]\n    logging.info('Testing performance in best val model: mean_dice : %f mean_hd95 : %f' % (performance, mean_hd95))\n    return \"Testing Finished!\"\n\n\nif __name__ == \"__main__\":\n\n    if not args.deterministic:\n        cudnn.benchmark = True\n        cudnn.deterministic = False\n    else:\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n\n    dataset_config = {\n        'Synapse': {\n            'Dataset': Synapse_dataset,\n            'volume_path': '/kaggle/input/transeunet-synapse/data/Synapse/test_vol_h5',\n            'list_dir': '/kaggle/working/lists',\n            'num_classes': 9,\n            'z_spacing': 1,\n        },\n    }\n    dataset_name = args.dataset\n    args.num_classes = dataset_config[dataset_name]['num_classes']\n    args.volume_path = dataset_config[dataset_name]['volume_path']\n    args.Dataset = dataset_config[dataset_name]['Dataset']\n    args.list_dir = dataset_config[dataset_name]['list_dir']\n    args.z_spacing = dataset_config[dataset_name]['z_spacing']\n    args.is_pretrain = True\n\n    # name the same snapshot defined in train script!\n    args.exp = 'TU_' + dataset_name + str(args.img_size)\n    snapshot_path = \"../model/{}/{}\".format(args.exp, 'TU')\n    snapshot_path = snapshot_path + '_pretrain' if args.is_pretrain else snapshot_path\n    snapshot_path += '_' + args.vit_name\n    snapshot_path = snapshot_path + '_skip' + str(args.n_skip)\n    snapshot_path = snapshot_path + '_vitpatch' + str(args.vit_patches_size) if args.vit_patches_size!=16 else snapshot_path\n    snapshot_path = snapshot_path + '_epo' + str(args.max_epochs) if args.max_epochs != 30 else snapshot_path\n    if dataset_name == 'ACDC':  # using max_epoch instead of iteration to control training duration\n        snapshot_path = snapshot_path + '_' + str(args.max_iterations)[0:2] + 'k' if args.max_iterations != 30000 else snapshot_path\n    snapshot_path = snapshot_path+'_bs'+str(args.batch_size)\n    snapshot_path = snapshot_path + '_lr' + str(args.base_lr) if args.base_lr != 0.01 else snapshot_path\n    snapshot_path = snapshot_path + '_'+str(args.img_size)\n    snapshot_path = snapshot_path + '_s'+str(args.seed) if args.seed!=1234 else snapshot_path\n\n    config_vit = CONFIGS_ViT_seg[args.vit_name]\n    config_vit.n_classes = args.num_classes\n    config_vit.n_skip = args.n_skip\n    config_vit.patches.size = (args.vit_patches_size, args.vit_patches_size)\n    if args.vit_name.find('R50') !=-1:\n        config_vit.patches.grid = (int(args.img_size/args.vit_patches_size), int(args.img_size/args.vit_patches_size))\n    net = ViT_seg(config_vit, img_size=args.img_size, num_classes=config_vit.n_classes).cuda()\n\n    snapshot = os.path.join(snapshot_path, 'best_model.pth')\n    if not os.path.exists(snapshot): snapshot = snapshot.replace('best_model', 'epoch_'+str(args.max_epochs-1))\n    net.load_state_dict(torch.load(snapshot))\n    snapshot_name = snapshot_path.split('/')[-1]\n\n    log_folder = './test_log/test_log_' + args.exp\n    os.makedirs(log_folder, exist_ok=True)\n    logging.basicConfig(filename=log_folder + '/'+snapshot_name+\".txt\", level=logging.INFO, format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.info(str(args))\n    logging.info(snapshot_name)\n\n    if args.is_savenii:\n        args.test_save_dir = '../predictions'\n        test_save_path = os.path.join(args.test_save_dir, args.exp, snapshot_name)\n        os.makedirs(test_save_path, exist_ok=True)\n    else:\n        test_save_path = None\n    inference(args, net, test_save_path)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!CUDA_VISIBLE_DEVICES=0 python train.py --dataset Synapse --vit_name R50-ViT-B_16","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:50:47.221476Z","iopub.execute_input":"2023-09-09T13:50:47.221846Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nNamespace(root_path='/kaggle/input/transeunet-synapse/data/Synapse/train_npz', dataset='Synapse', list_dir='/kaggle/working/lists/lists_Synapse', num_classes=9, max_iterations=30000, max_epochs=150, batch_size=24, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, n_skip=3, vit_name='R50-ViT-B_16', vit_patches_size=16, f='', is_pretrain=True, exp='TU_Synapse224')\nThe length of train set is: 2211\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n93 iterations per epoch. 13950 max iterations \n  1%|▏                             | 1/150 [01:40<4:08:28, 100.05s/it]iteration 100 : loss : 0.400164, loss_ce: 0.091674\n  1%|▍                              | 2/150 [03:15<3:59:41, 97.17s/it]iteration 200 : loss : 0.406735, loss_ce: 0.125478\n  2%|▌                              | 3/150 [04:50<3:55:36, 96.17s/it]iteration 300 : loss : 0.383927, loss_ce: 0.088658\n  3%|▊                              | 4/150 [06:25<3:53:30, 95.96s/it]iteration 400 : loss : 0.322300, loss_ce: 0.059228\n  3%|█                              | 5/150 [08:01<3:51:51, 95.94s/it]iteration 500 : loss : 0.292933, loss_ce: 0.069869\n  4%|█▏                             | 6/150 [09:37<3:50:03, 95.86s/it]iteration 600 : loss : 0.261206, loss_ce: 0.053228\n  5%|█▍                             | 7/150 [11:13<3:48:48, 96.01s/it]iteration 700 : loss : 0.269062, loss_ce: 0.074908\n  5%|█▋                             | 8/150 [12:49<3:47:10, 95.99s/it]iteration 800 : loss : 0.202833, loss_ce: 0.037100\n  6%|█▊                             | 9/150 [14:25<3:45:42, 96.05s/it]iteration 900 : loss : 0.193060, loss_ce: 0.043088\n  7%|██                            | 10/150 [16:02<3:44:13, 96.10s/it]iteration 1000 : loss : 0.138051, loss_ce: 0.021510\n  7%|██▏                           | 11/150 [17:38<3:42:50, 96.19s/it]iteration 1100 : loss : 0.138895, loss_ce: 0.035552\n  8%|██▍                           | 12/150 [19:14<3:41:19, 96.23s/it]iteration 1200 : loss : 0.130158, loss_ce: 0.021302\n  9%|██▌                           | 13/150 [20:51<3:39:52, 96.29s/it]iteration 1300 : loss : 0.149759, loss_ce: 0.027392\n 10%|███                           | 15/150 [24:04<3:36:48, 96.36s/it]iteration 1400 : loss : 0.100192, loss_ce: 0.038440\n 11%|███▏                          | 16/150 [25:40<3:35:15, 96.38s/it]iteration 1500 : loss : 0.077800, loss_ce: 0.028043\n 11%|███▍                          | 17/150 [27:16<3:33:34, 96.35s/it]iteration 1600 : loss : 0.065713, loss_ce: 0.017660\n 12%|███▌                          | 18/150 [28:53<3:31:56, 96.33s/it]iteration 1700 : loss : 0.068249, loss_ce: 0.016472\n 13%|███▊                          | 19/150 [30:28<3:30:00, 96.19s/it]iteration 1800 : loss : 0.060769, loss_ce: 0.016907\n","output_type":"stream"}]}]}