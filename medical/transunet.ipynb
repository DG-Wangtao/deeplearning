{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TransUNet\n\nhttps://github.com/Beckschen/TransUNet","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/Beckschen/TransUNet.git","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:26:31.696089Z","iopub.execute_input":"2023-09-24T16:26:31.696983Z","iopub.status.idle":"2023-09-24T16:26:32.661076Z","shell.execute_reply.started":"2023-09-24T16:26:31.696945Z","shell.execute_reply":"2023-09-24T16:26:32.659874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 准备依赖环境","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision numpy scipy tqdm tensorboard tensorboardX ml-collections medpy SimpleITK h5py\n!pip uninstall -y datasets","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:26:32.663594Z","iopub.execute_input":"2023-09-24T16:26:32.664068Z","iopub.status.idle":"2023-09-24T16:26:46.689297Z","shell.execute_reply.started":"2023-09-24T16:26:32.664030Z","shell.execute_reply":"2023-09-24T16:26:46.688031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 准备数据集\n参考 https://github.com/Beckschen/TransUNet/tree/main#2-prepare-data\n\n#### 下载Google imagenet 21预训练模型¶\n关于ViT预训练模型简介：\n\nhttps://github.com/google-research/vision_transformer\nhttps://zhuanlan.zhihu.com/p/445122996","metadata":{}},{"cell_type":"code","source":"!mkdir model && gsutil -m cp \\\n  \"gs://vit_models/imagenet21k/R26+ViT-B_32.npz\" \\\n  \"gs://vit_models/imagenet21k/R50+ViT-B_16.npz\" \\\n  \"gs://vit_models/imagenet21k/R50+ViT-L_32.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-B_16.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-B_32.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-B_8.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-H_14.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-L_16.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-L_32.npz\" \\\n  ./model\n\n# 下载一次就够了","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:26:46.691609Z","iopub.execute_input":"2023-09-24T16:26:46.692023Z","iopub.status.idle":"2023-09-24T16:26:47.686234Z","shell.execute_reply.started":"2023-09-24T16:26:46.691986Z","shell.execute_reply":"2023-09-24T16:26:47.684905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 训练模型","metadata":{}},{"cell_type":"markdown","source":"## 使用dltrack数据","metadata":{}},{"cell_type":"code","source":" %%writefile /kaggle/working/TransUNet/networks/vit_seg_configs.py\n# %load /kaggle/working/TransUNet/networks/vit_seg_configs.py\nimport ml_collections\n\ndef get_b16_config():\n    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 768\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 3072\n    config.transformer.num_heads = 12\n    config.transformer.num_layers = 12\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n\n    config.classifier = 'seg'\n    config.representation_size = None\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '/kaggle/working/model/ViT-B_16.npz'\n    config.patch_size = 16\n\n    config.decoder_channels = (256, 128, 64, 16)\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_testing():\n    \"\"\"Returns a minimal configuration for testing.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 1\n    config.transformer.num_heads = 1\n    config.transformer.num_layers = 1\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config\n\ndef get_r50_b16_config():\n    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.pretrained_path = '/kaggle/working/model/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 2\n    config.n_skip = 3\n    config.activation = 'softmax'\n\n    return config\n\n\ndef get_b32_config():\n    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.size = (32, 32)\n    config.pretrained_path = '/kaggle/working/model/ViT-B_32.npz'\n    return config\n\n\ndef get_l16_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1024\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 4096\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 24\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.representation_size = None\n\n    # custom\n    config.classifier = 'seg'\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '/kaggle/working/model/ViT-L_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_r50_l16_config():\n    \"\"\"Returns the Resnet50 + ViT-L/16 configuration. customized \"\"\"\n    config = get_l16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.resnet_pretrained_path = '/kaggle/working/model/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_l32_config():\n    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n    config = get_l16_config()\n    config.patches.size = (32, 32)\n    return config\n\n\ndef get_h14_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n    config.hidden_size = 1280\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 5120\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 32\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n\n    return config\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:26:47.690894Z","iopub.execute_input":"2023-09-24T16:26:47.691410Z","iopub.status.idle":"2023-09-24T16:26:47.703761Z","shell.execute_reply.started":"2023-09-24T16:26:47.691379Z","shell.execute_reply":"2023-09-24T16:26:47.702516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/TransUNet/datasets/dataset_apo.py\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset\n\nfrom PIL import Image\nfrom typing import Tuple\nfrom pathlib import Path\nclass APODataSet(Dataset):\n    # 格式不对的异常数据\n    invalid_img = [10, 184, 185]\n    def __init__(self, img_dir, mask_dir: str, size) -> None:\n        # 获取所有图片路径\n        self.img_paths = list(Path(img_dir).glob(\"*\"))\n        self.mask_paths = list(Path(mask_dir).glob(\"*\"))\n        for idx in self.invalid_img:\n            del self.img_paths[idx]\n            del self.mask_paths[idx]\n        \n        \n        # 设置 transforms\n        self.transform = transforms.Compose([transforms.Resize(size), transforms.ToTensor()])\n#         self.transform = transforms.Compose([transforms.PILToTensor()])\n\n    # 使用函数加载原始图像\n    def load_orig_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.img_paths[index]\n        return Image.open(image_path) \n    \n    # 使用函数加载mask图像\n    def load_mask_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.mask_paths[index]\n        return Image.open(image_path) \n\n    #  重写 __len__() 方法 (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.img_paths)\n\n    # 重写 __getitem__() 方法 (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"Returns one sample of data, image and mask (X, y).\"\n        orig_img = self.load_orig_image(index)\n        mask_img = self.load_mask_image(index)\n        \n        orig_img = self.transform(orig_img)\n        mask_img = self.transform(mask_img)\n        mask_img = mask_img[0]\n#         if orig_img.size()[0] != 3:\n#             print(\"{}: orig_img size: {}\".format(index,orig_img.size()))\n#             return None\n        # return data, mask (X, y)\n        return orig_img, mask_img\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:27:56.041980Z","iopub.execute_input":"2023-09-24T16:27:56.043115Z","iopub.status.idle":"2023-09-24T16:27:56.054012Z","shell.execute_reply.started":"2023-09-24T16:27:56.043076Z","shell.execute_reply":"2023-09-24T16:27:56.052603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/TransUNet/trainer_apo.py\nimport argparse\nimport logging\nimport os\nimport random\nimport sys\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tensorboardX import SummaryWriter\nfrom torch.nn.modules.loss import CrossEntropyLoss, BCELoss\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom utils import DiceLoss\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, random_split\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n\n\ndef trainer_apo(args, model, snapshot_path):\n    from datasets.dataset_apo import APODataSet\n    logging.basicConfig(filename=snapshot_path + \"/log.txt\", level=logging.INFO,\n                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.info(str(args))\n    \n    base_lr = args.base_lr\n    num_classes = args.num_classes\n    batch_size = args.batch_size * args.n_gpu\n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n    \n    dataset =  APODataSet(img_dir = \"/kaggle/input/dltrack/apo_images\",\n                      mask_dir = \"/kaggle/input/dltrack/apo_masks\",\n                     size = [args.img_size, args.img_size])\n\n    total = len(dataset)\n    train_size = int(0.8*total)\n    validate_size = total - train_size\n    train_data, validate_data = random_split(dataset, [train_size, validate_size])\n    print(\"dataset info\\ntotal: {}, train_size: {}, validate_size: {}\".format(total, len(train_data), len(validate_data)))\n\n    trainloader = DataLoader(dataset=train_data,\n                                     batch_size=batch_size,\n                                     num_workers=2,\n                                     pin_memory=True,\n                                     worker_init_fn=worker_init_fn,\n                                     shuffle=True)\n\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    model.train()\n#     ce_loss = BCELoss()\n    ce_loss = CrossEntropyLoss()\n    dice_loss = DiceLoss(num_classes)\n    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n    writer = SummaryWriter(snapshot_path + '/log')\n    iter_num = 0\n    max_epoch = args.max_epochs\n    max_iterations = args.max_epochs * len(trainloader)  # max_epoch = max_iterations // len(trainloader) + 1\n    logging.info(\"{} iterations per epoch. {} max iterations \".format(len(trainloader), max_iterations))\n    best_performance = 0.0\n    iterator = tqdm(range(max_epoch), ncols=70)\n    \n    transform = transforms.ToPILImage()\n    for epoch_num in iterator:\n        for i_batch, sampled_batch in enumerate(trainloader):\n            image_batch, label_batch = sampled_batch\n            image_batch, label_batch = image_batch.cuda(), label_batch.cuda()\n            outputs = model(image_batch)\n#             m = nn.Sigmoid()\n            loss_ce = ce_loss(outputs, label_batch[:].long())\n#             loss_ce = ce_loss(outputs, label_batch)\n            label_batch = label_batch.squeeze(dim=1)\n            loss_dice = dice_loss(outputs, label_batch, softmax=True)\n#             loss_dice = dice_loss(outputs, label_batch, softmax=True)\n            loss = 0.5 * loss_ce + 0.5 * loss_dice\n#             loss = loss_dice\n#             loss = loss_ce\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            lr_ = base_lr * (1.0 - iter_num / max_iterations) ** 0.9\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr_\n\n            iter_num = iter_num + 1\n            writer.add_scalar('info/lr', lr_, iter_num)\n            writer.add_scalar('info/total_loss', loss, iter_num)\n#             writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n\n            if iter_num % 20 == 0:\n            \n                orig_img = transform(image_batch[0])\n                mask_img = transform(label_batch[0])\n                pre_img = transform(outputs[0])\n\n                fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15, 12))\n                ax1.imshow(orig_img)\n                ax1.set_title(\"origin_img\")\n                ax2.imshow(mask_img)\n                ax2.set_title(\"mask_img\")\n                ax3.imshow(pre_img)\n                ax3.set_title(\"pre_img\")\n                plt.savefig('output/images/{}.png'.format(iter_num))\n                logging.info('iteration %d : loss : %f' % (iter_num, loss.item()))\n                \n                \n                image = image_batch[1, 0:1, :, :]\n                image = (image - image.min()) / (image.max() - image.min())\n                writer.add_image('train/Image', image, iter_num)\n                outputs = torch.argmax(torch.softmax(outputs, dim=1), dim=1, keepdim=True)\n                writer.add_image('train/Prediction', outputs[1, ...] * 50, iter_num)\n                labs = label_batch[1, ...].unsqueeze(0) * 50\n                writer.add_image('train/GroundTruth', labs, iter_num)\n\n        save_interval = 50  # int(max_epoch/6)\n        if epoch_num > int(max_epoch / 2) and (epoch_num + 1) % save_interval == 0:\n            save_mode_path = os.path.join(snapshot_path, 'epoch_' + str(epoch_num) + '.pth')\n            torch.save(model.state_dict(), save_mode_path)\n            logging.info(\"save model to {}\".format(save_mode_path))\n\n        if epoch_num >= max_epoch - 1:\n            save_mode_path = os.path.join(snapshot_path, 'epoch_' + str(epoch_num) + '.pth')\n            torch.save(model.state_dict(), save_mode_path)\n            logging.info(\"save model to {}\".format(save_mode_path))\n            iterator.close()\n            break\n\n    writer.close()\n    return \"Training Finished!\"","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:33:04.599373Z","iopub.execute_input":"2023-09-24T16:33:04.599815Z","iopub.status.idle":"2023-09-24T16:33:04.613216Z","shell.execute_reply.started":"2023-09-24T16:33:04.599784Z","shell.execute_reply":"2023-09-24T16:33:04.611813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/TransUNet/train-apo.py\nimport argparse\nimport logging\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom networks.vit_seg_modeling import VisionTransformer as ViT_seg\nfrom networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\nfrom trainer_apo import trainer_apo\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--root_path', type=str,\n                    default='../data/Synapse/train_npz', help='root dir for data')\nparser.add_argument('--dataset', type=str,\n                    default='Synapse', help='experiment_name')\nparser.add_argument('--list_dir', type=str,\n                    default='./lists/lists_Synapse', help='list dir')\nparser.add_argument('--num_classes', type=int,\n                    default=9, help='output channel of network')\nparser.add_argument('--max_iterations', type=int,\n                    default=30000, help='maximum epoch number to train')\nparser.add_argument('--max_epochs', type=int,\n                    default=150, help='maximum epoch number to train')\nparser.add_argument('--batch_size', type=int,\n                    default=24, help='batch_size per gpu')\nparser.add_argument('--n_gpu', type=int, default=1, help='total gpu')\nparser.add_argument('--deterministic', type=int,  default=1,\n                    help='whether use deterministic training')\nparser.add_argument('--base_lr', type=float,  default=0.01,\n                    help='segmentation network learning rate')\nparser.add_argument('--img_size', type=int,\n                    default=224, help='input patch size of network input')\nparser.add_argument('--seed', type=int,\n                    default=1234, help='random seed')\nparser.add_argument('--n_skip', type=int,\n                    default=3, help='using number of skip-connect, default is num')\nparser.add_argument('--vit_name', type=str,\n                    default='R50-ViT-B_16', help='select one vit model')\nparser.add_argument('--vit_patches_size', type=int,\n                    default=16, help='vit_patches_size, default is 16')\nargs = parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    if not args.deterministic:\n        cudnn.benchmark = True\n        cudnn.deterministic = False\n    else:\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    dataset_name = args.dataset\n    dataset_config = {\n        'Synapse': {\n            'root_path': '../data/Synapse/train_npz',\n            'list_dir': './lists/lists_Synapse',\n            'num_classes': 9,\n        },\n        'dltrack': {\n            'root_path': '',\n            'list_dir': '',\n            'num_classes': 1,\n        },\n    }\n    args.num_classes = dataset_config[dataset_name]['num_classes']\n    args.root_path = dataset_config[dataset_name]['root_path']\n    args.list_dir = dataset_config[dataset_name]['list_dir']\n    args.is_pretrain = True\n    args.exp = 'TU_' + dataset_name + str(args.img_size)\n    snapshot_path = \"../model/{}/{}\".format(args.exp, 'TU')\n    snapshot_path = snapshot_path + '_pretrain' if args.is_pretrain else snapshot_path\n    snapshot_path += '_' + args.vit_name\n    snapshot_path = snapshot_path + '_skip' + str(args.n_skip)\n    snapshot_path = snapshot_path + '_vitpatch' + str(args.vit_patches_size) if args.vit_patches_size!=16 else snapshot_path\n    snapshot_path = snapshot_path+'_'+str(args.max_iterations)[0:2]+'k' if args.max_iterations != 30000 else snapshot_path\n    snapshot_path = snapshot_path + '_epo' +str(args.max_epochs) if args.max_epochs != 30 else snapshot_path\n    snapshot_path = snapshot_path+'_bs'+str(args.batch_size)\n    snapshot_path = snapshot_path + '_lr' + str(args.base_lr) if args.base_lr != 0.01 else snapshot_path\n    snapshot_path = snapshot_path + '_'+str(args.img_size)\n    snapshot_path = snapshot_path + '_s'+str(args.seed) if args.seed!=1234 else snapshot_path\n\n    if not os.path.exists(snapshot_path):\n        os.makedirs(snapshot_path)\n    config_vit = CONFIGS_ViT_seg[args.vit_name]\n    config_vit.n_classes = args.num_classes\n    config_vit.n_skip = args.n_skip\n    if args.vit_name.find('R50') != -1:\n        config_vit.patches.grid = (int(args.img_size / args.vit_patches_size), int(args.img_size / args.vit_patches_size))\n    net = ViT_seg(config_vit, img_size=args.img_size, num_classes=config_vit.n_classes).cuda()\n#     net.load_from(weights=np.load(config_vit.pretrained_path))\n\n    trainer = {'dltrack': trainer_apo,}\n    trainer[dataset_name](args, net, snapshot_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:26:47.746436Z","iopub.execute_input":"2023-09-24T16:26:47.747005Z","iopub.status.idle":"2023-09-24T16:26:47.763864Z","shell.execute_reply.started":"2023-09-24T16:26:47.746973Z","shell.execute_reply":"2023-09-24T16:26:47.762677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r TransUNet/output/images\n!mkdir -p TransUNet/output/images\n!cd TransUNet && python train-apo.py --dataset dltrack --num_classes 1 --vit_name R50-ViT-B_16","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:33:10.003918Z","iopub.execute_input":"2023-09-24T16:33:10.004324Z","iopub.status.idle":"2023-09-24T16:33:29.155392Z","shell.execute_reply.started":"2023-09-24T16:33:10.004292Z","shell.execute_reply":"2023-09-24T16:33:29.154141Z"},"trusted":true},"execution_count":null,"outputs":[]}]}