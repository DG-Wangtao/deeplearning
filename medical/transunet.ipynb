{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TransUNet\n\nhttps://github.com/Beckschen/TransUNet","metadata":{}},{"cell_type":"markdown","source":"## 准备依赖环境","metadata":{}},{"cell_type":"markdown","source":"## 准备数据集\n参考 https://github.com/Beckschen/TransUNet/tree/main#2-prepare-data\n- transunet-synapse\n- transunet-lists\n","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision numpy scipy tqdm tensorboard tensorboardX ml-collections medpy SimpleITK h5py","metadata":{"execution":{"iopub.status.busy":"2023-09-08T07:02:02.112853Z","iopub.execute_input":"2023-09-08T07:02:02.113519Z","iopub.status.idle":"2023-09-08T07:02:29.059234Z","shell.execute_reply.started":"2023-09-08T07:02:02.113475Z","shell.execute_reply":"2023-09-08T07:02:29.058116Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.23.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.12.3)\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (2.6)\nCollecting ml-collections\n  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting medpy\n  Downloading MedPy-0.4.0.tar.gz (151 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: SimpleITK in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (3.9.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.4.3)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (68.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.3.7)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.40.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (21.3)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from ml-collections) (6.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from ml-collections) (1.16.0)\nCollecting contextlib2 (from ml-collections)\n  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorboardX) (3.0.9)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\nBuilding wheels for collected packages: ml-collections, medpy\n  Building wheel for ml-collections (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=40180a5c4e3a3943d32ce9ca6061a36c8fc33588b513aab141dc62c61fdf4399\n  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n  Building wheel for medpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for medpy: filename=MedPy-0.4.0-py3-none-any.whl size=214946 sha256=c4ec9f423afb896b41e3dfda6fe273306365d1d09152766981174fea90e6befe\n  Stored in directory: /root/.cache/pip/wheels/d4/32/c7/6380ab2edb8cca018d39a0f1d43250fd9791922c963117de46\nSuccessfully built ml-collections medpy\nInstalling collected packages: contextlib2, ml-collections, medpy\nSuccessfully installed contextlib2-21.6.0 medpy-0.4.0 ml-collections-0.1.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 下载Google imagenet 21预训练模型\n\n关于ViT预训练模型简介：\n- https://github.com/google-research/vision_transformer\n- https://zhuanlan.zhihu.com/p/445122996","metadata":{}},{"cell_type":"code","source":"!gsutil -m cp \\\n  \"gs://vit_models/imagenet21k/R26+ViT-B_32.npz\" \\\n  \"gs://vit_models/imagenet21k/R50+ViT-B_16.npz\" \\\n  \"gs://vit_models/imagenet21k/R50+ViT-L_32.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-B_16.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-B_32.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-B_8.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-H_14.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-L_16.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-L_32.npz\" \\\n  .\n\n# 下载一次就够了","metadata":{"execution":{"iopub.status.busy":"2023-09-08T07:02:29.061934Z","iopub.execute_input":"2023-09-08T07:02:29.062986Z","iopub.status.idle":"2023-09-08T07:04:19.591895Z","shell.execute_reply.started":"2023-09-08T07:02:29.062949Z","shell.execute_reply":"2023-09-08T07:04:19.590679Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Copying gs://vit_models/imagenet21k/R26+ViT-B_32.npz...\nCopying gs://vit_models/imagenet21k/R50+ViT-B_16.npz...                         \nCopying gs://vit_models/imagenet21k/R50+ViT-L_32.npz...                         \nCopying gs://vit_models/imagenet21k/ViT-B_16.npz...                             \nCopying gs://vit_models/imagenet21k/ViT-B_32.npz...                             \nCopying gs://vit_models/imagenet21k/ViT-B_8.npz...                              \nCopying gs://vit_models/imagenet21k/ViT-H_14.npz...                             \nCopying gs://vit_models/imagenet21k/ViT-L_16.npz...                             \nCopying gs://vit_models/imagenet21k/ViT-L_32.npz...                             \n| [9/9 files][  8.2 GiB/  8.2 GiB] 100% Done  22.5 MiB/s ETA 00:00:00           \nOperation completed over 9 objects/8.2 GiB.                                      \n","output_type":"stream"}]},{"cell_type":"markdown","source":"### dataset.py\n数据集处理工具，实现pytorch的DataSet类，用于加载训练集、标签和测试数据","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport h5py\nimport numpy as np\nimport torch\nfrom scipy import ndimage\nfrom scipy.ndimage.interpolation import zoom\nfrom torch.utils.data import Dataset\n\n\ndef random_rot_flip(image, label):\n    k = np.random.randint(0, 4)\n    image = np.rot90(image, k)\n    label = np.rot90(label, k)\n    axis = np.random.randint(0, 2)\n    image = np.flip(image, axis=axis).copy()\n    label = np.flip(label, axis=axis).copy()\n    return image, label\n\n\ndef random_rotate(image, label):\n    angle = np.random.randint(-20, 20)\n    image = ndimage.rotate(image, angle, order=0, reshape=False)\n    label = ndimage.rotate(label, angle, order=0, reshape=False)\n    return image, label\n\n\nclass RandomGenerator(object):\n    def __init__(self, output_size):\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['label']\n\n        if random.random() > 0.5:\n            image, label = random_rot_flip(image, label)\n        elif random.random() > 0.5:\n            image, label = random_rotate(image, label)\n        x, y = image.shape\n        if x != self.output_size[0] or y != self.output_size[1]:\n            image = zoom(image, (self.output_size[0] / x, self.output_size[1] / y), order=3)  # why not 3?\n            label = zoom(label, (self.output_size[0] / x, self.output_size[1] / y), order=0)\n        image = torch.from_numpy(image.astype(np.float32)).unsqueeze(0)\n        label = torch.from_numpy(label.astype(np.float32))\n        sample = {'image': image, 'label': label.long()}\n        return sample\n\n\nclass Synapse_dataset(Dataset):\n    def __init__(self, base_dir, list_dir, split, transform=None):\n        self.transform = transform  # using transform in torch!\n        self.split = split\n        self.sample_list = open(os.path.join(list_dir, self.split+'.txt')).readlines()\n        self.data_dir = base_dir\n\n    def __len__(self):\n        return len(self.sample_list)\n\n    def __getitem__(self, idx):\n        if self.split == \"train\":\n            slice_name = self.sample_list[idx].strip('\\n')\n            data_path = os.path.join(self.data_dir, slice_name+'.npz')\n            data = np.load(data_path)\n            image, label = data['image'], data['label']\n        else:\n            vol_name = self.sample_list[idx].strip('\\n')\n            filepath = self.data_dir + \"/{}.npy.h5\".format(vol_name)\n            data = h5py.File(filepath)\n            image, label = data['image'][:], data['label'][:]\n\n        sample = {'image': image, 'label': label}\n        if self.transform:\n            sample = self.transform(sample)\n        sample['case_name'] = self.sample_list[idx].strip('\\n')\n        return sample","metadata":{"execution":{"iopub.status.busy":"2023-09-08T07:04:19.594473Z","iopub.execute_input":"2023-09-08T07:04:19.594894Z","iopub.status.idle":"2023-09-08T07:04:23.208309Z","shell.execute_reply.started":"2023-09-08T07:04:19.594853Z","shell.execute_reply":"2023-09-08T07:04:23.207337Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### utils.py\n\n一些工具方法\n- DiceLoss 损失函数： https://zhuanlan.zhihu.com/p/86704421\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom medpy import metric\nfrom scipy.ndimage import zoom\nimport torch.nn as nn\nimport SimpleITK as sitk\n\n\nclass DiceLoss(nn.Module):\n    def __init__(self, n_classes):\n        super(DiceLoss, self).__init__()\n        self.n_classes = n_classes\n\n    def _one_hot_encoder(self, input_tensor):\n        tensor_list = []\n        for i in range(self.n_classes):\n            temp_prob = input_tensor == i  # * torch.ones_like(input_tensor)\n            tensor_list.append(temp_prob.unsqueeze(1))\n        output_tensor = torch.cat(tensor_list, dim=1)\n        return output_tensor.float()\n\n    def _dice_loss(self, score, target):\n        target = target.float()\n        smooth = 1e-5\n        intersect = torch.sum(score * target)\n        y_sum = torch.sum(target * target)\n        z_sum = torch.sum(score * score)\n        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n        loss = 1 - loss\n        return loss\n\n    def forward(self, inputs, target, weight=None, softmax=False):\n        if softmax:\n            inputs = torch.softmax(inputs, dim=1)\n        target = self._one_hot_encoder(target)\n        if weight is None:\n            weight = [1] * self.n_classes\n        assert inputs.size() == target.size(), 'predict {} & target {} shape do not match'.format(inputs.size(), target.size())\n        class_wise_dice = []\n        loss = 0.0\n        for i in range(0, self.n_classes):\n            dice = self._dice_loss(inputs[:, i], target[:, i])\n            class_wise_dice.append(1.0 - dice.item())\n            loss += dice * weight[i]\n        return loss / self.n_classes\n\n\ndef calculate_metric_percase(pred, gt):\n    pred[pred > 0] = 1\n    gt[gt > 0] = 1\n    if pred.sum() > 0 and gt.sum()>0:\n        dice = metric.binary.dc(pred, gt)\n        hd95 = metric.binary.hd95(pred, gt)\n        return dice, hd95\n    elif pred.sum() > 0 and gt.sum()==0:\n        return 1, 0\n    else:\n        return 0, 0\n\n\ndef test_single_volume(image, label, net, classes, patch_size=[256, 256], test_save_path=None, case=None, z_spacing=1):\n    image, label = image.squeeze(0).cpu().detach().numpy(), label.squeeze(0).cpu().detach().numpy()\n    if len(image.shape) == 3:\n        prediction = np.zeros_like(label)\n        for ind in range(image.shape[0]):\n            slice = image[ind, :, :]\n            x, y = slice.shape[0], slice.shape[1]\n            if x != patch_size[0] or y != patch_size[1]:\n                slice = zoom(slice, (patch_size[0] / x, patch_size[1] / y), order=3)  # previous using 0\n            input = torch.from_numpy(slice).unsqueeze(0).unsqueeze(0).float().cuda()\n            net.eval()\n            with torch.no_grad():\n                outputs = net(input)\n                out = torch.argmax(torch.softmax(outputs, dim=1), dim=1).squeeze(0)\n                out = out.cpu().detach().numpy()\n                if x != patch_size[0] or y != patch_size[1]:\n                    pred = zoom(out, (x / patch_size[0], y / patch_size[1]), order=0)\n                else:\n                    pred = out\n                prediction[ind] = pred\n    else:\n        input = torch.from_numpy(image).unsqueeze(\n            0).unsqueeze(0).float().cuda()\n        net.eval()\n        with torch.no_grad():\n            out = torch.argmax(torch.softmax(net(input), dim=1), dim=1).squeeze(0)\n            prediction = out.cpu().detach().numpy()\n    metric_list = []\n    for i in range(1, classes):\n        metric_list.append(calculate_metric_percase(prediction == i, label == i))\n\n    if test_save_path is not None:\n        img_itk = sitk.GetImageFromArray(image.astype(np.float32))\n        prd_itk = sitk.GetImageFromArray(prediction.astype(np.float32))\n        lab_itk = sitk.GetImageFromArray(label.astype(np.float32))\n        img_itk.SetSpacing((1, 1, z_spacing))\n        prd_itk.SetSpacing((1, 1, z_spacing))\n        lab_itk.SetSpacing((1, 1, z_spacing))\n        sitk.WriteImage(prd_itk, test_save_path + '/'+case + \"_pred.nii.gz\")\n        sitk.WriteImage(img_itk, test_save_path + '/'+ case + \"_img.nii.gz\")\n        sitk.WriteImage(lab_itk, test_save_path + '/'+ case + \"_gt.nii.gz\")\n    return metric_list","metadata":{"execution":{"iopub.status.busy":"2023-09-08T07:04:23.211153Z","iopub.execute_input":"2023-09-08T07:04:23.211908Z","iopub.status.idle":"2023-09-08T07:04:24.254116Z","shell.execute_reply.started":"2023-09-08T07:04:23.211872Z","shell.execute_reply":"2023-09-08T07:04:24.253102Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## networks\n获取预训练模型的配置，用于在训练之前预先加载该模型","metadata":{}},{"cell_type":"markdown","source":"### vit_seg_configs.py","metadata":{}},{"cell_type":"code","source":"import ml_collections\n\ndef get_b16_config():\n    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 768\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 3072\n    config.transformer.num_heads = 12\n    config.transformer.num_layers = 12\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n\n    config.classifier = 'seg'\n    config.representation_size = None\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '/kaggle/working/ViT-B_16.npz'\n    config.patch_size = 16\n\n    config.decoder_channels = (256, 128, 64, 16)\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_testing():\n    \"\"\"Returns a minimal configuration for testing.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 1\n    config.transformer.num_heads = 1\n    config.transformer.num_layers = 1\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config\n\ndef get_r50_b16_config():\n    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.pretrained_path = '/kaggle/working/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 2\n    config.n_skip = 3\n    config.activation = 'softmax'\n\n    return config\n\n\ndef get_b32_config():\n    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.size = (32, 32)\n    config.pretrained_path = '/kaggle/working/ViT-B_32.npz'\n    return config\n\n\ndef get_l16_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1024\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 4096\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 24\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.representation_size = None\n\n    # custom\n    config.classifier = 'seg'\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '/kaggle/working/ViT-L_16.npz_.gstmp'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_r50_l16_config():\n    \"\"\"Returns the Resnet50 + ViT-L/16 configuration. customized \"\"\"\n    config = get_l16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.resnet_pretrained_path = '/kaggle/working/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_l32_config():\n    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n    config = get_l16_config()\n    config.patches.size = (32, 32)\n    return config\n\n\ndef get_h14_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n    config.hidden_size = 1280\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 5120\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 32\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n\n    return config","metadata":{"execution":{"iopub.status.busy":"2023-09-08T07:04:24.255475Z","iopub.execute_input":"2023-09-08T07:04:24.255818Z","iopub.status.idle":"2023-09-08T07:04:24.331726Z","shell.execute_reply.started":"2023-09-08T07:04:24.255787Z","shell.execute_reply":"2023-09-08T07:04:24.330773Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### vit_seg_modeling.py\n\n网络模型定义","metadata":{}},{"cell_type":"code","source":"# coding=utf-8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\nimport math\n\nfrom os.path import join as pjoin\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nfrom torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\nfrom torch.nn.modules.utils import _pair\nfrom scipy import ndimage\n# from . import vit_seg_configs as configs\n# from .vit_seg_modeling_resnet_skip import ResNetV2\n\n\nlogger = logging.getLogger(__name__)\n\n\nATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\nATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\nATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\nATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\nFC_0 = \"MlpBlock_3/Dense_0\"\nFC_1 = \"MlpBlock_3/Dense_1\"\nATTENTION_NORM = \"LayerNorm_0\"\nMLP_NORM = \"LayerNorm_2\"\n\n\ndef np2th(weights, conv=False):\n    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n    if conv:\n        weights = weights.transpose([3, 2, 0, 1])\n    return torch.from_numpy(weights)\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n\n\nclass Attention(nn.Module):\n    def __init__(self, config, vis):\n        super(Attention, self).__init__()\n        self.vis = vis\n        self.num_attention_heads = config.transformer[\"num_heads\"]\n        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = Linear(config.hidden_size, self.all_head_size)\n        self.key = Linear(config.hidden_size, self.all_head_size)\n        self.value = Linear(config.hidden_size, self.all_head_size)\n\n        self.out = Linear(config.hidden_size, config.hidden_size)\n        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n\n        self.softmax = Softmax(dim=-1)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        attention_probs = self.softmax(attention_scores)\n        weights = attention_probs if self.vis else None\n        attention_probs = self.attn_dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        attention_output = self.out(context_layer)\n        attention_output = self.proj_dropout(attention_output)\n        return attention_output, weights\n\n\nclass Mlp(nn.Module):\n    def __init__(self, config):\n        super(Mlp, self).__init__()\n        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n        self.act_fn = ACT2FN[\"gelu\"]\n        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n\n        self._init_weights()\n\n    def _init_weights(self):\n        nn.init.xavier_uniform_(self.fc1.weight)\n        nn.init.xavier_uniform_(self.fc2.weight)\n        nn.init.normal_(self.fc1.bias, std=1e-6)\n        nn.init.normal_(self.fc2.bias, std=1e-6)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act_fn(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Embeddings(nn.Module):\n    \"\"\"Construct the embeddings from patch, position embeddings.\n    \"\"\"\n    def __init__(self, config, img_size, in_channels=3):\n        super(Embeddings, self).__init__()\n        self.hybrid = None\n        self.config = config\n        img_size = _pair(img_size)\n\n        if config.patches.get(\"grid\") is not None:   # ResNet\n            grid_size = config.patches[\"grid\"]\n            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n            patch_size_real = (patch_size[0] * 16, patch_size[1] * 16)\n            n_patches = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1])  \n            self.hybrid = True\n        else:\n            patch_size = _pair(config.patches[\"size\"])\n            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n            self.hybrid = False\n\n        if self.hybrid:\n            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers, width_factor=config.resnet.width_factor)\n            in_channels = self.hybrid_model.width * 16\n        self.patch_embeddings = Conv2d(in_channels=in_channels,\n                                       out_channels=config.hidden_size,\n                                       kernel_size=patch_size,\n                                       stride=patch_size)\n        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, config.hidden_size))\n\n        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n\n\n    def forward(self, x):\n        if self.hybrid:\n            x, features = self.hybrid_model(x)\n        else:\n            features = None\n        x = self.patch_embeddings(x)  # (B, hidden. n_patches^(1/2), n_patches^(1/2))\n        x = x.flatten(2)\n        x = x.transpose(-1, -2)  # (B, n_patches, hidden)\n\n        embeddings = x + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n        return embeddings, features\n\n\nclass Block(nn.Module):\n    def __init__(self, config, vis):\n        super(Block, self).__init__()\n        self.hidden_size = config.hidden_size\n        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        self.ffn = Mlp(config)\n        self.attn = Attention(config, vis)\n\n    def forward(self, x):\n        h = x\n        x = self.attention_norm(x)\n        x, weights = self.attn(x)\n        x = x + h\n\n        h = x\n        x = self.ffn_norm(x)\n        x = self.ffn(x)\n        x = x + h\n        return x, weights\n\n    def load_from(self, weights, n_block):\n        ROOT = f\"Transformer/encoderblock_{n_block}\"\n        with torch.no_grad():\n            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n\n            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n\n            self.attn.query.weight.copy_(query_weight)\n            self.attn.key.weight.copy_(key_weight)\n            self.attn.value.weight.copy_(value_weight)\n            self.attn.out.weight.copy_(out_weight)\n            self.attn.query.bias.copy_(query_bias)\n            self.attn.key.bias.copy_(key_bias)\n            self.attn.value.bias.copy_(value_bias)\n            self.attn.out.bias.copy_(out_bias)\n\n            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n\n            self.ffn.fc1.weight.copy_(mlp_weight_0)\n            self.ffn.fc2.weight.copy_(mlp_weight_1)\n            self.ffn.fc1.bias.copy_(mlp_bias_0)\n            self.ffn.fc2.bias.copy_(mlp_bias_1)\n\n            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n\n\nclass Encoder(nn.Module):\n    def __init__(self, config, vis):\n        super(Encoder, self).__init__()\n        self.vis = vis\n        self.layer = nn.ModuleList()\n        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        for _ in range(config.transformer[\"num_layers\"]):\n            layer = Block(config, vis)\n            self.layer.append(copy.deepcopy(layer))\n\n    def forward(self, hidden_states):\n        attn_weights = []\n        for layer_block in self.layer:\n            hidden_states, weights = layer_block(hidden_states)\n            if self.vis:\n                attn_weights.append(weights)\n        encoded = self.encoder_norm(hidden_states)\n        return encoded, attn_weights\n\n\nclass Transformer(nn.Module):\n    def __init__(self, config, img_size, vis):\n        super(Transformer, self).__init__()\n        self.embeddings = Embeddings(config, img_size=img_size)\n        self.encoder = Encoder(config, vis)\n\n    def forward(self, input_ids):\n        embedding_output, features = self.embeddings(input_ids)\n        encoded, attn_weights = self.encoder(embedding_output)  # (B, n_patch, hidden)\n        return encoded, attn_weights, features\n\n\nclass Conv2dReLU(nn.Sequential):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding=0,\n            stride=1,\n            use_batchnorm=True,\n    ):\n        conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=not (use_batchnorm),\n        )\n        relu = nn.ReLU(inplace=True)\n\n        bn = nn.BatchNorm2d(out_channels)\n\n        super(Conv2dReLU, self).__init__(conv, bn, relu)\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            skip_channels=0,\n            use_batchnorm=True,\n    ):\n        super().__init__()\n        self.conv1 = Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.conv2 = Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n    def forward(self, x, skip=None):\n        x = self.up(x)\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\n\nclass SegmentationHead(nn.Sequential):\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n        super().__init__(conv2d, upsampling)\n\n\nclass DecoderCup(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        head_channels = 512\n        self.conv_more = Conv2dReLU(\n            config.hidden_size,\n            head_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=True,\n        )\n        decoder_channels = config.decoder_channels\n        in_channels = [head_channels] + list(decoder_channels[:-1])\n        out_channels = decoder_channels\n\n        if self.config.n_skip != 0:\n            skip_channels = self.config.skip_channels\n            for i in range(4-self.config.n_skip):  # re-select the skip channels according to n_skip\n                skip_channels[3-i]=0\n\n        else:\n            skip_channels=[0,0,0,0]\n\n        blocks = [\n            DecoderBlock(in_ch, out_ch, sk_ch) for in_ch, out_ch, sk_ch in zip(in_channels, out_channels, skip_channels)\n        ]\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, hidden_states, features=None):\n        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n        x = hidden_states.permute(0, 2, 1)\n        x = x.contiguous().view(B, hidden, h, w)\n        x = self.conv_more(x)\n        for i, decoder_block in enumerate(self.blocks):\n            if features is not None:\n                skip = features[i] if (i < self.config.n_skip) else None\n            else:\n                skip = None\n            x = decoder_block(x, skip=skip)\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n        super(VisionTransformer, self).__init__()\n        self.num_classes = num_classes\n        self.zero_head = zero_head\n        self.classifier = config.classifier\n        self.transformer = Transformer(config, img_size, vis)\n        self.decoder = DecoderCup(config)\n        self.segmentation_head = SegmentationHead(\n            in_channels=config['decoder_channels'][-1],\n            out_channels=config['n_classes'],\n            kernel_size=3,\n        )\n        self.config = config\n\n    def forward(self, x):\n        if x.size()[1] == 1:\n            x = x.repeat(1,3,1,1)\n        x, attn_weights, features = self.transformer(x)  # (B, n_patch, hidden)\n        x = self.decoder(x, features)\n        logits = self.segmentation_head(x)\n        return logits\n\n    def load_from(self, weights):\n        with torch.no_grad():\n\n            res_weight = weights\n            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n\n            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n\n            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n\n            posemb_new = self.transformer.embeddings.position_embeddings\n            if posemb.size() == posemb_new.size():\n                self.transformer.embeddings.position_embeddings.copy_(posemb)\n            elif posemb.size()[1]-1 == posemb_new.size()[1]:\n                posemb = posemb[:, 1:]\n                self.transformer.embeddings.position_embeddings.copy_(posemb)\n            else:\n                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n                ntok_new = posemb_new.size(1)\n                if self.classifier == \"seg\":\n                    _, posemb_grid = posemb[:, :1], posemb[0, 1:]\n                gs_old = int(np.sqrt(len(posemb_grid)))\n                gs_new = int(np.sqrt(ntok_new))\n                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)  # th2np\n                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n                posemb = posemb_grid\n                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n\n            # Encoder whole\n            for bname, block in self.transformer.encoder.named_children():\n                for uname, unit in block.named_children():\n                    unit.load_from(weights, n_block=uname)\n\n            if self.transformer.embeddings.hybrid:\n                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(res_weight[\"conv_root/kernel\"], conv=True))\n                gn_weight = np2th(res_weight[\"gn_root/scale\"]).view(-1)\n                gn_bias = np2th(res_weight[\"gn_root/bias\"]).view(-1)\n                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n\n                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n                    for uname, unit in block.named_children():\n                        unit.load_from(res_weight, n_block=bname, n_unit=uname)\n\nCONFIGS = {\n    'ViT-B_16': get_b16_config(),\n    'ViT-B_32': get_b32_config(),\n    'ViT-L_16': get_l16_config(),\n    'ViT-L_32': get_l32_config(),\n    'ViT-H_14': get_h14_config(),\n    'R50-ViT-B_16': get_r50_b16_config(),\n    'R50-ViT-L_16': get_r50_l16_config(),\n    'testing': get_testing(),\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T07:04:24.333220Z","iopub.execute_input":"2023-09-08T07:04:24.333579Z","iopub.status.idle":"2023-09-08T07:04:24.419369Z","shell.execute_reply.started":"2023-09-08T07:04:24.333545Z","shell.execute_reply":"2023-09-08T07:04:24.418249Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### vit_seg_modeling_resnet_skip.py\n\n残差网络ResNet模型","metadata":{}},{"cell_type":"code","source":"import math\n\nfrom os.path import join as pjoin\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef np2th(weights, conv=False):\n    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n    if conv:\n        weights = weights.transpose([3, 2, 0, 1])\n    return torch.from_numpy(weights)\n\n\nclass StdConv2d(nn.Conv2d):\n\n    def forward(self, x):\n        w = self.weight\n        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n        w = (w - m) / torch.sqrt(v + 1e-5)\n        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n                        self.dilation, self.groups)\n\n\ndef conv3x3(cin, cout, stride=1, groups=1, bias=False):\n    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n                     padding=1, bias=bias, groups=groups)\n\n\ndef conv1x1(cin, cout, stride=1, bias=False):\n    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n                     padding=0, bias=bias)\n\n\nclass PreActBottleneck(nn.Module):\n    \"\"\"Pre-activation (v2) bottleneck block.\n    \"\"\"\n\n    def __init__(self, cin, cout=None, cmid=None, stride=1):\n        super().__init__()\n        cout = cout or cin\n        cmid = cmid or cout//4\n\n        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n        self.conv1 = conv1x1(cin, cmid, bias=False)\n        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n        self.conv3 = conv1x1(cmid, cout, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n\n        if (stride != 1 or cin != cout):\n            # Projection also with pre-activation according to paper.\n            self.downsample = conv1x1(cin, cout, stride, bias=False)\n            self.gn_proj = nn.GroupNorm(cout, cout)\n\n    def forward(self, x):\n\n        # Residual branch\n        residual = x\n        if hasattr(self, 'downsample'):\n            residual = self.downsample(x)\n            residual = self.gn_proj(residual)\n\n        # Unit's branch\n        y = self.relu(self.gn1(self.conv1(x)))\n        y = self.relu(self.gn2(self.conv2(y)))\n        y = self.gn3(self.conv3(y))\n\n        y = self.relu(residual + y)\n        return y\n\n    def load_from(self, weights, n_block, n_unit):\n        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n\n        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n\n        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n\n        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n\n        self.conv1.weight.copy_(conv1_weight)\n        self.conv2.weight.copy_(conv2_weight)\n        self.conv3.weight.copy_(conv3_weight)\n\n        self.gn1.weight.copy_(gn1_weight.view(-1))\n        self.gn1.bias.copy_(gn1_bias.view(-1))\n\n        self.gn2.weight.copy_(gn2_weight.view(-1))\n        self.gn2.bias.copy_(gn2_bias.view(-1))\n\n        self.gn3.weight.copy_(gn3_weight.view(-1))\n        self.gn3.bias.copy_(gn3_bias.view(-1))\n\n        if hasattr(self, 'downsample'):\n            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n\n            self.downsample.weight.copy_(proj_conv_weight)\n            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))\n\nclass ResNetV2(nn.Module):\n    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n\n    def __init__(self, block_units, width_factor):\n        super().__init__()\n        width = int(64 * width_factor)\n        self.width = width\n\n        self.root = nn.Sequential(OrderedDict([\n            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n            ('relu', nn.ReLU(inplace=True)),\n            # ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n        ]))\n\n        self.body = nn.Sequential(OrderedDict([\n            ('block1', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n                ))),\n            ('block2', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n                ))),\n            ('block3', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n                ))),\n        ]))\n\n    def forward(self, x):\n        features = []\n        b, c, in_size, _ = x.size()\n        x = self.root(x)\n        features.append(x)\n        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)(x)\n        for i in range(len(self.body)-1):\n            x = self.body[i](x)\n            right_size = int(in_size / 4 / (i+1))\n            if x.size()[2] != right_size:\n                pad = right_size - x.size()[2]\n                assert pad < 3 and pad > 0, \"x {} should {}\".format(x.size(), right_size)\n                feat = torch.zeros((b, x.size()[1], right_size, right_size), device=x.device)\n                feat[:, :, 0:x.size()[2], 0:x.size()[3]] = x[:]\n            else:\n                feat = x\n            features.append(feat)\n        x = self.body[-1](x)\n        return x, features[::-1]","metadata":{"execution":{"iopub.status.busy":"2023-09-08T07:04:24.421193Z","iopub.execute_input":"2023-09-08T07:04:24.421986Z","iopub.status.idle":"2023-09-08T07:04:24.461485Z","shell.execute_reply.started":"2023-09-08T07:04:24.421948Z","shell.execute_reply":"2023-09-08T07:04:24.460458Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## train","metadata":{}},{"cell_type":"markdown","source":"### trianer.py\n\n用函数定义训练过程","metadata":{}},{"cell_type":"code","source":"import argparse\nimport logging\nimport os\nimport random\nimport sys\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tensorboardX import SummaryWriter\nfrom torch.nn.modules.loss import CrossEntropyLoss\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n# from utils import DiceLoss\nfrom torchvision import transforms\n\ndef trainer_synapse(args, model, snapshot_path):\n#     from datasets.dataset_synapse import Synapse_dataset, RandomGenerator\n    logging.basicConfig(filename=snapshot_path + \"/log.txt\", level=logging.INFO,\n                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.info(str(args))\n    base_lr = args.base_lr\n    num_classes = args.num_classes\n    batch_size = args.batch_size * args.n_gpu\n    # max_iterations = args.max_iterations\n    db_train = Synapse_dataset(base_dir=args.root_path, list_dir=args.list_dir, split=\"train\",\n                               transform=transforms.Compose(\n                                   [RandomGenerator(output_size=[args.img_size, args.img_size])]))\n    print(\"The length of train set is: {}\".format(len(db_train)))\n\n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n\n    trainloader = DataLoader(db_train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True,\n                             worker_init_fn=worker_init_fn)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    model.train()\n    ce_loss = CrossEntropyLoss()\n    dice_loss = DiceLoss(num_classes)\n    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n    writer = SummaryWriter(snapshot_path + '/log')\n    iter_num = 0\n    max_epoch = args.max_epochs\n    max_iterations = args.max_epochs * len(trainloader)  # max_epoch = max_iterations // len(trainloader) + 1\n    logging.info(\"{} iterations per epoch. {} max iterations \".format(len(trainloader), max_iterations))\n    best_performance = 0.0\n    iterator = tqdm(range(max_epoch), ncols=70)\n    for epoch_num in iterator:\n        for i_batch, sampled_batch in enumerate(trainloader):\n            image_batch, label_batch = sampled_batch['image'], sampled_batch['label']\n            image_batch, label_batch = image_batch.cuda(), label_batch.cuda()\n            outputs = model(image_batch)\n            loss_ce = ce_loss(outputs, label_batch[:].long())\n            loss_dice = dice_loss(outputs, label_batch, softmax=True)\n            loss = 0.5 * loss_ce + 0.5 * loss_dice\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            lr_ = base_lr * (1.0 - iter_num / max_iterations) ** 0.9\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr_\n\n            iter_num = iter_num + 1\n            writer.add_scalar('info/lr', lr_, iter_num)\n            writer.add_scalar('info/total_loss', loss, iter_num)\n            writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n            \n            if iter_num % 100 == 0:\n                logging.info('iteration %d : loss : %f, loss_ce: %f' % (iter_num, loss.item(), loss_ce.item()))\n\n            if iter_num % 20 == 0:\n                image = image_batch[1, 0:1, :, :]\n                image = (image - image.min()) / (image.max() - image.min())\n                writer.add_image('train/Image', image, iter_num)\n                outputs = torch.argmax(torch.softmax(outputs, dim=1), dim=1, keepdim=True)\n                writer.add_image('train/Prediction', outputs[1, ...] * 50, iter_num)\n                labs = label_batch[1, ...].unsqueeze(0) * 50\n                writer.add_image('train/GroundTruth', labs, iter_num)\n\n        save_interval = 50  # int(max_epoch/6)\n        if epoch_num > int(max_epoch / 2) and (epoch_num + 1) % save_interval == 0:\n            save_mode_path = os.path.join(snapshot_path, 'epoch_' + str(epoch_num) + '.pth')\n            torch.save(model.state_dict(), save_mode_path)\n            logging.info(\"save model to {}\".format(save_mode_path))\n\n        if epoch_num >= max_epoch - 1:\n            save_mode_path = os.path.join(snapshot_path, 'epoch_' + str(epoch_num) + '.pth')\n            torch.save(model.state_dict(), save_mode_path)\n            logging.info(\"save model to {}\".format(save_mode_path))\n            iterator.close()\n            break\n\n    writer.close()\n    return \"Training Finished!\"","metadata":{"execution":{"iopub.status.busy":"2023-09-08T07:04:24.463272Z","iopub.execute_input":"2023-09-08T07:04:24.463664Z","iopub.status.idle":"2023-09-08T07:04:31.685338Z","shell.execute_reply.started":"2023-09-08T07:04:24.463626Z","shell.execute_reply":"2023-09-08T07:04:31.684389Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### train.py\n\n训练过程入口文件","metadata":{}},{"cell_type":"code","source":"import argparse\nimport logging\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\n# from networks.vit_seg_modeling import VisionTransformer as ViT_seg\n# from networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\n# from trainer import trainer_synapse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--root_path', type=str,\n                    default='/kaggle/input/transeunet-synapse/data/Synapse/train_npz', help='root dir for data')\nparser.add_argument('--dataset', type=str,\n                    default='Synapse', help='experiment_name')\nparser.add_argument('--list_dir', type=str,\n                    default='/kaggle/input/transunet-lists/lists/lists_Synapse', help='list dir')\nparser.add_argument('--num_classes', type=int,\n                    default=9, help='output channel of network')\nparser.add_argument('--max_iterations', type=int,\n                    default=30000, help='maximum epoch number to train')\nparser.add_argument('--max_epochs', type=int,\n                    default=150, help='maximum epoch number to train')\nparser.add_argument('--batch_size', type=int,\n                    default=24, help='batch_size per gpu')\nparser.add_argument('--n_gpu', type=int, default=1, help='total gpu')\nparser.add_argument('--deterministic', type=int,  default=1,\n                    help='whether use deterministic training')\nparser.add_argument('--base_lr', type=float,  default=0.01,\n                    help='segmentation network learning rate')\nparser.add_argument('--img_size', type=int,\n                    default=224, help='input patch size of network input')\nparser.add_argument('--seed', type=int,\n                    default=1234, help='random seed')\nparser.add_argument('--n_skip', type=int,\n                    default=3, help='using number of skip-connect, default is num')\nparser.add_argument('--vit_name', type=str,\n                    default='R50-ViT-B_16', help='select one vit model')\nparser.add_argument('--vit_patches_size', type=int,\n                    default=16, help='vit_patches_size, default is 16')\nparser.add_argument('-f', type=str,\n                    default='', help='unkonw')\nargs = parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    if not args.deterministic:\n        cudnn.benchmark = True\n        cudnn.deterministic = False\n    else:\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    dataset_name = args.dataset\n    dataset_config = {\n        'Synapse': {\n            'root_path': '/kaggle/input/transeunet-synapse/data/Synapse/train_npz',\n            'list_dir': '/kaggle/input/transunet-lists/lists/lists_Synapse',\n            'num_classes': 9,\n        },\n    }\n    args.num_classes = dataset_config[dataset_name]['num_classes']\n    args.root_path = dataset_config[dataset_name]['root_path']\n    args.list_dir = dataset_config[dataset_name]['list_dir']\n    args.is_pretrain = True\n    args.exp = 'TU_' + dataset_name + str(args.img_size)\n    snapshot_path = \"./model/{}/{}\".format(args.exp, 'TU')\n    snapshot_path = snapshot_path + '_pretrain' if args.is_pretrain else snapshot_path\n    snapshot_path += '_' + args.vit_name\n    snapshot_path = snapshot_path + '_skip' + str(args.n_skip)\n    snapshot_path = snapshot_path + '_vitpatch' + str(args.vit_patches_size) if args.vit_patches_size!=16 else snapshot_path\n    snapshot_path = snapshot_path+'_'+str(args.max_iterations)[0:2]+'k' if args.max_iterations != 30000 else snapshot_path\n    snapshot_path = snapshot_path + '_epo' +str(args.max_epochs) if args.max_epochs != 30 else snapshot_path\n    snapshot_path = snapshot_path+'_bs'+str(args.batch_size)\n    snapshot_path = snapshot_path + '_lr' + str(args.base_lr) if args.base_lr != 0.01 else snapshot_path\n    snapshot_path = snapshot_path + '_'+str(args.img_size)\n    snapshot_path = snapshot_path + '_s'+str(args.seed) if args.seed!=1234 else snapshot_path\n\n    if not os.path.exists(snapshot_path):\n        os.makedirs(snapshot_path)\n    config_vit = CONFIGS[args.vit_name]\n    config_vit.n_classes = args.num_classes\n    config_vit.n_skip = args.n_skip\n    if args.vit_name.find('R50') != -1:\n        config_vit.patches.grid = (int(args.img_size / args.vit_patches_size), int(args.img_size / args.vit_patches_size))\n    net = VisionTransformer(config_vit, img_size=args.img_size, num_classes=config_vit.n_classes).cuda()\n    net.load_from(weights=np.load(config_vit.pretrained_path))\n\n    trainer = {'Synapse': trainer_synapse,}\n    trainer[dataset_name](args, net, snapshot_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T07:04:31.686689Z","iopub.execute_input":"2023-09-08T07:04:31.687292Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Namespace(root_path='/kaggle/input/transeunet-synapse/data/Synapse/train_npz', dataset='Synapse', list_dir='/kaggle/input/transunet-lists/lists/lists_Synapse', num_classes=9, max_iterations=30000, max_epochs=150, batch_size=24, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, n_skip=3, vit_name='R50-ViT-B_16', vit_patches_size=16, f='/root/.local/share/jupyter/runtime/kernel-2c73da3e-2ad2-4610-ba09-c63ae174263e.json', is_pretrain=True, exp='TU_Synapse224')\nThe length of train set is: 2211\n93 iterations per epoch. 13950 max iterations \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n  1%|▏                             | 1/150 [01:42<4:15:27, 102.87s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 100 : loss : 0.439028, loss_ce: 0.110756\n","output_type":"stream"},{"name":"stderr","text":"  1%|▍                              | 2/150 [03:17<4:01:06, 97.75s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 200 : loss : 0.410243, loss_ce: 0.136760\n","output_type":"stream"},{"name":"stderr","text":"  2%|▌                              | 3/150 [04:51<3:55:47, 96.24s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 300 : loss : 0.351141, loss_ce: 0.082103\n","output_type":"stream"},{"name":"stderr","text":"  3%|▊                              | 4/150 [06:25<3:52:21, 95.49s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 400 : loss : 0.301044, loss_ce: 0.056079\n","output_type":"stream"},{"name":"stderr","text":"  3%|█                              | 5/150 [08:00<3:49:54, 95.13s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 500 : loss : 0.288834, loss_ce: 0.071275\n","output_type":"stream"},{"name":"stderr","text":"  4%|█▏                             | 6/150 [09:34<3:47:36, 94.84s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 600 : loss : 0.246746, loss_ce: 0.048445\n","output_type":"stream"},{"name":"stderr","text":"  5%|█▍                             | 7/150 [11:09<3:45:42, 94.70s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 700 : loss : 0.227343, loss_ce: 0.073694\n","output_type":"stream"},{"name":"stderr","text":"  5%|█▋                             | 8/150 [12:43<3:43:48, 94.57s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 800 : loss : 0.165020, loss_ce: 0.040341\n","output_type":"stream"},{"name":"stderr","text":"  6%|█▊                             | 9/150 [14:17<3:41:56, 94.44s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 900 : loss : 0.167847, loss_ce: 0.052268\n","output_type":"stream"},{"name":"stderr","text":"  7%|██                            | 10/150 [15:52<3:41:03, 94.74s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 1000 : loss : 0.088066, loss_ce: 0.020652\n","output_type":"stream"},{"name":"stderr","text":"  7%|██▏                           | 11/150 [17:28<3:39:47, 94.88s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 1100 : loss : 0.114278, loss_ce: 0.036003\n","output_type":"stream"},{"name":"stderr","text":"  8%|██▍                           | 12/150 [19:02<3:38:13, 94.88s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 1200 : loss : 0.098416, loss_ce: 0.021916\n","output_type":"stream"},{"name":"stderr","text":"  9%|██▌                           | 13/150 [20:37<3:36:21, 94.76s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 1300 : loss : 0.154241, loss_ce: 0.028378\n","output_type":"stream"},{"name":"stderr","text":" 10%|███                           | 15/150 [23:46<3:32:40, 94.53s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 1400 : loss : 0.081498, loss_ce: 0.033902\n","output_type":"stream"},{"name":"stderr","text":" 11%|███▏                          | 16/150 [25:20<3:30:56, 94.45s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 1500 : loss : 0.084358, loss_ce: 0.028992\n","output_type":"stream"},{"name":"stderr","text":" 11%|███▍                          | 17/150 [26:55<3:29:40, 94.59s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 1600 : loss : 0.065668, loss_ce: 0.016411\n","output_type":"stream"},{"name":"stderr","text":" 12%|███▌                          | 18/150 [28:29<3:27:59, 94.55s/it]","output_type":"stream"},{"name":"stdout","text":"iteration 1700 : loss : 0.070870, loss_ce: 0.017302\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### test.py\n使用测试集测试训练后的模型准确度","metadata":{}},{"cell_type":"code","source":"import argparse\nimport logging\nimport os\nimport random\nimport sys\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n# from datasets.dataset_synapse import Synapse_dataset\nfrom utils import test_single_volume\nfrom networks.vit_seg_modeling import VisionTransformer as ViT_seg\nfrom networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--volume_path', type=str,\n                    default='/kaggle/input/transeunet-synapse/data/Synapse/test_vol_h5', help='root dir for validation volume data')  # for acdc volume_path=root_dir\nparser.add_argument('--dataset', type=str,\n                    default='Synapse', help='experiment_name')\nparser.add_argument('--num_classes', type=int,\n                    default=4, help='output channel of network')\nparser.add_argument('--list_dir', type=str,\n                    default='/kaggle/input/transunet-lists/lists/lists_Synapse', help='list dir')\n\nparser.add_argument('--max_iterations', type=int,default=20000, help='maximum epoch number to train')\nparser.add_argument('--max_epochs', type=int, default=30, help='maximum epoch number to train')\nparser.add_argument('--batch_size', type=int, default=24,\n                    help='batch_size per gpu')\nparser.add_argument('--img_size', type=int, default=224, help='input patch size of network input')\nparser.add_argument('--is_savenii', action=\"store_true\", help='whether to save results during inference')\n\nparser.add_argument('--n_skip', type=int, default=3, help='using number of skip-connect, default is num')\nparser.add_argument('--vit_name', type=str, default='ViT-B_16', help='select one vit model')\n\nparser.add_argument('--test_save_dir', type=str, default='../predictions', help='saving prediction as nii!')\nparser.add_argument('--deterministic', type=int,  default=1, help='whether use deterministic training')\nparser.add_argument('--base_lr', type=float,  default=0.01, help='segmentation network learning rate')\nparser.add_argument('--seed', type=int, default=1234, help='random seed')\nparser.add_argument('--vit_patches_size', type=int, default=16, help='vit_patches_size, default is 16')\nargs = parser.parse_args()\n\n\ndef inference(args, model, test_save_path=None):\n    db_test = args.Dataset(base_dir=args.volume_path, split=\"test_vol\", list_dir=args.list_dir)\n    testloader = DataLoader(db_test, batch_size=1, shuffle=False, num_workers=1)\n    logging.info(\"{} test iterations per epoch\".format(len(testloader)))\n    model.eval()\n    metric_list = 0.0\n    for i_batch, sampled_batch in tqdm(enumerate(testloader)):\n        h, w = sampled_batch[\"image\"].size()[2:]\n        image, label, case_name = sampled_batch[\"image\"], sampled_batch[\"label\"], sampled_batch['case_name'][0]\n        metric_i = test_single_volume(image, label, model, classes=args.num_classes, patch_size=[args.img_size, args.img_size],\n                                      test_save_path=test_save_path, case=case_name, z_spacing=args.z_spacing)\n        metric_list += np.array(metric_i)\n        logging.info('idx %d case %s mean_dice %f mean_hd95 %f' % (i_batch, case_name, np.mean(metric_i, axis=0)[0], np.mean(metric_i, axis=0)[1]))\n    metric_list = metric_list / len(db_test)\n    for i in range(1, args.num_classes):\n        logging.info('Mean class %d mean_dice %f mean_hd95 %f' % (i, metric_list[i-1][0], metric_list[i-1][1]))\n    performance = np.mean(metric_list, axis=0)[0]\n    mean_hd95 = np.mean(metric_list, axis=0)[1]\n    logging.info('Testing performance in best val model: mean_dice : %f mean_hd95 : %f' % (performance, mean_hd95))\n    return \"Testing Finished!\"\n\n\nif __name__ == \"__main__\":\n\n    if not args.deterministic:\n        cudnn.benchmark = True\n        cudnn.deterministic = False\n    else:\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n\n    dataset_config = {\n        'Synapse': {\n            'Dataset': Synapse_dataset,\n            'volume_path': '../data/Synapse/test_vol_h5',\n            'list_dir': './lists/lists_Synapse',\n            'num_classes': 9,\n            'z_spacing': 1,\n        },\n    }\n    dataset_name = args.dataset\n    args.num_classes = dataset_config[dataset_name]['num_classes']\n    args.volume_path = dataset_config[dataset_name]['volume_path']\n    args.Dataset = dataset_config[dataset_name]['Dataset']\n    args.list_dir = dataset_config[dataset_name]['list_dir']\n    args.z_spacing = dataset_config[dataset_name]['z_spacing']\n    args.is_pretrain = True\n\n    # name the same snapshot defined in train script!\n    args.exp = 'TU_' + dataset_name + str(args.img_size)\n    snapshot_path = \"../model/{}/{}\".format(args.exp, 'TU')\n    snapshot_path = snapshot_path + '_pretrain' if args.is_pretrain else snapshot_path\n    snapshot_path += '_' + args.vit_name\n    snapshot_path = snapshot_path + '_skip' + str(args.n_skip)\n    snapshot_path = snapshot_path + '_vitpatch' + str(args.vit_patches_size) if args.vit_patches_size!=16 else snapshot_path\n    snapshot_path = snapshot_path + '_epo' + str(args.max_epochs) if args.max_epochs != 30 else snapshot_path\n    if dataset_name == 'ACDC':  # using max_epoch instead of iteration to control training duration\n        snapshot_path = snapshot_path + '_' + str(args.max_iterations)[0:2] + 'k' if args.max_iterations != 30000 else snapshot_path\n    snapshot_path = snapshot_path+'_bs'+str(args.batch_size)\n    snapshot_path = snapshot_path + '_lr' + str(args.base_lr) if args.base_lr != 0.01 else snapshot_path\n    snapshot_path = snapshot_path + '_'+str(args.img_size)\n    snapshot_path = snapshot_path + '_s'+str(args.seed) if args.seed!=1234 else snapshot_path\n\n    config_vit = CONFIGS_ViT_seg[args.vit_name]\n    config_vit.n_classes = args.num_classes\n    config_vit.n_skip = args.n_skip\n    config_vit.patches.size = (args.vit_patches_size, args.vit_patches_size)\n    if args.vit_name.find('R50') !=-1:\n        config_vit.patches.grid = (int(args.img_size/args.vit_patches_size), int(args.img_size/args.vit_patches_size))\n    net = ViT_seg(config_vit, img_size=args.img_size, num_classes=config_vit.n_classes).cuda()\n\n    snapshot = os.path.join(snapshot_path, 'best_model.pth')\n    if not os.path.exists(snapshot): snapshot = snapshot.replace('best_model', 'epoch_'+str(args.max_epochs-1))\n    net.load_state_dict(torch.load(snapshot))\n    snapshot_name = snapshot_path.split('/')[-1]\n\n    log_folder = './test_log/test_log_' + args.exp\n    os.makedirs(log_folder, exist_ok=True)\n    logging.basicConfig(filename=log_folder + '/'+snapshot_name+\".txt\", level=logging.INFO, format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.info(str(args))\n    logging.info(snapshot_name)\n\n    if args.is_savenii:\n        args.test_save_dir = '../predictions'\n        test_save_path = os.path.join(args.test_save_dir, args.exp, snapshot_name)\n        os.makedirs(test_save_path, exist_ok=True)\n    else:\n        test_save_path = None\n    inference(args, net, test_save_path)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}