{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TransUNet\n\nhttps://github.com/Beckschen/TransUNet","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/Beckschen/TransUNet.git","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:59:09.578979Z","iopub.execute_input":"2023-09-24T03:59:09.579692Z","iopub.status.idle":"2023-09-24T03:59:10.539283Z","shell.execute_reply.started":"2023-09-24T03:59:09.579653Z","shell.execute_reply":"2023-09-24T03:59:10.538158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 准备依赖环境","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision numpy scipy tqdm tensorboard tensorboardX ml-collections medpy SimpleITK h5py\n!pip uninstall -y datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 准备数据集\n参考 https://github.com/Beckschen/TransUNet/tree/main#2-prepare-data\n\n#### 下载Google imagenet 21预训练模型¶\n关于ViT预训练模型简介：\n\nhttps://github.com/google-research/vision_transformer\nhttps://zhuanlan.zhihu.com/p/445122996","metadata":{}},{"cell_type":"code","source":"!mkdir model && gsutil -m cp \\\n  \"gs://vit_models/imagenet21k/R26+ViT-B_32.npz\" \\\n  \"gs://vit_models/imagenet21k/R50+ViT-B_16.npz\" \\\n  \"gs://vit_models/imagenet21k/R50+ViT-L_32.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-B_16.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-B_32.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-B_8.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-H_14.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-L_16.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-L_32.npz\" \\\n  ./model\n\n# 下载一次就够了","metadata":{"execution":{"iopub.status.busy":"2023-09-24T04:05:37.205547Z","iopub.execute_input":"2023-09-24T04:05:37.205926Z","iopub.status.idle":"2023-09-24T04:07:10.672321Z","shell.execute_reply.started":"2023-09-24T04:05:37.205898Z","shell.execute_reply":"2023-09-24T04:07:10.671158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 训练模型","metadata":{}},{"cell_type":"markdown","source":"## 使用dltrack数据","metadata":{}},{"cell_type":"code","source":" %%writefile /kaggle/working/TransUNet/networks/vit_seg_configs.py\n# %load /kaggle/working/TransUNet/networks/vit_seg_configs.py\nimport ml_collections\n\ndef get_b16_config():\n    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 768\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 3072\n    config.transformer.num_heads = 12\n    config.transformer.num_layers = 12\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n\n    config.classifier = 'seg'\n    config.representation_size = None\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '/kaggle/working/model/ViT-B_16.npz'\n    config.patch_size = 16\n\n    config.decoder_channels = (256, 128, 64, 16)\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_testing():\n    \"\"\"Returns a minimal configuration for testing.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 1\n    config.transformer.num_heads = 1\n    config.transformer.num_layers = 1\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config\n\ndef get_r50_b16_config():\n    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.pretrained_path = '/kaggle/working/model/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 2\n    config.n_skip = 3\n    config.activation = 'softmax'\n\n    return config\n\n\ndef get_b32_config():\n    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.size = (32, 32)\n    config.pretrained_path = '/kaggle/working/model/ViT-B_32.npz'\n    return config\n\n\ndef get_l16_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1024\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 4096\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 24\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.representation_size = None\n\n    # custom\n    config.classifier = 'seg'\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '/kaggle/working/model/ViT-L_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_r50_l16_config():\n    \"\"\"Returns the Resnet50 + ViT-L/16 configuration. customized \"\"\"\n    config = get_l16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.resnet_pretrained_path = '/kaggle/working/model/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_l32_config():\n    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n    config = get_l16_config()\n    config.patches.size = (32, 32)\n    return config\n\n\ndef get_h14_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n    config.hidden_size = 1280\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 5120\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 32\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n\n    return config\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T04:01:12.156070Z","iopub.execute_input":"2023-09-24T04:01:12.156629Z","iopub.status.idle":"2023-09-24T04:01:12.168663Z","shell.execute_reply.started":"2023-09-24T04:01:12.156599Z","shell.execute_reply":"2023-09-24T04:01:12.167536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/TransUNet/datasets/dataset_apo.py\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset\n\nfrom PIL import Image\nfrom typing import Tuple\nfrom pathlib import Path\nclass APODataSet(Dataset):\n    # 格式不对的异常数据\n    invalid_img = [10, 184, 185]\n    def __init__(self, img_dir, mask_dir: str, size) -> None:\n        # 获取所有图片路径\n        self.img_paths = list(Path(img_dir).glob(\"*\"))\n        self.mask_paths = list(Path(mask_dir).glob(\"*\"))\n        for idx in self.invalid_img:\n            del self.img_paths[idx]\n            del self.mask_paths[idx]\n        \n        \n        # 设置 transforms\n        self.transform = transforms.Compose([transforms.Resize(size), transforms.ToTensor()])\n#         self.transform = transforms.Compose([transforms.PILToTensor()])\n\n    # 使用函数加载原始图像\n    def load_orig_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.img_paths[index]\n        return Image.open(image_path) \n    \n    # 使用函数加载mask图像\n    def load_mask_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.mask_paths[index]\n        return Image.open(image_path) \n\n    #  重写 __len__() 方法 (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.img_paths)\n\n    # 重写 __getitem__() 方法 (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"Returns one sample of data, image and mask (X, y).\"\n        orig_img = self.load_orig_image(index)\n        mask_img = self.load_mask_image(index)\n        \n        orig_img = self.transform(orig_img)\n        mask_img = self.transform(mask_img)\n        mask_img = mask_img[0]\n#         if orig_img.size()[0] != 3:\n#             print(\"{}: orig_img size: {}\".format(index,orig_img.size()))\n#             return None\n        # return data, mask (X, y)\n        return orig_img, mask_img\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:08:34.134474Z","iopub.execute_input":"2023-09-24T05:08:34.135359Z","iopub.status.idle":"2023-09-24T05:08:34.144386Z","shell.execute_reply.started":"2023-09-24T05:08:34.135315Z","shell.execute_reply":"2023-09-24T05:08:34.143434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/TransUNet/trainer_apo.py\nimport argparse\nimport logging\nimport os\nimport random\nimport sys\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tensorboardX import SummaryWriter\nfrom torch.nn.modules.loss import CrossEntropyLoss\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n# from utils import DiceLoss\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, random_split\n\n\nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n#         inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n        return 1 - dice\n\ndef trainer_apo(args, model, snapshot_path):\n    from datasets.dataset_apo import APODataSet\n    logging.basicConfig(filename=snapshot_path + \"/log.txt\", level=logging.INFO,\n                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.info(str(args))\n    \n    base_lr = args.base_lr\n    num_classes = args.num_classes\n    batch_size = args.batch_size * args.n_gpu\n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n    \n    dataset =  APODataSet(img_dir = \"/kaggle/input/dltrack/apo_images\",\n                      mask_dir = \"/kaggle/input/dltrack/apo_masks\",\n                     size = [args.img_size, args.img_size])\n\n    total = len(dataset)\n    train_size = int(0.8*total)\n    validate_size = total - train_size\n    train_data, validate_data = random_split(dataset, [train_size, validate_size])\n    print(\"dataset info\\ntotal: {}, train_size: {}, validate_size: {}\".format(total, len(train_data), len(validate_data)))\n\n    trainloader = DataLoader(dataset=train_data,\n                                     batch_size=batch_size,\n                                     num_workers=2,\n                                     pin_memory=True,\n                                     worker_init_fn=worker_init_fn,\n                                     shuffle=True)\n\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    model.train()\n    ce_loss = CrossEntropyLoss()\n    dice_loss = DiceLoss()\n    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n    writer = SummaryWriter(snapshot_path + '/log')\n    iter_num = 0\n    max_epoch = args.max_epochs\n    max_iterations = args.max_epochs * len(trainloader)  # max_epoch = max_iterations // len(trainloader) + 1\n    logging.info(\"{} iterations per epoch. {} max iterations \".format(len(trainloader), max_iterations))\n    best_performance = 0.0\n    iterator = tqdm(range(max_epoch), ncols=70)\n    for epoch_num in iterator:\n        for i_batch, sampled_batch in enumerate(trainloader):\n            image_batch, label_batch = sampled_batch\n            image_batch, label_batch = image_batch.cuda(), label_batch.cuda()\n            outputs = model(image_batch)\n#             loss_ce = ce_loss(outputs, label_batch)\n#             label_batch = label_batch.squeeze(dim=1)\n#             loss_dice = dice_loss(outputs, label_batch, softmax=True)\n            loss_dice = dice_loss(outputs, label_batch)\n#             loss = 0.5 * loss_ce + 0.5 * loss_dice\n            loss = loss_dice\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            lr_ = base_lr * (1.0 - iter_num / max_iterations) ** 0.9\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr_\n\n            iter_num = iter_num + 1\n            writer.add_scalar('info/lr', lr_, iter_num)\n            writer.add_scalar('info/total_loss', loss, iter_num)\n#             writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n\n            logging.info('iteration %d : loss : %f' % (iter_num, loss.item()))\n\n            if iter_num % 20 == 0:\n                image = image_batch[1, 0:1, :, :]\n                image = (image - image.min()) / (image.max() - image.min())\n                writer.add_image('train/Image', image, iter_num)\n                outputs = torch.argmax(torch.softmax(outputs, dim=1), dim=1, keepdim=True)\n                writer.add_image('train/Prediction', outputs[1, ...] * 50, iter_num)\n                labs = label_batch[1, ...].unsqueeze(0) * 50\n                writer.add_image('train/GroundTruth', labs, iter_num)\n\n        save_interval = 50  # int(max_epoch/6)\n        if epoch_num > int(max_epoch / 2) and (epoch_num + 1) % save_interval == 0:\n            save_mode_path = os.path.join(snapshot_path, 'epoch_' + str(epoch_num) + '.pth')\n            torch.save(model.state_dict(), save_mode_path)\n            logging.info(\"save model to {}\".format(save_mode_path))\n\n        if epoch_num >= max_epoch - 1:\n            save_mode_path = os.path.join(snapshot_path, 'epoch_' + str(epoch_num) + '.pth')\n            torch.save(model.state_dict(), save_mode_path)\n            logging.info(\"save model to {}\".format(save_mode_path))\n            iterator.close()\n            break\n\n    writer.close()\n    return \"Training Finished!\"","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:09:40.567734Z","iopub.execute_input":"2023-09-24T05:09:40.568140Z","iopub.status.idle":"2023-09-24T05:09:40.578832Z","shell.execute_reply.started":"2023-09-24T05:09:40.568107Z","shell.execute_reply":"2023-09-24T05:09:40.577882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/TransUNet/train-apo.py\nimport argparse\nimport logging\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom networks.vit_seg_modeling import VisionTransformer as ViT_seg\nfrom networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\nfrom trainer_apo import trainer_apo\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--root_path', type=str,\n                    default='../data/Synapse/train_npz', help='root dir for data')\nparser.add_argument('--dataset', type=str,\n                    default='Synapse', help='experiment_name')\nparser.add_argument('--list_dir', type=str,\n                    default='./lists/lists_Synapse', help='list dir')\nparser.add_argument('--num_classes', type=int,\n                    default=9, help='output channel of network')\nparser.add_argument('--max_iterations', type=int,\n                    default=30000, help='maximum epoch number to train')\nparser.add_argument('--max_epochs', type=int,\n                    default=150, help='maximum epoch number to train')\nparser.add_argument('--batch_size', type=int,\n                    default=24, help='batch_size per gpu')\nparser.add_argument('--n_gpu', type=int, default=1, help='total gpu')\nparser.add_argument('--deterministic', type=int,  default=1,\n                    help='whether use deterministic training')\nparser.add_argument('--base_lr', type=float,  default=0.01,\n                    help='segmentation network learning rate')\nparser.add_argument('--img_size', type=int,\n                    default=224, help='input patch size of network input')\nparser.add_argument('--seed', type=int,\n                    default=1234, help='random seed')\nparser.add_argument('--n_skip', type=int,\n                    default=3, help='using number of skip-connect, default is num')\nparser.add_argument('--vit_name', type=str,\n                    default='R50-ViT-B_16', help='select one vit model')\nparser.add_argument('--vit_patches_size', type=int,\n                    default=16, help='vit_patches_size, default is 16')\nargs = parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    if not args.deterministic:\n        cudnn.benchmark = True\n        cudnn.deterministic = False\n    else:\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    dataset_name = args.dataset\n    dataset_config = {\n        'Synapse': {\n            'root_path': '../data/Synapse/train_npz',\n            'list_dir': './lists/lists_Synapse',\n            'num_classes': 9,\n        },\n        'dltrack': {\n            'root_path': '',\n            'list_dir': '',\n            'num_classes': 1,\n        },\n    }\n    args.num_classes = dataset_config[dataset_name]['num_classes']\n    args.root_path = dataset_config[dataset_name]['root_path']\n    args.list_dir = dataset_config[dataset_name]['list_dir']\n    args.is_pretrain = True\n    args.exp = 'TU_' + dataset_name + str(args.img_size)\n    snapshot_path = \"../model/{}/{}\".format(args.exp, 'TU')\n    snapshot_path = snapshot_path + '_pretrain' if args.is_pretrain else snapshot_path\n    snapshot_path += '_' + args.vit_name\n    snapshot_path = snapshot_path + '_skip' + str(args.n_skip)\n    snapshot_path = snapshot_path + '_vitpatch' + str(args.vit_patches_size) if args.vit_patches_size!=16 else snapshot_path\n    snapshot_path = snapshot_path+'_'+str(args.max_iterations)[0:2]+'k' if args.max_iterations != 30000 else snapshot_path\n    snapshot_path = snapshot_path + '_epo' +str(args.max_epochs) if args.max_epochs != 30 else snapshot_path\n    snapshot_path = snapshot_path+'_bs'+str(args.batch_size)\n    snapshot_path = snapshot_path + '_lr' + str(args.base_lr) if args.base_lr != 0.01 else snapshot_path\n    snapshot_path = snapshot_path + '_'+str(args.img_size)\n    snapshot_path = snapshot_path + '_s'+str(args.seed) if args.seed!=1234 else snapshot_path\n\n    if not os.path.exists(snapshot_path):\n        os.makedirs(snapshot_path)\n    config_vit = CONFIGS_ViT_seg[args.vit_name]\n    config_vit.n_classes = args.num_classes\n    config_vit.n_skip = args.n_skip\n    if args.vit_name.find('R50') != -1:\n        config_vit.patches.grid = (int(args.img_size / args.vit_patches_size), int(args.img_size / args.vit_patches_size))\n    net = ViT_seg(config_vit, img_size=args.img_size, num_classes=config_vit.n_classes).cuda()\n    net.load_from(weights=np.load(config_vit.pretrained_path))\n\n    trainer = {'dltrack': trainer_apo,}\n    trainer[dataset_name](args, net, snapshot_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T04:01:12.204288Z","iopub.execute_input":"2023-09-24T04:01:12.205033Z","iopub.status.idle":"2023-09-24T04:01:12.219652Z","shell.execute_reply.started":"2023-09-24T04:01:12.204958Z","shell.execute_reply":"2023-09-24T04:01:12.218750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd TransUNet && python train-apo.py --dataset dltrack --vit_name R50-ViT-B_16","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:09:44.228330Z","iopub.execute_input":"2023-09-24T05:09:44.229070Z"},"trusted":true},"execution_count":null,"outputs":[]}]}