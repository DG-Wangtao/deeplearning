{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TransUNet\n\nhttps://github.com/Beckschen/TransUNet","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/Beckschen/TransUNet.git","metadata":{"execution":{"iopub.status.busy":"2023-10-05T17:18:33.401061Z","iopub.execute_input":"2023-10-05T17:18:33.401417Z","iopub.status.idle":"2023-10-05T17:18:34.453217Z","shell.execute_reply.started":"2023-10-05T17:18:33.401389Z","shell.execute_reply":"2023-10-05T17:18:34.451887Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"fatal: destination path 'TransUNet' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## å‡†å¤‡ä¾èµ–ç¯å¢ƒ","metadata":{}},{"cell_type":"code","source":"# !pip install torch torchvision numpy scipy tqdm tensorboard tensorboardX ml-collections medpy SimpleITK h5py\n!pip install scipy scikit-image torch torchvision pathlib wandb segmentation-models-pytorch\n!pip install ml-collections\n!pip uninstall -y datasets","metadata":{"execution":{"iopub.status.busy":"2023-10-05T17:18:34.457626Z","iopub.execute_input":"2023-10-05T17:18:34.458087Z","iopub.status.idle":"2023-10-05T17:19:03.807859Z","shell.execute_reply.started":"2023-10-05T17:18:34.458042Z","shell.execute_reply":"2023-10-05T17:19:03.806719Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.2)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (0.21.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\nCollecting pathlib\n  Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.9)\nCollecting segmentation-models-pytorch\n  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.23.5)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (3.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2.31.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2023.4.12)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.4.1)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (21.3)\nRequirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (0.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nCollecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting timm==0.9.2 (from segmentation-models-pytorch)\n  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.1)\nCollecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.16.4)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.3.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2023.9.0)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=810db8df6fe967af7c7f4372db58778d9b6baedfbdb9b693c5cc9c83e7e24e9c\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=2d4001c5ff4309b0c09a2787f956a3aa56e0c15cc62cda1d31ad6135825d52b8\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: pathlib, munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n  Attempting uninstall: timm\n    Found existing installation: timm 0.9.7\n    Uninstalling timm-0.9.7:\n      Successfully uninstalled timm-0.9.7\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pathlib-1.0.1 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.3 timm-0.9.2\nCollecting ml-collections\n  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from ml-collections) (1.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from ml-collections) (6.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from ml-collections) (1.16.0)\nCollecting contextlib2 (from ml-collections)\n  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\nBuilding wheels for collected packages: ml-collections\n  Building wheel for ml-collections (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=ffa5ae99ca3d6ee0ea6a3f2f9643d5a63544e2404bf7a1207718878692f51aea\n  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\nSuccessfully built ml-collections\nInstalling collected packages: contextlib2, ml-collections\nSuccessfully installed contextlib2-21.6.0 ml-collections-0.1.1\nFound existing installation: datasets 2.1.0\nUninstalling datasets-2.1.0:\n  Successfully uninstalled datasets-2.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ['WANDB_API_KEY']='d561f1229ba7c4e207ca34042f29a43552a7447e'\n!wandb login","metadata":{"execution":{"iopub.status.busy":"2023-10-05T17:19:03.809743Z","iopub.execute_input":"2023-10-05T17:19:03.810119Z","iopub.status.idle":"2023-10-05T17:19:07.772500Z","shell.execute_reply.started":"2023-10-05T17:19:03.810082Z","shell.execute_reply":"2023-10-05T17:19:07.771297Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtorwayland\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### å‡†å¤‡æ•°æ®é›†\nå‚è€ƒ https://github.com/Beckschen/TransUNet/tree/main#2-prepare-data\n\n#### ä¸‹è½½Google imagenet 21é¢„è®­ç»ƒæ¨¡å‹Â¶\nå…³äºViTé¢„è®­ç»ƒæ¨¡å‹ç®€ä»‹ï¼š\n\nhttps://github.com/google-research/vision_transformer\nhttps://zhuanlan.zhihu.com/p/445122996","metadata":{}},{"cell_type":"code","source":"!mkdir model && gsutil -m cp \\\n  \"gs://vit_models/imagenet21k/R26+ViT-B_32.npz\" \\\n  \"gs://vit_models/imagenet21k/R50+ViT-B_16.npz\" \\\n  \"gs://vit_models/imagenet21k/R50+ViT-L_32.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-B_16.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-B_32.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-B_8.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-H_14.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-L_16.npz\" \\\n  \"gs://vit_models/imagenet21k/ViT-L_32.npz\" \\\n  ./model\n\n# ä¸‹è½½ä¸€æ¬¡å°±å¤Ÿäº†","metadata":{"execution":{"iopub.status.busy":"2023-10-05T17:19:07.775463Z","iopub.execute_input":"2023-10-05T17:19:07.775864Z","iopub.status.idle":"2023-10-05T17:19:08.790789Z","shell.execute_reply.started":"2023-10-05T17:19:07.775814Z","shell.execute_reply":"2023-10-05T17:19:08.789492Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory â€˜modelâ€™: File exists\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## è®­ç»ƒæ¨¡å‹","metadata":{}},{"cell_type":"code","source":"%%writefile  TransUNet/train-dl.py\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport ml_collections\nfrom networks.vit_seg_modeling import VisionTransformer as ViT_seg\nimport ml_collections\n\nimport time\nimport torch.optim as optim\nimport segmentation_models_pytorch as smp\n\nimport os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n# plt.style.use(\"ggplot\")\n# %matplotlib inline\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.nn.functional import relu, pad\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom PIL import Image\nfrom typing import Tuple\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom tqdm import tqdm\nimport wandb\nimport logging\n\n\n# TODO: imageå’Œmaskåç§°ä¸ä¸€æ ·æ—¶è·³è¿‡\nclass APODataSet(Dataset):\n    # æ ¼å¼ä¸å¯¹çš„å¼‚å¸¸æ•°æ®\n    invalid_img = [10, 184, 185]\n    def __init__(self, img_dir, mask_dir: str, size, channels = 3) -> None:\n        # è·å–æ‰€æœ‰å›¾ç‰‡è·¯å¾„\n        self.img_paths = list(Path(img_dir).glob(\"*\"))\n        self.mask_paths = list(Path(mask_dir).glob(\"*\"))\n        for idx in self.invalid_img:\n            del self.img_paths[idx]\n            del self.mask_paths[idx]\n        \n        \n        transformers = [\n            transforms.Resize(size),\n            transforms.ToTensor()\n        ]\n        if channels == 1:\n            transformers.insert(0, transforms.Grayscale(num_output_channels=1))\n\n        # è®¾ç½® transforms\n        self.transform = transforms.Compose(transformers)\n#         self.transform = transforms.Compose([transforms.PILToTensor()])\n\n    # ä½¿ç”¨å‡½æ•°åŠ è½½åŸå§‹å›¾åƒ\n    def load_orig_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.img_paths[index]\n        return Image.open(image_path) \n    \n    # ä½¿ç”¨å‡½æ•°åŠ è½½tmaskå›¾åƒ\n    def load_mask_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.mask_paths[index]\n        return Image.open(image_path) \n\n    #  é‡å†™ __len__() æ–¹æ³• (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.img_paths)\n\n    # é‡å†™ __getitem__() æ–¹æ³• (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"Returns one sample of data, image and mask (X, y).\"\n        orig_img = self.load_orig_image(index)\n        mask_img = self.load_mask_image(index)\n        \n        orig_img = self.transform(orig_img)\n        mask_img = self.transform(mask_img)\n#         mask_img = mask_img[0]\n#         if orig_img.size()[0] != 3:\n#             print(\"{}: orig_img size: {}\".format(index,orig_img.size()))\n#             return None\n        # return data, mask (X, y)\n        return orig_img, mask_img\n\n\n\n@torch.inference_mode()\ndef evaluate(net, dataloader, device, amp):\n    net.eval()\n    num_val_batches = len(dataloader)\n    dice_score = 0\n    iou_score = 0\n\n    n_classes = 1\n    criterion = nn.CrossEntropyLoss() if n_classes > 1 else nn.BCEWithLogitsLoss()\n    dice_loss = smp.losses.DiceLoss(mode='binary', log_loss=True, from_logits = True).cuda()\n   \n    \n    # iterate over the validation set\n    with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n        for batch in tqdm(dataloader, total=num_val_batches, desc='Validation round', unit='batch', position=0 ,leave=True):\n            image, mask_true = batch\n\n            # move images and labels to correct device and type\n            image = image.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n            mask_true = mask_true.to(device=device, dtype=torch.float32)\n\n            # predict the mask\n            mask_pred = net(image)\n            dice_score += criterion(mask_pred, mask_true.float())\n            dice_score += dice_loss(mask_pred, mask_true)\n            \n            tp, fp, fn, tn = smp.metrics.get_stats(mask_pred, mask_true.long(), mode='binary', threshold=0.5)\n            iou_score += smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n            \n    dice_loss = (dice_score / max(num_val_batches, 1))\n    iou_score = (iou_score / max(num_val_batches, 1))\n    print(\"Validation dice loss: {}, IoU Score {}\".format(dice_loss, iou_score))\n    return (dice_loss, iou_score)\n\ndef train(model, device, \n          project = 'U-Net',\n          epochs: int = 60,\n          learning_rate: float = 1e-5, \n          weight_decay: float = 1e-8,\n          momentum: float = 0.999,\n          batch_size: int = 2,\n          amp: bool = False,\n          val_percent: float = 0.1,\n          gradient_clipping: float = 1.0):\n    n_channels = 3\n    \n    if isinstance(model, nn.DataParallel):\n        n_classes = model.module.num_classes\n    else:\n        n_classes = model.num_classes\n        \n    # (Initialize logging)\n    experiment = wandb.init(project=project, resume='allow', anonymous='must')\n    experiment.config.update(\n        dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n             val_percent=val_percent, amp=amp)\n    )\n\n    logging.info(f'''Starting training:\n        Epochs:          {epochs}\n        Batch size:      {batch_size}\n        Learning rate:   {learning_rate}\n        Training size:   {n_train}\n        Validation size: {n_val}\n        Device:          {device.type}\n        Mixed Precision: {amp}\n    ''')\n    \n\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n#     optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],\n#                               nesterov=config['nesterov'], weight_decay=config['weight_decay'])\n\n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2, eta_min=5e-5)\n    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n    \n    criterion = nn.CrossEntropyLoss().cuda()\n    dice_loss = smp.losses.DiceLoss(mode='binary').cuda()\n    \n    global_step = 0\n    \n\n    # 5. Begin training\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss = 0\n        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n            for batch in trainloader:\n                images, true_masks = batch\n\n                assert images.shape[1] == n_channels, \\\n                    f'Network has been defined with {n_channels} input channels, ' \\\n                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n                    'the images are loaded correctly.'\n\n                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n                \n                true_masks = true_masks.to(device=device, dtype=torch.long)\n\n                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n                    masks_pred = model(images)\n                    loss = criterion(masks_pred, true_masks.float())\n                    loss += dice_loss(masks_pred, true_masks)\n                    tp, fp, fn, tn = smp.metrics.get_stats(masks_pred, true_masks.long(), mode='binary', threshold=0.5)\n                    iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n    \n                optimizer.zero_grad(set_to_none=True)\n                grad_scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n                grad_scaler.step(optimizer)\n                grad_scaler.update()\n\n                pbar.update(images.shape[0])\n                global_step += 1\n                epoch_loss += loss.item()\n                experiment.log({\n                    'train iou': iou_score,\n                    'train loss': loss.item(),\n                    'step': global_step,\n                    'epoch': epoch\n                })\n                pbar.set_postfix(**{'loss (batch)': loss.item()})\n\n                # Evaluation round\n                division_step = (n_train // (5 * batch_size))\n                if division_step > 0:\n                    if global_step % division_step == 0:\n                        histograms = {}\n                        for tag, value in model.named_parameters():\n                            tag = tag.replace('/', '.')\n                            if not (torch.isinf(value) | torch.isnan(value)).any():\n                                histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n                            if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():\n                                histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n\n                        val_score, iou_score = evaluate(model, valloader, device, amp)\n                        model.train()\n                        scheduler.step(val_score)\n\n                        logging.info('Validation Dice score: {}'.format(val_score))\n                        try:\n                            experiment.log({\n                                'learning rate': optimizer.param_groups[0]['lr'],\n                                'validation Dice': val_score,\n                                'validation IoU Score': iou_score,\n                                'images': wandb.Image(images[0].cpu()),\n                                'masks': {\n                                    'true': wandb.Image(true_masks[0].float().cpu()),\n                                    'pred': wandb.Image(masks_pred[0].float().cpu()),\n                                },\n                                'step': global_step,\n                                'epoch': epoch,\n                                **histograms\n                            })\n                        except:\n                            pass\n            \n\ndef get_b16_config():\n    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 768\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 3072\n    config.transformer.num_heads = 12\n    config.transformer.num_layers = 12\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n\n    config.classifier = 'seg'\n    config.representation_size = None\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '/kaggle/working/model/ViT-B_16.npz'\n    config.patch_size = 16\n\n    config.decoder_channels = (256, 128, 64, 16)\n    config.n_classes = 2\n    config.activation = 'softmax'\n    return config\n\n\ndef get_testing():\n    \"\"\"Returns a minimal configuration for testing.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 1\n    config.transformer.num_heads = 1\n    config.transformer.num_layers = 1\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config\n\ndef get_r50_b16_config():\n    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.pretrained_path = '/kaggle/working/model/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 1\n    config.n_skip = 3\n    config.activation = 'softmax'\n\n    return config\n\n\nimage_size = 512\n\ndataset =  APODataSet(img_dir = \"/kaggle/input/dltrack/apo_images\",\n                      mask_dir = \"/kaggle/input/dltrack/apo_masks\",\n                     size = [image_size, image_size])\n\n\ntotal = len(dataset)\ntrain_size = int(0.8*total)\nvalidate_size = total - train_size\ntrain_data, validate_data = random_split(dataset, [train_size, validate_size])\nprint(\"dataset info\\ntotal: {}, train_size: {}, validate_size: {}\".format(total, len(train_data), len(validate_data)))\n\ntrainloader = DataLoader(dataset=train_data,\n                                     batch_size=2,\n                                     num_workers=0,\n                                     shuffle=True)\n\nvalloader = DataLoader(dataset=validate_data,\n                                    batch_size=1, \n                                    num_workers=0, \n                                    shuffle=False)\n\n\nn_train = len(train_data)\nn_val = len(validate_data)\n\nconfig_vit = get_r50_b16_config()\n\n\nconfig_vit.patches.grid = (int(image_size / 16), int(image_size / 16))\nmodel = ViT_seg(config_vit, img_size=image_size, num_classes=1).cuda()\n\n# model.load_from(weights=np.load(config_vit.pretrained_path))\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(memory_format=torch.channels_last)\nmodel.to(device)\n\ntrain(model, device, project='TransUNet')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T17:19:08.793197Z","iopub.execute_input":"2023-10-05T17:19:08.793613Z","iopub.status.idle":"2023-10-05T17:19:08.809205Z","shell.execute_reply.started":"2023-10-05T17:19:08.793573Z","shell.execute_reply":"2023-10-05T17:19:08.808276Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Overwriting TransUNet/train-dl.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python TransUNet/train-dl.py","metadata":{"execution":{"iopub.status.busy":"2023-10-05T17:19:08.810919Z","iopub.execute_input":"2023-10-05T17:19:08.811416Z","iopub.status.idle":"2023-10-05T17:22:15.304484Z","shell.execute_reply.started":"2023-10-05T17:19:08.811382Z","shell.execute_reply":"2023-10-05T17:22:15.303269Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\ndataset info\ntotal: 571, train_size: 456, validate_size: 115\nLet's use 2 GPUs!\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtorwayland\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20231005_171921-h3xrfw64\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msummer-dust-11\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/torwayland/TransUNet\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/torwayland/TransUNet/runs/h3xrfw64\u001b[0m\nValidation round: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115/115 [00:18<00:00,  6.07batch/s]\nValidation dice loss: 2.840527296066284, IoU Score 0.36754536628723145\nValidation round: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115/115 [00:18<00:00,  6.32batch/s]\nValidation dice loss: 2.4272634983062744, IoU Score 0.32039129734039307\nValidation round:  13%|â–ˆâ–ˆâ–‹                  | 15/115 [00:02<00:15,  6.30batch/s]^C\nValidation round:  13%|â–ˆâ–ˆâ–‹                  | 15/115 [00:02<00:17,  5.65batch/s]\nEpoch 1/60:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 270/456 [02:22<01:38,  1.89img/s, loss (batch)=0.894]\nTraceback (most recent call last):\n  File \"/kaggle/working/TransUNet/train-dl.py\", line 358, in <module>\n    train(model, device, project='TransUNet')\n  File \"/kaggle/working/TransUNet/train-dl.py\", line 234, in train\n    val_score, iou_score = evaluate(model, valloader, device, amp)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/kaggle/working/TransUNet/train-dl.py\", line 126, in evaluate\n    iou_score += smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n  File \"/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/metrics/functional.py\", line 427, in iou_score\n    return _compute_metric(\n  File \"/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/metrics/functional.py\", line 253, in _compute_metric\n    class_weights = torch.tensor(class_weights).to(tp.device)\nKeyboardInterrupt\n\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n","output_type":"stream"}]}]}