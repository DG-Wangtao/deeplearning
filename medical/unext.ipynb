{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# UNeXt \n> https://github.com/jeya-maria-jose/UNeXt-pytorch","metadata":{}},{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"_uuid":"2a78186a-a663-4493-a796-33b0f558bf7a","_cell_guid":"1b772f0f-3a0c-4b91-b596-998d2e0fddd9","jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2023-10-05T15:18:10.469109Z","iopub.execute_input":"2023-10-05T15:18:10.469512Z","iopub.status.idle":"2023-10-05T15:18:10.482425Z","shell.execute_reply.started":"2023-10-05T15:18:10.469482Z","shell.execute_reply":"2023-10-05T15:18:10.481032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 引入依赖包","metadata":{}},{"cell_type":"code","source":"!pip install scipy scikit-image torch torchvision pathlib wandb segmentation-models-pytorch timm","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:18:10.637107Z","iopub.execute_input":"2023-10-05T15:18:10.637510Z","iopub.status.idle":"2023-10-05T15:18:20.461259Z","shell.execute_reply.started":"2023-10-05T15:18:10.637481Z","shell.execute_reply":"2023-10-05T15:18:20.459843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.nn.functional import relu, pad\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom PIL import Image\nfrom typing import Tuple\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom tqdm import tqdm\nimport wandb\nimport logging","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:18:20.464135Z","iopub.execute_input":"2023-10-05T15:18:20.464549Z","iopub.status.idle":"2023-10-05T15:18:20.476588Z","shell.execute_reply.started":"2023-10-05T15:18:20.464505Z","shell.execute_reply":"2023-10-05T15:18:20.475585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 设置wandb账号\n用作统计与数据分析","metadata":{}},{"cell_type":"code","source":"os.environ['WANDB_API_KEY']='d561f1229ba7c4e207ca34042f29a43552a7447e'\n!wandb login","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:18:20.478134Z","iopub.execute_input":"2023-10-05T15:18:20.478516Z","iopub.status.idle":"2023-10-05T15:18:22.898557Z","shell.execute_reply.started":"2023-10-05T15:18:20.478480Z","shell.execute_reply":"2023-10-05T15:18:22.897372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nimport torch.nn.functional as F\nimport os\nimport matplotlib.pyplot as plt\n# from utils import *\n\nimport timm\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nimport types\nimport math\nfrom abc import ABCMeta, abstractmethod\n# from mmcv.cnn import ConvModule\nimport pdb\n\n\n\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, bias=False)\n\n\ndef shift(dim):\n            x_shift = [ torch.roll(x_c, shift, dim) for x_c, shift in zip(xs, range(-self.pad, self.pad+1))]\n            x_cat = torch.cat(x_shift, 1)\n            x_cat = torch.narrow(x_cat, 2, self.pad, H)\n            x_cat = torch.narrow(x_cat, 3, self.pad, W)\n            return x_cat\n\nclass shiftmlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., shift_size=5):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.dim = in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n        self.shift_size = shift_size\n        self.pad = shift_size // 2\n\n        \n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n    \n#     def shift(x, dim):\n#         x = F.pad(x, \"constant\", 0)\n#         x = torch.chunk(x, shift_size, 1)\n#         x = [ torch.roll(x_c, shift, dim) for x_s, shift in zip(x, range(-pad, pad+1))]\n#         x = torch.cat(x, 1)\n#         return x[:, :, pad:-pad, pad:-pad]\n\n    def forward(self, x, H, W):\n        # pdb.set_trace()\n        B, N, C = x.shape\n\n        xn = x.transpose(1, 2).view(B, C, H, W).contiguous()\n        xn = F.pad(xn, (self.pad, self.pad, self.pad, self.pad) , \"constant\", 0)\n        xs = torch.chunk(xn, self.shift_size, 1)\n        x_shift = [torch.roll(x_c, shift, 2) for x_c, shift in zip(xs, range(-self.pad, self.pad+1))]\n        x_cat = torch.cat(x_shift, 1)\n        x_cat = torch.narrow(x_cat, 2, self.pad, H)\n        x_s = torch.narrow(x_cat, 3, self.pad, W)\n\n\n        x_s = x_s.reshape(B,C,H*W).contiguous()\n        x_shift_r = x_s.transpose(1,2)\n\n\n        x = self.fc1(x_shift_r)\n\n        x = self.dwconv(x, H, W)\n        x = self.act(x) \n        x = self.drop(x)\n\n        xn = x.transpose(1, 2).view(B, C, H, W).contiguous()\n        xn = F.pad(xn, (self.pad, self.pad, self.pad, self.pad) , \"constant\", 0)\n        xs = torch.chunk(xn, self.shift_size, 1)\n        x_shift = [torch.roll(x_c, shift, 3) for x_c, shift in zip(xs, range(-self.pad, self.pad+1))]\n        x_cat = torch.cat(x_shift, 1)\n        x_cat = torch.narrow(x_cat, 2, self.pad, H)\n        x_s = torch.narrow(x_cat, 3, self.pad, W)\n        x_s = x_s.reshape(B,C,H*W).contiguous()\n        x_shift_c = x_s.transpose(1,2)\n\n        x = self.fc2(x_shift_c)\n        x = self.drop(x)\n        return x\n\n\n\nclass shiftedBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n        super().__init__()\n\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = shiftmlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n\n        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n        return x\n\n\nclass DWConv(nn.Module):\n    def __init__(self, dim=768):\n        super(DWConv, self).__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        x = x.transpose(1, 2).view(B, C, H, W)\n        x = self.dwconv(x)\n        x = x.flatten(2).transpose(1, 2)\n\n        return x\n\nclass OverlapPatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n        self.norm = nn.LayerNorm(embed_dim)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n\n        return x, H, W\n\n\nclass UNext(nn.Module):\n\n    ## Conv 3 + MLP 2 + shifted MLP\n    \n    def __init__(self,  num_classes, input_channels=3, deep_supervision=False,img_size=224, patch_size=16, in_chans=3,  embed_dims=[ 128, 160, 256],\n                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n                 depths=[1, 1, 1], sr_ratios=[8, 4, 2, 1], **kwargs):\n        super().__init__()\n        \n        self.encoder1 = nn.Conv2d(3, 16, 3, stride=1, padding=1)  \n        self.encoder2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)  \n        self.encoder3 = nn.Conv2d(32, 128, 3, stride=1, padding=1)\n\n        self.ebn1 = nn.BatchNorm2d(16)\n        self.ebn2 = nn.BatchNorm2d(32)\n        self.ebn3 = nn.BatchNorm2d(128)\n        \n        self.norm3 = norm_layer(embed_dims[1])\n        self.norm4 = norm_layer(embed_dims[2])\n\n        self.dnorm3 = norm_layer(160)\n        self.dnorm4 = norm_layer(128)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n\n        self.block1 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.block2 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[2], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock1 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock2 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n                                              embed_dim=embed_dims[1])\n        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n                                              embed_dim=embed_dims[2])\n\n        self.decoder1 = nn.Conv2d(256, 160, 3, stride=1,padding=1)  \n        self.decoder2 =   nn.Conv2d(160, 128, 3, stride=1, padding=1)  \n        self.decoder3 =   nn.Conv2d(128, 32, 3, stride=1, padding=1) \n        self.decoder4 =   nn.Conv2d(32, 16, 3, stride=1, padding=1)\n        self.decoder5 =   nn.Conv2d(16, 16, 3, stride=1, padding=1)\n\n        self.dbn1 = nn.BatchNorm2d(160)\n        self.dbn2 = nn.BatchNorm2d(128)\n        self.dbn3 = nn.BatchNorm2d(32)\n        self.dbn4 = nn.BatchNorm2d(16)\n        \n        self.final = nn.Conv2d(16, num_classes, kernel_size=1)\n\n        self.soft = nn.Softmax(dim =1)\n\n    def forward(self, x):\n        \n        B = x.shape[0]\n        ### Encoder\n        ### Conv Stage\n\n        ### Stage 1\n        out = F.relu(F.max_pool2d(self.ebn1(self.encoder1(x)),2,2))\n        t1 = out\n        ### Stage 2\n        out = F.relu(F.max_pool2d(self.ebn2(self.encoder2(out)),2,2))\n        t2 = out\n        ### Stage 3\n        out = F.relu(F.max_pool2d(self.ebn3(self.encoder3(out)),2,2))\n        t3 = out\n\n        ### Tokenized MLP Stage\n        ### Stage 4\n\n        out,H,W = self.patch_embed3(out)\n        for i, blk in enumerate(self.block1):\n            out = blk(out, H, W)\n        out = self.norm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        t4 = out\n\n        ### Bottleneck\n\n        out ,H,W= self.patch_embed4(out)\n        for i, blk in enumerate(self.block2):\n            out = blk(out, H, W)\n        out = self.norm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        ### Stage 4\n\n        out = F.relu(F.interpolate(self.dbn1(self.decoder1(out)),scale_factor=(2,2),mode ='bilinear'))\n        \n        out = torch.add(out,t4)\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        for i, blk in enumerate(self.dblock1):\n            out = blk(out, H, W)\n\n        ### Stage 3\n        \n        out = self.dnorm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        out = F.relu(F.interpolate(self.dbn2(self.decoder2(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t3)\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        \n        for i, blk in enumerate(self.dblock2):\n            out = blk(out, H, W)\n\n        out = self.dnorm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        out = F.relu(F.interpolate(self.dbn3(self.decoder3(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t2)\n        out = F.relu(F.interpolate(self.dbn4(self.decoder4(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t1)\n        out = F.relu(F.interpolate(self.decoder5(out),scale_factor=(2,2),mode ='bilinear'))\n\n        return self.final(out)\n\n\nclass UNext_S(nn.Module):\n\n    ## Conv 3 + MLP 2 + shifted MLP w less parameters\n    \n    def __init__(self,  num_classes, input_channels=3, deep_supervision=False,img_size=224, patch_size=16, in_chans=3,  embed_dims=[32, 64, 128, 512],\n                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n                 depths=[1, 1, 1], sr_ratios=[8, 4, 2, 1], **kwargs):\n        super().__init__()\n        \n        self.encoder1 = nn.Conv2d(3, 8, 3, stride=1, padding=1)  \n        self.encoder2 = nn.Conv2d(8, 16, 3, stride=1, padding=1)  \n        self.encoder3 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n\n        self.ebn1 = nn.BatchNorm2d(8)\n        self.ebn2 = nn.BatchNorm2d(16)\n        self.ebn3 = nn.BatchNorm2d(32)\n        \n        self.norm3 = norm_layer(embed_dims[1])\n        self.norm4 = norm_layer(embed_dims[2])\n\n        self.dnorm3 = norm_layer(64)\n        self.dnorm4 = norm_layer(32)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n\n        self.block1 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.block2 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[2], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock1 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock2 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n                                              embed_dim=embed_dims[1])\n        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n                                              embed_dim=embed_dims[2])\n\n        self.decoder1 = nn.Conv2d(128, 64, 3, stride=1,padding=1)  \n        self.decoder2 =   nn.Conv2d(64, 32, 3, stride=1, padding=1)  \n        self.decoder3 =   nn.Conv2d(32, 16, 3, stride=1, padding=1) \n        self.decoder4 =   nn.Conv2d(16, 8, 3, stride=1, padding=1)\n        self.decoder5 =   nn.Conv2d(8, 8, 3, stride=1, padding=1)\n\n        self.dbn1 = nn.BatchNorm2d(64)\n        self.dbn2 = nn.BatchNorm2d(32)\n        self.dbn3 = nn.BatchNorm2d(16)\n        self.dbn4 = nn.BatchNorm2d(8)\n        \n        self.final = nn.Conv2d(8, num_classes, kernel_size=1)\n\n        self.soft = nn.Softmax(dim =1)\n\n    def forward(self, x):\n        \n        B = x.shape[0]\n        ### Encoder\n        ### Conv Stage\n\n        ### Stage 1\n        out = F.relu(F.max_pool2d(self.ebn1(self.encoder1(x)),2,2))\n        t1 = out\n        ### Stage 2\n        out = F.relu(F.max_pool2d(self.ebn2(self.encoder2(out)),2,2))\n        t2 = out\n        ### Stage 3\n        out = F.relu(F.max_pool2d(self.ebn3(self.encoder3(out)),2,2))\n        t3 = out\n\n        ### Tokenized MLP Stage\n        ### Stage 4\n\n        out,H,W = self.patch_embed3(out)\n        for i, blk in enumerate(self.block1):\n            out = blk(out, H, W)\n        out = self.norm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        t4 = out\n\n        ### Bottleneck\n\n        out ,H,W= self.patch_embed4(out)\n        for i, blk in enumerate(self.block2):\n            out = blk(out, H, W)\n        out = self.norm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        ### Stage 4\n\n        out = F.relu(F.interpolate(self.dbn1(self.decoder1(out)),scale_factor=(2,2),mode ='bilinear'))\n        \n        out = torch.add(out,t4)\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        for i, blk in enumerate(self.dblock1):\n            out = blk(out, H, W)\n\n        ### Stage 3\n        \n        out = self.dnorm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        out = F.relu(F.interpolate(self.dbn2(self.decoder2(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t3)\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        \n        for i, blk in enumerate(self.dblock2):\n            out = blk(out, H, W)\n\n        out = self.dnorm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        out = F.relu(F.interpolate(self.dbn3(self.decoder3(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t2)\n        out = F.relu(F.interpolate(self.dbn4(self.decoder4(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t1)\n        out = F.relu(F.interpolate(self.decoder5(out),scale_factor=(2,2),mode ='bilinear'))\n\n        return self.final(out)\n\n\n#EOF\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:18:22.902488Z","iopub.execute_input":"2023-10-05T15:18:22.902793Z","iopub.status.idle":"2023-10-05T15:18:22.983809Z","shell.execute_reply.started":"2023-10-05T15:18:22.902763Z","shell.execute_reply":"2023-10-05T15:18:22.982643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataSet","metadata":{}},{"cell_type":"code","source":"# TODO: image和mask名称不一样时跳过\nclass APODataSet(Dataset):\n    # 格式不对的异常数据\n    invalid_img = [10, 184, 185]\n    def __init__(self, img_dir, mask_dir: str, size, channels = 3) -> None:\n        # 获取所有图片路径\n        self.img_paths = list(Path(img_dir).glob(\"*\"))\n        self.mask_paths = list(Path(mask_dir).glob(\"*\"))\n        for idx in self.invalid_img:\n            del self.img_paths[idx]\n            del self.mask_paths[idx]\n        \n        \n        transformers = [\n            transforms.Resize(size),\n            transforms.ToTensor()\n        ]\n        if channels == 1:\n            transformers.insert(0, transforms.Grayscale(num_output_channels=1))\n\n        # 设置 transforms\n        self.transform = transforms.Compose(transformers)\n#         self.transform = transforms.Compose([transforms.PILToTensor()])\n\n    # 使用函数加载原始图像\n    def load_orig_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.img_paths[index]\n        return Image.open(image_path) \n    \n    # 使用函数加载tmask图像\n    def load_mask_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.mask_paths[index]\n        return Image.open(image_path) \n\n    #  重写 __len__() 方法 (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.img_paths)\n\n    # 重写 __getitem__() 方法 (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"Returns one sample of data, image and mask (X, y).\"\n        orig_img = self.load_orig_image(index)\n        mask_img = self.load_mask_image(index)\n        \n        orig_img = self.transform(orig_img)\n        mask_img = self.transform(mask_img)\n#         mask_img = mask_img[0]\n#         if orig_img.size()[0] != 3:\n#             print(\"{}: orig_img size: {}\".format(index,orig_img.size()))\n#             return None\n        # return data, mask (X, y)\n        return orig_img, mask_img\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:18:22.985972Z","iopub.execute_input":"2023-10-05T15:18:22.986504Z","iopub.status.idle":"2023-10-05T15:18:23.005239Z","shell.execute_reply.started":"2023-10-05T15:18:22.986442Z","shell.execute_reply":"2023-10-05T15:18:23.004179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset =  APODataSet(img_dir = \"/kaggle/input/dltrack/apo_images\",\n                      mask_dir = \"/kaggle/input/dltrack/apo_masks\",\n                     size = [512, 512])\n\ntotal = len(dataset)\ntrain_size = int(0.8*total)\nvalidate_size = total - train_size\ntrain_data, validate_data = random_split(dataset, [train_size, validate_size])\nprint(\"dataset info\\ntotal: {}, train_size: {}, validate_size: {}\".format(total, len(train_data), len(validate_data)))\n\ntrainloader = DataLoader(dataset=train_data,\n                                     batch_size=2,\n                                     num_workers=0,\n                                     shuffle=True)\n\nvalloader = DataLoader(dataset=validate_data,\n                                    batch_size=1, \n                                    num_workers=0, \n                                    shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:18:23.007207Z","iopub.execute_input":"2023-10-05T15:18:23.008307Z","iopub.status.idle":"2023-10-05T15:18:23.495407Z","shell.execute_reply.started":"2023-10-05T15:18:23.008268Z","shell.execute_reply":"2023-10-05T15:18:23.494304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 检查异常图片\n要把找到的异常数据去除","metadata":{}},{"cell_type":"code","source":"for index in range(len(dataset)):\n    orig_img, mask_img = dataset[index]\n    if orig_img.size()[0] != 3:\n        print(\"{}: orig_img size: {}\".format(index,orig_img.size()))\nprint(\"[done]\")","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:18:23.496738Z","iopub.execute_input":"2023-10-05T15:18:23.497610Z","iopub.status.idle":"2023-10-05T15:18:43.663771Z","shell.execute_reply.started":"2023-10-05T15:18:23.497571Z","shell.execute_reply":"2023-10-05T15:18:43.662507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 随机显示一张原始图片和其对应的标记图片","metadata":{}},{"cell_type":"code","source":"idx = random.randint(0, len(dataset))\norig_img, mask_img = dataset[idx]\nprint(orig_img.size())\n\ntransform = transforms.ToPILImage()\nprint(\"showing image of {}: \".format(idx))\n\norig_img = transform(orig_img)\nmask_img = transform(mask_img)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 12))\n\nax1.imshow(orig_img)\nax1.set_title(\"origin_img\")\n\nax2.imshow(mask_img)\nax2.set_title(\"mask_img\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:18:43.665718Z","iopub.execute_input":"2023-10-05T15:18:43.666536Z","iopub.status.idle":"2023-10-05T15:18:44.149316Z","shell.execute_reply.started":"2023-10-05T15:18:43.666490Z","shell.execute_reply":"2023-10-05T15:18:44.148425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 模型训练","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nimport torch.nn.functional as F\nimport os\nimport matplotlib.pyplot as plt\n\n\nimport timm\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nimport types\nimport math\nfrom abc import ABCMeta, abstractmethod\n# from mmcv.cnn import ConvModule\nimport pdb\n\n\nclass shiftmlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., shift_size=5):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.dim = in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n        self.shift_size = shift_size\n        self.pad = shift_size // 2\n\n        \n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n    \n#     def shift(x, dim):\n#         x = F.pad(x, \"constant\", 0)\n#         x = torch.chunk(x, shift_size, 1)\n#         x = [ torch.roll(x_c, shift, dim) for x_s, shift in zip(x, range(-pad, pad+1))]\n#         x = torch.cat(x, 1)\n#         return x[:, :, pad:-pad, pad:-pad]\n\n    def forward(self, x, H, W):\n        # pdb.set_trace()\n        B, N, C = x.shape\n\n        xn = x.transpose(1, 2).view(B, C, H, W).contiguous()\n        xn = F.pad(xn, (self.pad, self.pad, self.pad, self.pad) , \"constant\", 0)\n        xs = torch.chunk(xn, self.shift_size, 1)\n        x_shift = [torch.roll(x_c, shift, 2) for x_c, shift in zip(xs, range(-self.pad, self.pad+1))]\n        x_cat = torch.cat(x_shift, 1)\n        x_cat = torch.narrow(x_cat, 2, self.pad, H)\n        x_s = torch.narrow(x_cat, 3, self.pad, W)\n\n\n        x_s = x_s.reshape(B,C,H*W).contiguous()\n        x_shift_r = x_s.transpose(1,2)\n\n\n        x = self.fc1(x_shift_r)\n\n        x = self.dwconv(x, H, W)\n        x = self.act(x) \n        x = self.drop(x)\n\n        xn = x.transpose(1, 2).view(B, C, H, W).contiguous()\n        xn = F.pad(xn, (self.pad, self.pad, self.pad, self.pad) , \"constant\", 0)\n        xs = torch.chunk(xn, self.shift_size, 1)\n        x_shift = [torch.roll(x_c, shift, 3) for x_c, shift in zip(xs, range(-self.pad, self.pad+1))]\n        x_cat = torch.cat(x_shift, 1)\n        x_cat = torch.narrow(x_cat, 2, self.pad, H)\n        x_s = torch.narrow(x_cat, 3, self.pad, W)\n        x_s = x_s.reshape(B,C,H*W).contiguous()\n        x_shift_c = x_s.transpose(1,2)\n\n        x = self.fc2(x_shift_c)\n        x = self.drop(x)\n        return x\n\n\n\nclass shiftedBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n        super().__init__()\n\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = shiftmlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n\n        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n        return x\n\n\nclass DWConv(nn.Module):\n    def __init__(self, dim=768):\n        super(DWConv, self).__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        x = x.transpose(1, 2).view(B, C, H, W)\n        x = self.dwconv(x)\n        x = x.flatten(2).transpose(1, 2)\n\n        return x\n\nclass OverlapPatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=512, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n        self.norm = nn.LayerNorm(embed_dim)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n\n        return x, H, W\n\n\nclass UNext(nn.Module):\n\n    ## Conv 3 + MLP 2 + shifted MLP\n    \n    def __init__(self,  num_classes, input_channels=3, deep_supervision=False,img_size=224, patch_size=2, embed_dims=[128, 160, 256],\n                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n                 depths=[1, 1, 1], sr_ratios=[8, 4, 2, 1], **kwargs):\n        super().__init__()\n        self.n_classes = num_classes\n        self.n_channels = input_channels\n        \n        self.encoder1 = nn.Conv2d(input_channels, 16, 3, stride=1, padding=1)  \n        self.encoder2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)  \n        self.encoder3 = nn.Conv2d(32, 128, 3, stride=1, padding=1)\n\n        self.ebn1 = nn.BatchNorm2d(16)\n        self.ebn2 = nn.BatchNorm2d(32)\n        self.ebn3 = nn.BatchNorm2d(128)\n        \n        self.norm3 = norm_layer(embed_dims[1])\n        self.norm4 = norm_layer(embed_dims[2])\n\n        self.dnorm3 = norm_layer(160)\n        self.dnorm4 = norm_layer(128)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n\n        self.block1 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.block2 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[2], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock1 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock2 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n                                              embed_dim=embed_dims[1])\n        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n                                              embed_dim=embed_dims[2])\n\n        self.decoder1 = nn.Conv2d(256, 160, 3, stride=1,padding=1)  \n        self.decoder2 =   nn.Conv2d(160, 128, 3, stride=1, padding=1)  \n        self.decoder3 =   nn.Conv2d(128, 32, 3, stride=1, padding=1) \n        self.decoder4 =   nn.Conv2d(32, 16, 3, stride=1, padding=1)\n        self.decoder5 =   nn.Conv2d(16, 16, 3, stride=1, padding=1)\n\n        self.dbn1 = nn.BatchNorm2d(160)\n        self.dbn2 = nn.BatchNorm2d(128)\n        self.dbn3 = nn.BatchNorm2d(32)\n        self.dbn4 = nn.BatchNorm2d(16)\n        \n        self.final = nn.Conv2d(16, num_classes, kernel_size=1)\n\n        self.soft = nn.Softmax(dim =1)\n\n    def forward(self, x):\n        \n        B = x.shape[0]\n        ### Encoder\n        ### Conv Stage\n\n        ### Stage 1\n        out = F.relu(F.max_pool2d(self.ebn1(self.encoder1(x)),2,2))\n        t1 = out\n        ### Stage 2\n        out = F.relu(F.max_pool2d(self.ebn2(self.encoder2(out)),2,2))\n        t2 = out\n        ### Stage 3\n        out = F.relu(F.max_pool2d(self.ebn3(self.encoder3(out)),2,2))\n        t3 = out\n\n        ### Tokenized MLP Stage\n        ### Stage 4\n\n        out,H,W = self.patch_embed3(out)\n        for i, blk in enumerate(self.block1):\n            out = blk(out, H, W)\n        out = self.norm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        t4 = out\n\n        ### Bottleneck\n\n        out ,H,W= self.patch_embed4(out)\n        for i, blk in enumerate(self.block2):\n            out = blk(out, H, W)\n        out = self.norm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        ### Stage 4\n\n        out = F.relu(F.interpolate(self.dbn1(self.decoder1(out)),scale_factor=(2,2),mode ='bilinear'))\n        \n        out = torch.add(out,t4)\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        for i, blk in enumerate(self.dblock1):\n            out = blk(out, H, W)\n\n        ### Stage 3\n        \n        out = self.dnorm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        out = F.relu(F.interpolate(self.dbn2(self.decoder2(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t3)\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        \n        for i, blk in enumerate(self.dblock2):\n            out = blk(out, H, W)\n\n        out = self.dnorm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        out = F.relu(F.interpolate(self.dbn3(self.decoder3(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t2)\n        out = F.relu(F.interpolate(self.dbn4(self.decoder4(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t1)\n        out = F.relu(F.interpolate(self.decoder5(out),scale_factor=(2,2),mode ='bilinear'))\n\n        return self.final(out)\n\n\nclass UNext_S(nn.Module):\n\n    ## Conv 3 + MLP 2 + shifted MLP w less parameters\n    \n    def __init__(self,  num_classes, input_channels=3, deep_supervision=False,img_size=224, patch_size=2, in_chans=3,  embed_dims=[32, 64, 128, 512],\n                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n                 depths=[1, 1, 1], sr_ratios=[8, 4, 2, 1], **kwargs):\n        super().__init__()\n        \n        self.encoder1 = nn.Conv2d(3, 8, 3, stride=1, padding=1)  \n        self.encoder2 = nn.Conv2d(8, 16, 3, stride=1, padding=1)  \n        self.encoder3 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n\n        self.ebn1 = nn.BatchNorm2d(8)\n        self.ebn2 = nn.BatchNorm2d(16)\n        self.ebn3 = nn.BatchNorm2d(32)\n        \n        self.norm3 = norm_layer(embed_dims[1])\n        self.norm4 = norm_layer(embed_dims[2])\n\n        self.dnorm3 = norm_layer(64)\n        self.dnorm4 = norm_layer(32)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n\n        self.block1 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.block2 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[2], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock1 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock2 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n                                              embed_dim=embed_dims[1])\n        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n                                              embed_dim=embed_dims[2])\n\n        self.decoder1 = nn.Conv2d(128, 64, 3, stride=1,padding=1)  \n        self.decoder2 =   nn.Conv2d(64, 32, 3, stride=1, padding=1)  \n        self.decoder3 =   nn.Conv2d(32, 16, 3, stride=1, padding=1) \n        self.decoder4 =   nn.Conv2d(16, 8, 3, stride=1, padding=1)\n        self.decoder5 =   nn.Conv2d(8, 8, 3, stride=1, padding=1)\n\n        self.dbn1 = nn.BatchNorm2d(64)\n        self.dbn2 = nn.BatchNorm2d(32)\n        self.dbn3 = nn.BatchNorm2d(16)\n        self.dbn4 = nn.BatchNorm2d(8)\n        \n        self.final = nn.Conv2d(8, num_classes, kernel_size=1)\n\n        self.soft = nn.Softmax(dim =1)\n\n    def forward(self, x):\n        \n        B = x.shape[0]\n        ### Encoder\n        ### Conv Stage\n\n        ### Stage 1\n        out = F.relu(F.max_pool2d(self.ebn1(self.encoder1(x)),2,2))\n        t1 = out\n        ### Stage 2\n        out = F.relu(F.max_pool2d(self.ebn2(self.encoder2(out)),2,2))\n        t2 = out\n        ### Stage 3\n        out = F.relu(F.max_pool2d(self.ebn3(self.encoder3(out)),2,2))\n        t3 = out\n\n        ### Tokenized MLP Stage\n        ### Stage 4\n\n        out,H,W = self.patch_embed3(out)\n        for i, blk in enumerate(self.block1):\n            out = blk(out, H, W)\n        out = self.norm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        t4 = out\n\n        ### Bottleneck\n\n        out ,H,W= self.patch_embed4(out)\n        for i, blk in enumerate(self.block2):\n            out = blk(out, H, W)\n        out = self.norm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        ### Stage 4\n\n        out = F.relu(F.interpolate(self.dbn1(self.decoder1(out)),scale_factor=(2,2),mode ='bilinear'))\n        \n        out = torch.add(out,t4)\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        for i, blk in enumerate(self.dblock1):\n            out = blk(out, H, W)\n\n        ### Stage 3\n        \n        out = self.dnorm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        out = F.relu(F.interpolate(self.dbn2(self.decoder2(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t3)\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        \n        for i, blk in enumerate(self.dblock2):\n            out = blk(out, H, W)\n\n        out = self.dnorm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        out = F.relu(F.interpolate(self.dbn3(self.decoder3(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t2)\n        out = F.relu(F.interpolate(self.dbn4(self.decoder4(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t1)\n        out = F.relu(F.interpolate(self.decoder5(out),scale_factor=(2,2),mode ='bilinear'))\n\n        return self.final(out)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:18:44.150640Z","iopub.execute_input":"2023-10-05T15:18:44.151284Z","iopub.status.idle":"2023-10-05T15:18:44.233670Z","shell.execute_reply.started":"2023-10-05T15:18:44.151246Z","shell.execute_reply":"2023-10-05T15:18:44.232742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n@torch.inference_mode()\ndef evaluate(net, dataloader, device, amp):\n    net.eval()\n    num_val_batches = len(dataloader)\n    dice_score = 0\n    iou_score = 0\n\n    if isinstance(model, nn.DataParallel):\n        n_classes = net.module.n_classes\n    else:\n        n_classes = net.n_classes\n    criterion = nn.CrossEntropyLoss() if n_classes > 1 else nn.BCEWithLogitsLoss()\n    dice_loss = smp.losses.DiceLoss(mode='binary', log_loss=True, from_logits = True).cuda()\n   \n    \n    # iterate over the validation set\n    with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n        for batch in tqdm(dataloader, total=num_val_batches, desc='Validation round', unit='batch', position=0 ,leave=True):\n            image, mask_true = batch\n\n            # move images and labels to correct device and type\n            image = image.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n            mask_true = mask_true.to(device=device, dtype=torch.float32)\n\n            # predict the mask\n            mask_pred = net(image)\n            dice_score += criterion(mask_pred, mask_true.float())\n            dice_score += dice_loss(mask_pred, mask_true)\n            \n            tp, fp, fn, tn = smp.metrics.get_stats(mask_pred, mask_true.long(), mode='binary', threshold=0.5)\n            iou_score += smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n            \n    dice_loss = (dice_score / max(num_val_batches, 1))\n    iou_score = (iou_score / max(num_val_batches, 1))\n    print(\"Validation dice loss: {}, IoU Score {}\".format(dice_loss, iou_score))\n    return (dice_loss, iou_score)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:18:44.237027Z","iopub.execute_input":"2023-10-05T15:18:44.237577Z","iopub.status.idle":"2023-10-05T15:18:44.252135Z","shell.execute_reply.started":"2023-10-05T15:18:44.237508Z","shell.execute_reply":"2023-10-05T15:18:44.251290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport torch.optim as optim\nimport segmentation_models_pytorch as smp\n\nn_train = len(train_data)\nn_val = len(validate_data)\n\n\ndef train(model, device, \n          project = 'U-Net',\n          epochs: int = 60,\n          learning_rate: float = 1e-5, \n          weight_decay: float = 1e-8,\n          momentum: float = 0.999,\n          batch_size: int = 2,\n          amp: bool = False,\n          val_percent: float = 0.1,\n          gradient_clipping: float = 1.0):\n    \n    if isinstance(model, nn.DataParallel):\n        n_classes = model.module.n_classes\n        n_channels = model.module.n_channels\n    else:\n        n_classes = model.n_classes\n        n_channels = model.n_channels\n        \n    # (Initialize logging)\n    experiment = wandb.init(project=project, resume='allow', anonymous='must')\n    experiment.config.update(\n        dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n             val_percent=val_percent, amp=amp)\n    )\n\n    logging.info(f'''Starting training:\n        Epochs:          {epochs}\n        Batch size:      {batch_size}\n        Learning rate:   {learning_rate}\n        Training size:   {n_train}\n        Validation size: {n_val}\n        Device:          {device.type}\n        Mixed Precision: {amp}\n    ''')\n    \n\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n#     optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],\n#                               nesterov=config['nesterov'], weight_decay=config['weight_decay'])\n\n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2, eta_min=5e-5)\n    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n    \n    criterion = nn.CrossEntropyLoss().cuda()\n    dice_loss = smp.losses.DiceLoss(mode='binary').cuda()\n    \n    global_step = 0\n    \n\n    # 5. Begin training\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss = 0\n        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n            for batch in trainloader:\n                images, true_masks = batch\n\n                assert images.shape[1] == n_channels, \\\n                    f'Network has been defined with {n_channels} input channels, ' \\\n                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n                    'the images are loaded correctly.'\n\n                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n                \n                true_masks = true_masks.to(device=device, dtype=torch.long)\n\n                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n                    masks_pred = model(images)\n                    loss = criterion(masks_pred, true_masks.float())\n                    loss += dice_loss(masks_pred, true_masks)\n                    tp, fp, fn, tn = smp.metrics.get_stats(masks_pred, true_masks.long(), mode='binary', threshold=0.5)\n                    iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n    \n                optimizer.zero_grad(set_to_none=True)\n                grad_scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n                grad_scaler.step(optimizer)\n                grad_scaler.update()\n\n                pbar.update(images.shape[0])\n                global_step += 1\n                epoch_loss += loss.item()\n                experiment.log({\n                    'train iou': iou_score,\n                    'train loss': loss.item(),\n                    'step': global_step,\n                    'epoch': epoch\n                })\n                pbar.set_postfix(**{'loss (batch)': loss.item()})\n\n                # Evaluation round\n                division_step = (n_train // (5 * batch_size))\n                if division_step > 0:\n                    if global_step % division_step == 0:\n                        histograms = {}\n                        for tag, value in model.named_parameters():\n                            tag = tag.replace('/', '.')\n                            if not (torch.isinf(value) | torch.isnan(value)).any():\n                                histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n                            if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():\n                                histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n\n                        val_score, iou_score = evaluate(model, valloader, device, amp)\n                        model.train()\n                        scheduler.step(val_score)\n\n                        logging.info('Validation Dice score: {}'.format(val_score))\n                        try:\n                            experiment.log({\n                                'learning rate': optimizer.param_groups[0]['lr'],\n                                'validation Dice': val_score,\n                                'validation IoU Score': iou_score,\n                                'images': wandb.Image(images[0].cpu()),\n                                'masks': {\n                                    'true': wandb.Image(true_masks[0].float().cpu()),\n                                    'pred': wandb.Image(masks_pred[0].float().cpu()),\n                                },\n                                'step': global_step,\n                                'epoch': epoch,\n                                **histograms\n                            })\n                        except:\n                            pass\n            \n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:18:44.254416Z","iopub.execute_input":"2023-10-05T15:18:44.255016Z","iopub.status.idle":"2023-10-05T15:18:45.782441Z","shell.execute_reply.started":"2023-10-05T15:18:44.254981Z","shell.execute_reply":"2023-10-05T15:18:45.780927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = UNext(input_channels=3, num_classes=1, img_size=512)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(memory_format=torch.channels_last)\nmodel.to(device)\n\ntrain(model, device, project='U-Next')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:18:45.784074Z","iopub.execute_input":"2023-10-05T15:18:45.784692Z","iopub.status.idle":"2023-10-05T15:20:18.425576Z","shell.execute_reply.started":"2023-10-05T15:18:45.784652Z","shell.execute_reply":"2023-10-05T15:20:18.423851Z"},"trusted":true},"execution_count":null,"outputs":[]}]}