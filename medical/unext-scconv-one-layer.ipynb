{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6361775,"sourceType":"datasetVersion","datasetId":3664633},{"sourceId":168526519,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chenshu321/unext-scconv-one-layer?scriptVersionId=182537722\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# UNeXt ScConv one layer\n> https://github.com/jeya-maria-jose/UNeXt-pytorch","metadata":{}},{"cell_type":"markdown","source":"## 引入依赖包","metadata":{}},{"cell_type":"code","source":"!pip install scipy scikit-image torch torchvision pathlib segmentation-models-pytorch\n!pip install wandb --upgrade","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:54:15.704011Z","iopub.execute_input":"2024-02-22T14:54:15.704313Z","iopub.status.idle":"2024-02-22T14:54:35.355645Z","shell.execute_reply.started":"2024-02-22T14:54:15.704285Z","shell.execute_reply":"2024-02-22T14:54:35.354512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%config Completer.use_jedi = False\nfrom apodatasettrainning import *\n\nimport torch\nfrom torch import nn\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nimport torch.nn.functional as F\nimport os\nimport matplotlib.pyplot as plt\n# from utils import *\n\nimport timm\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nimport types\nimport math\nfrom abc import ABCMeta, abstractmethod\n# from mmcv.cnn import ConvModule\nimport pdb\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:54:35.357154Z","iopub.execute_input":"2024-02-22T14:54:35.357486Z","iopub.status.idle":"2024-02-22T14:54:37.050238Z","shell.execute_reply.started":"2024-02-22T14:54:35.35745Z","shell.execute_reply":"2024-02-22T14:54:37.048896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ScConv\n> https://github.com/XxxxYi/Reproduction_of_ScConv_CVPR2023/blob/main/scconv.py","metadata":{}},{"cell_type":"code","source":"\n\nclass GroupNorm2d(nn.Module):\n\n    def __init__(self, n_groups: int = 16, n_channels: int = 16, eps: float = 1e-10):\n        super(GroupNorm2d, self).__init__()  \n        assert n_channels % n_groups == 0 \n        self.n_groups = n_groups  \n        self.gamma = nn.Parameter(torch.randn(n_channels, 1, 1))  # learnable gamma\n        self.beta = nn.Parameter(torch.zeros(n_channels, 1, 1))  # learnable beta\n        self.eps = eps \n\n    def forward(self, x):\n        N, C, H, W = x.size()\n        x = x.reshape(N, self.n_groups, -1) \n        mean = x.mean(dim=2, keepdim=True)  \n        std = x.std(dim=2, keepdim=True)\n        x = (x - mean) / (std + self.eps) \n        x = x.reshape(N, C, H, W)  \n        return x * self.gamma + self.beta  \n\n\n# Spatial and Reconstruct Unit\nclass SRU(nn.Module):\n\n    def __init__(\n            self,\n            n_channels: int,  # in_channels\n            n_groups: int = 16,  # 16\n            gate_treshold: float = 0.5,  # 0.5,\n            torch_gn:bool = True\n    ):\n        super().__init__()  \n\n        # initialize GroupNorm2d\n        self.gn = nn.GroupNorm( num_channels = n_channels, num_groups = n_groups ) if torch_gn else GroupNorm2d(n_channels = n_channels, n_groups = n_groups)\n        # self.gn = GroupNorm2d(n_groups=n_groups, n_channels=n_channels)\n        self.gate_treshold = gate_treshold  \n        self.sigomid = nn.Sigmoid()  \n\n    def forward(self, x):\n        gn_x = self.gn(x) \n        w_gamma = self.gn.gamma / sum(self.gn.gamma)  # cal gamma weight\n        reweights = self.sigomid(gn_x * w_gamma)  # importance\n\n        info_mask = reweights >= self.gate_treshold\n        noninfo_mask = reweights < self.gate_treshold\n        x_1 = info_mask * x  \n        x_2 = noninfo_mask * x  \n        x = self.reconstruct(x_1, x_2) \n        return x\n\n    def reconstruct(self, x_1, x_2):\n        x_11, x_12 = torch.split(x_1, x_1.size(1) // 2, dim=1)\n        x_21, x_22 = torch.split(x_2, x_2.size(1) // 2, dim=1)\n        return torch.cat([x_11 + x_22, x_12 + x_21], dim=1)\n\n\n# Channel Reduction Unit\nclass CRU(nn.Module):\n\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, alpha: float = 1 / 2, squeeze_radio: int = 2, groups: int = 2):\n        super().__init__()\n\n        self.up_channel = up_channel = int(alpha * in_channels)\n        self.low_channel = low_channel = in_channels - up_channel\n        self.squeeze1 = nn.Conv2d(up_channel, up_channel // squeeze_radio, kernel_size=1, bias=False)\n        self.squeeze2 = nn.Conv2d(low_channel, low_channel // squeeze_radio, kernel_size=1, bias=False)\n\n        in_ch = up_channel // squeeze_radio\n        out_ch = out_channels\n        #print(\"out_channels:\", out_channels, \"squeeze_radio: \", squeeze_radio, \"up_channel: \",up_channel,\"in_ch (out_channels // squeeze_radio): \", in_ch, \"out_ch(out_channels):\", out_ch)\n        \n        \n        if in_ch >= 16:\n            groups = 16\n    \n        self.GWC = nn.Conv2d(in_ch, out_channels, kernel_size=kernel_size, stride=1, padding=kernel_size // 2, groups=groups) \n        self.PWC1 = nn.Conv2d(in_ch, out_channels, kernel_size=1, bias=False)\n\n        #print(\"in_ch: \", in_ch, \"out_channels: \", out_channels,\"groups: \",groups)\n        #print(\"up_channel: \", self.up_channel, \"low_channel: \", self.low_channel)\n        in_ch = low_channel // squeeze_radio\n        out_ch = out_channels - low_channel // squeeze_radio\n        #print(\"out_channels:\", out_channels, \"squeeze_radio: \", squeeze_radio, \"low_channel: \",low_channel,\"in_ch (low_channel // squeeze_radio): \", in_ch, \"out_ch(out_channels - low_channel // squeeze_radio):\", out_ch)\n        self.PWC2 = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False) \n        #print(\"self.PWC2.weight.shape: \",self.PWC2.weight.shape)\n        self.pool = nn.AdaptiveAvgPool2d(1)  \n        \n    def forward(self, x):\n\n        up, low = torch.split(x, [self.up_channel, self.low_channel], dim=1)\n        up, low = self.squeeze1(up), self.squeeze2(low)\n\n        y1 = self.GWC(up) + self.PWC1(up)\n        \n        #print(\"low: \",low.shape)\n        pwc2 = self.PWC2(low)\n        #print(\"pwc2\", pwc2.shape)\n        y2 = torch.cat([pwc2, low], dim=1)\n\n        s1 = self.pool(y1)\n        s2 = self.pool(y2)\n        s = torch.cat([s1, s2], dim=1)\n        beta = F.softmax(s, dim=1)\n        beta1, beta2 = torch.split(beta, beta.size(1) // 2, dim=1)\n        y = beta1 * y1 + beta2 * y2\n        return y\n\n\n# Squeeze and Channel Reduction Convolution\nclass ScConv(nn.Module):\n\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, stride: int = 1, padding: int = 1, n_groups: int = 2, gate_treshold: float = 0.5, alpha: float = 1 / 2, squeeze_radio: int = 2, groups: int = 2):\n        super().__init__()\n\n        self.SRU = SRU(in_channels, n_groups=n_groups, gate_treshold=gate_treshold, torch_gn=False) \n        self.CRU = CRU(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, alpha=alpha, squeeze_radio=squeeze_radio, groups=groups)\n\n    def forward(self, x):\n        x = self.SRU(x)  \n        x = self.CRU(x) \n        return x\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:54:39.716209Z","iopub.execute_input":"2024-02-22T14:54:39.716642Z","iopub.status.idle":"2024-02-22T14:54:39.753145Z","shell.execute_reply.started":"2024-02-22T14:54:39.716592Z","shell.execute_reply":"2024-02-22T14:54:39.751948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nimport torch.nn.functional as F\nimport os\nimport matplotlib.pyplot as plt\n# from utils import *\n\nimport timm\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nimport types\nimport math\nfrom abc import ABCMeta, abstractmethod\n# from mmcv.cnn import ConvModule\nimport pdb\n\n\n\n# def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n#     \"\"\"1x1 convolution\"\"\"\n#     return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, bias=False)\n\n\ndef shift(dim):\n            x_shift = [ torch.roll(x_c, shift, dim) for x_c, shift in zip(xs, range(-self.pad, self.pad+1))]\n            x_cat = torch.cat(x_shift, 1)\n            x_cat = torch.narrow(x_cat, 2, self.pad, H)\n            x_cat = torch.narrow(x_cat, 3, self.pad, W)\n            return x_cat\n\nclass shiftmlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., shift_size=5):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.dim = in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n        self.shift_size = shift_size\n        self.pad = shift_size // 2\n\n        \n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n    \n#     def shift(x, dim):\n#         x = F.pad(x, \"constant\", 0)\n#         x = torch.chunk(x, shift_size, 1)\n#         x = [ torch.roll(x_c, shift, dim) for x_s, shift in zip(x, range(-pad, pad+1))]\n#         x = torch.cat(x, 1)\n#         return x[:, :, pad:-pad, pad:-pad]\n\n    def forward(self, x, H, W):\n        # pdb.set_trace()\n        B, N, C = x.shape\n\n        xn = x.transpose(1, 2).view(B, C, H, W).contiguous()\n        xn = F.pad(xn, (self.pad, self.pad, self.pad, self.pad) , \"constant\", 0)\n        xs = torch.chunk(xn, self.shift_size, 1)\n        x_shift = [torch.roll(x_c, shift, 2) for x_c, shift in zip(xs, range(-self.pad, self.pad+1))]\n        x_cat = torch.cat(x_shift, 1)\n        x_cat = torch.narrow(x_cat, 2, self.pad, H)\n        x_s = torch.narrow(x_cat, 3, self.pad, W)\n\n\n        x_s = x_s.reshape(B,C,H*W).contiguous()\n        x_shift_r = x_s.transpose(1,2)\n\n\n        x = self.fc1(x_shift_r)\n\n        x = self.dwconv(x, H, W)\n        x = self.act(x) \n        x = self.drop(x)\n\n        xn = x.transpose(1, 2).view(B, C, H, W).contiguous()\n        xn = F.pad(xn, (self.pad, self.pad, self.pad, self.pad) , \"constant\", 0)\n        xs = torch.chunk(xn, self.shift_size, 1)\n        x_shift = [torch.roll(x_c, shift, 3) for x_c, shift in zip(xs, range(-self.pad, self.pad+1))]\n        x_cat = torch.cat(x_shift, 1)\n        x_cat = torch.narrow(x_cat, 2, self.pad, H)\n        x_s = torch.narrow(x_cat, 3, self.pad, W)\n        x_s = x_s.reshape(B,C,H*W).contiguous()\n        x_shift_c = x_s.transpose(1,2)\n\n        x = self.fc2(x_shift_c)\n        x = self.drop(x)\n        return x\n\n\n\nclass shiftedBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n        super().__init__()\n\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = shiftmlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n\n        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n        return x\n\n\nclass DWConv(nn.Module):\n    def __init__(self, dim=768):\n        super(DWConv, self).__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        x = x.transpose(1, 2).view(B, C, H, W)\n        x = self.dwconv(x)\n        x = x.flatten(2).transpose(1, 2)\n\n        return x\n\nclass OverlapPatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n        self.norm = nn.LayerNorm(embed_dim)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n\n        return x, H, W\n\n\nsigmoid = nn.Sigmoid()\nif torch.cuda.is_available():\n    sigmoid = sigmoid.cuda()\n    \nclass UNext(nn.Module):\n\n    ## Conv 3 + MLP 2 + shifted MLP\n    \n    def __init__(self,  num_classes, input_channels=3, deep_supervision=False,img_size=224, patch_size=16, in_chans=3,  embed_dims=[ 192, 256, 512 ],\n                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n                 depths=[1, 1, 1], sr_ratios=[8, 4, 2, 1], **kwargs):\n        super().__init__()\n        self.num_classes = num_classes\n        self.input_channels = input_channels\n        self.encoder1 = nn.Conv2d(3, 16, 3, stride=1, padding=1)  \n        \n        self.encoder2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)  \n        self.encoder3 = nn.Conv2d(32, 128, 3, stride=1, padding=1)\n        self.encoder4 = ScConv(128, 192, 3, stride=1, padding=1)\n\n        self.ebn1 = nn.BatchNorm2d(16)\n        self.ebn2 = nn.BatchNorm2d(32)\n        self.ebn3 = nn.BatchNorm2d(128)\n        self.ebn4 = nn.BatchNorm2d(192)\n        \n        self.norm3 = norm_layer(embed_dims[1])#256\n        self.norm4 = norm_layer(embed_dims[2])#512\n\n        self.dnorm3 = norm_layer(256)\n        self.dnorm4 = norm_layer(192)\n\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n\n        self.block1 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.block2 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[2], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock1 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock2 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n                                              embed_dim=embed_dims[1])\n        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n                                              embed_dim=embed_dims[2])\n\n        self.decoder1 =   nn.Conv2d(512, 256, 3, stride=1,padding=1) \n        self.decoder2 =   nn.Conv2d(256, 192, 3, stride=1,padding=1)  \n        self.decoder3 =   ScConv(192, 128, 3, stride=1, padding=1)  \n        self.decoder4 =   nn.Conv2d(128, 32, 3, stride=1, padding=1) \n        self.decoder5 =   nn.Conv2d(32, 16, 3, stride=1, padding=1)\n        self.decoder6 =   nn.Conv2d(16, 16, 3, stride=1, padding=1)\n\n        self.dbn1 = nn.BatchNorm2d(256)\n        self.dbn2 = nn.BatchNorm2d(192)\n        self.dbn3 = nn.BatchNorm2d(128)\n        self.dbn4 = nn.BatchNorm2d(32)\n        self.dbn5 = nn.BatchNorm2d(16)\n        \n        self.final = nn.Conv2d(16, num_classes, kernel_size=1)\n\n        self.soft = nn.Softmax(dim =1)\n\n    def forward(self, x):\n        \n        B = x.shape[0]\n        ### Encoder\n        ### Conv Stage\n\n        ### Stage 1\n        out = F.relu(F.max_pool2d(self.ebn1(self.encoder1(x)),2,2))\n        t1 = out\n        ### Stage 2\n        out = F.relu(F.max_pool2d(self.ebn2(self.encoder2(out)),2,2))\n        t2 = out\n        ### Stage 3\n        out = F.relu(F.max_pool2d(self.ebn3(self.encoder3(out)),2,2))\n        t3 = out\n        ### Stage 4\n        out = F.relu(F.max_pool2d(self.ebn4(self.encoder4(out)),2,2))\n        t4 = out\n        \n        \n        ### Tokenized MLP Stage\n        ### Stage 5\n\n        out,H,W = self.patch_embed3(out)\n        for i, blk in enumerate(self.block1):\n            out = blk(out, H, W)\n        out = self.norm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        t5 = out\n        #print(\"t5=\",out.shape)\n        ### Bottleneck\n\n        out ,H,W= self.patch_embed4(out)\n        for i, blk in enumerate(self.block2):\n            out = blk(out, H, W)\n        out = self.norm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        #print(\"bottleneck=\",out.shape)\n\n        ### Stage 4\n\n        out = F.relu(F.interpolate(self.dbn1(self.decoder1(out)),scale_factor=(2,2),mode ='bilinear'))\n        #print(\"d5=\",out)\n        out = torch.add(out,t5)\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        for i, blk in enumerate(self.dblock1):\n            out = blk(out, H, W)\n\n        ### Stage 3\n        \n        out = self.dnorm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        out = F.relu(F.interpolate(self.dbn2(self.decoder2(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t4)\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        \n        for i, blk in enumerate(self.dblock2):\n            out = blk(out, H, W)\n          \n        out = self.dnorm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        out = F.relu(F.interpolate(self.dbn3(self.decoder3(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t3)\n        out = F.relu(F.interpolate(self.dbn4(self.decoder4(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t2)\n        out = F.relu(F.interpolate(self.dbn5(self.decoder5(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t1)\n        out = F.relu(F.interpolate(self.decoder6(out),scale_factor=(2,2),mode ='bilinear'))\n\n        out = self.final(out)\n        if not self.training:\n            out = sigmoid(out)\n            out = torch.where(out>0.5,torch.ones_like(out),torch.zeros_like(out))\n        return out\n\n\nclass UNext_S(nn.Module):\n\n    ## Conv 3 + MLP 2 + shifted MLP w less parameters\n    \n    def __init__(self,  num_classes, input_channels=3, deep_supervision=False,img_size=224, patch_size=16, in_chans=3,  embed_dims=[32, 64, 128, 512],\n                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n                 depths=[1, 1, 1], sr_ratios=[8, 4, 2, 1], **kwargs):\n        super().__init__()\n        self.input_channels = input_channels\n        self.num_classes = num_classes\n        self.encoder1 = nn.Conv2d(3, 8, 3, stride=1, padding=1)  \n        self.encoder2 = nn.Conv2d(8, 16, 3, stride=1, padding=1)  \n        self.encoder3 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n\n        self.ebn1 = nn.BatchNorm2d(8)\n        self.ebn2 = nn.BatchNorm2d(16)\n        self.ebn3 = nn.BatchNorm2d(32)\n        \n        self.norm3 = norm_layer(embed_dims[1])\n        self.norm4 = norm_layer(embed_dims[2])\n\n        self.dnorm3 = norm_layer(64)\n        self.dnorm4 = norm_layer(32)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n\n        self.block1 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.block2 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[2], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock1 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock2 = nn.ModuleList([shiftedBlock(\n            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n                                              embed_dim=embed_dims[1])\n        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n                                              embed_dim=embed_dims[2])\n\n        self.decoder1 =  nn.Conv2d(128, 64, 3, stride=1,padding=1)  \n        self.decoder2 =  nn.Conv2d(64, 32, 3, stride=1, padding=1)  \n        self.decoder3 =  nn.Conv2d(32, 16, 3, stride=1, padding=1) \n        self.decoder4 =  nn.Conv2d(16, 8, 3, stride=1, padding=1)\n        self.decoder5 =  nn.Conv2d(8, 8, 3, stride=1, padding=1)\n\n        self.dbn1 = nn.BatchNorm2d(64)\n        self.dbn2 = nn.BatchNorm2d(32)\n        self.dbn3 = nn.BatchNorm2d(16)\n        self.dbn4 = nn.BatchNorm2d(8)\n        \n        self.final = nn.Conv2d(8, num_classes, kernel_size=1)\n\n        self.soft = nn.Softmax(dim =1)\n\n    def forward(self, x):\n        \n        B = x.shape[0]\n        ### Encoder\n        ### Conv Stage\n\n        ### Stage 1\n        out = F.relu(F.max_pool2d(self.ebn1(self.encoder1(x)),2,2))\n        t1 = out\n        ### Stage 2\n        out = F.relu(F.max_pool2d(self.ebn2(self.encoder2(out)),2,2))\n        t2 = out\n        ### Stage 3\n        out = F.relu(F.max_pool2d(self.ebn3(self.encoder3(out)),2,2))\n        t3 = out\n\n        ### Tokenized MLP Stage\n        ### Stage 4\n\n        out,H,W = self.patch_embed3(out)\n        for i, blk in enumerate(self.block1):\n            out = blk(out, H, W)\n        out = self.norm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        t4 = out\n\n        ### Bottleneck\n\n        out ,H,W= self.patch_embed4(out)\n        for i, blk in enumerate(self.block2):\n            out = blk(out, H, W)\n        out = self.norm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        ### Stage 4\n\n        out = F.relu(F.interpolate(self.dbn1(self.decoder1(out)),scale_factor=(2,2),mode ='bilinear'))\n        \n        out = torch.add(out,t4)\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        for i, blk in enumerate(self.dblock1):\n            out = blk(out, H, W)\n\n        ### Stage 3\n        \n        out = self.dnorm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        out = F.relu(F.interpolate(self.dbn2(self.decoder2(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t3)\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        \n        for i, blk in enumerate(self.dblock2):\n            out = blk(out, H, W)\n\n        out = self.dnorm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        out = F.relu(F.interpolate(self.dbn3(self.decoder3(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t2)\n        out = F.relu(F.interpolate(self.dbn4(self.decoder4(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t1)\n        out = F.relu(F.interpolate(self.decoder5(out),scale_factor=(2,2),mode ='bilinear'))\n\n        out = self.final(out)\n        if not self.training:\n            out = sigmoid(out)\n            out = torch.where(out>0.5,torch.ones_like(out),torch.zeros_like(out))\n        return out\n\n\n#EOF\n\n\n## DataSet","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:54:39.755176Z","iopub.execute_input":"2024-02-22T14:54:39.75582Z","iopub.status.idle":"2024-02-22T14:54:40.359648Z","shell.execute_reply.started":"2024-02-22T14:54:39.755779Z","shell.execute_reply":"2024-02-22T14:54:40.358803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: image和mask名称不一样时跳过\nclass APODataSet(Dataset):\n    # 格式不对的异常数据\n    def __init__(self, img_dir, mask_dir: str, size, mixCut: bool = False) -> None:\n        self.mixCut = mixCut\n        # 获取所有图片路径\n        img_paths = list(Path(img_dir).glob(\"*\"))\n        mask_paths = list(Path(mask_dir).glob(\"*\"))\n        self.images = []\n        self.masks = []\n        for img_idx in range(len(img_paths)):\n            img_path = img_paths[img_idx]\n            img = self.load_image(img_path)\n            num_channels = len(img.getbands())\n            if num_channels != 3:\n                continue\n            \n            mask_path = mask_paths[img_idx]\n            self.images.append(img_path)\n            self.masks.append(mask_path)\n            \n        self.transform = transforms.Compose([ transforms.Resize(size), transforms.ToTensor()])\n\n    def load_image(self, path) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        return Image.open(path)\n    \n    #  重写 __len__() 方法 (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.images)\n    \n    #  CutMix 的切块功能\n \n    def rand_bbox(self, size, lam):\n        W = size[1]\n        H = size[2]\n        cut_rat = np.sqrt(1. - lam)\n        cut_w = int(W * cut_rat)\n        cut_h = int(H * cut_rat)\n\n        # uniform\n        cx = np.random.randint(W)\n        cy = np.random.randint(H)\n\n        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n        bby1 = np.clip(cy - cut_h // 2, 0, H)\n        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n        bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n        return bbx1, bby1, bbx2, bby2\n    \n    # 重写 __getitem__() 方法 (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"Returns one sample of data, image and mask (X, y).\"\n        orig_img = self.load_image(self.images[index])\n        mask_img = self.load_image(self.masks[index])\n\n        orig_img = self.transform(orig_img)\n        mask_img = self.transform(mask_img)\n        \n        \n        #cutmix\n        if self.mixCut:\n            rand_index = random.randint(0, len(self)-1)\n            rand_orig_img = self.load_image(self.images[rand_index])\n            rand_mask_img = self.load_image(self.masks[rand_index])\n\n            rand_orig_img = self.transform(rand_orig_img)\n            rand_mask_img = self.transform(rand_mask_img)\n\n\n            lam = np.random.beta(1., 1.)\n            rand_index = torch.randperm(len(self)).cuda()\n            bbx1, bby1, bbx2, bby2 = self.rand_bbox(mask_img.size(), lam)\n\n\n            orig_img[:, bbx1:bbx2, bby1:bby2] = rand_orig_img[:, bbx1:bbx2, bby1:bby2]\n            mask_img[:, bbx1:bbx2, bby1:bby2] = rand_mask_img[:, bbx1:bbx2, bby1:bby2]\n        \n        \n        mask_img = torch.where(mask_img>0.5,torch.ones_like(mask_img),torch.zeros_like(mask_img))\n        \n        \n        return orig_img, mask_img\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:54:40.36089Z","iopub.execute_input":"2024-02-22T14:54:40.361211Z","iopub.status.idle":"2024-02-22T14:54:40.375153Z","shell.execute_reply.started":"2024-02-22T14:54:40.361181Z","shell.execute_reply":"2024-02-22T14:54:40.374145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 设置wandb账号\n用作统计与数据分析","metadata":{}},{"cell_type":"code","source":"os.environ['WANDB_API_KEY']='d561f1229ba7c4e207ca34042f29a43552a7447e'\nos.environ['WANDB_MODE']='offline'","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:54:47.071983Z","iopub.execute_input":"2024-02-22T14:54:47.072394Z","iopub.status.idle":"2024-02-22T14:54:49.691228Z","shell.execute_reply.started":"2024-02-22T14:54:47.072352Z","shell.execute_reply":"2024-02-22T14:54:49.690193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=100\nmodel = UNext(input_channels=3, num_classes=1, img_size=512)\nStarTrain(project=\"U-Next-ScConv-onelayer\", model=model, epochs=epochs, batch_size=8)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:54:49.692688Z","iopub.execute_input":"2024-02-22T14:54:49.693043Z","iopub.status.idle":"2024-02-22T14:57:23.552079Z","shell.execute_reply.started":"2024-02-22T14:54:49.693007Z","shell.execute_reply":"2024-02-22T14:57:23.550434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wandb login --verify d561f1229ba7c4e207ca34042f29a43552a7447e\n!wandb sync wandb/latest-run","metadata":{},"execution_count":null,"outputs":[]}]}