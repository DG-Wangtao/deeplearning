{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transfuse","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:16.283078Z","iopub.execute_input":"2023-10-05T14:57:16.283469Z","iopub.status.idle":"2023-10-05T14:57:16.300041Z","shell.execute_reply.started":"2023-10-05T14:57:16.283441Z","shell.execute_reply":"2023-10-05T14:57:16.298983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 引入依赖包","metadata":{}},{"cell_type":"code","source":"!pip install scipy scikit-image torch torchvision pathlib wandb segmentation-models-pytorch timm","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:16.445948Z","iopub.execute_input":"2023-10-05T14:57:16.446237Z","iopub.status.idle":"2023-10-05T14:57:25.846321Z","shell.execute_reply.started":"2023-10-05T14:57:16.446214Z","shell.execute_reply":"2023-10-05T14:57:25.845057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.nn.functional import relu, pad\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom PIL import Image\nfrom typing import Tuple\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom tqdm import tqdm\nimport wandb\nimport logging","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:25.852601Z","iopub.execute_input":"2023-10-05T14:57:25.854880Z","iopub.status.idle":"2023-10-05T14:57:25.869442Z","shell.execute_reply.started":"2023-10-05T14:57:25.854843Z","shell.execute_reply":"2023-10-05T14:57:25.868338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 设置wandb账号\n用作统计与数据分析","metadata":{}},{"cell_type":"code","source":"os.environ['WANDB_API_KEY']='d561f1229ba7c4e207ca34042f29a43552a7447e'\n!wandb login","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:25.872158Z","iopub.execute_input":"2023-10-05T14:57:25.872775Z","iopub.status.idle":"2023-10-05T14:57:28.833631Z","shell.execute_reply.started":"2023-10-05T14:57:25.872745Z","shell.execute_reply":"2023-10-05T14:57:28.832376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataSet","metadata":{}},{"cell_type":"markdown","source":"### 数据集定义","metadata":{}},{"cell_type":"code","source":"# TODO: image和mask名称不一样时跳过\nclass APODataSet(Dataset):\n    # 格式不对的异常数据\n    invalid_img = [10, 184, 185]\n    def __init__(self, img_dir, mask_dir: str, size, channels = 3) -> None:\n        # 获取所有图片路径\n        self.img_paths = list(Path(img_dir).glob(\"*\"))\n        self.mask_paths = list(Path(mask_dir).glob(\"*\"))\n        for idx in self.invalid_img:\n            del self.img_paths[idx]\n            del self.mask_paths[idx]\n        \n        \n        transformers = [\n            transforms.Resize(size),\n            transforms.ToTensor()\n        ]\n        if channels == 1:\n            transformers.insert(0, transforms.Grayscale(num_output_channels=1))\n\n        # 设置 transforms\n        self.transform = transforms.Compose(transformers)\n#         self.transform = transforms.Compose([transforms.PILToTensor()])\n\n    # 使用函数加载原始图像\n    def load_orig_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.img_paths[index]\n        return Image.open(image_path) \n    \n    # 使用函数加载tmask图像\n    def load_mask_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.mask_paths[index]\n        return Image.open(image_path) \n\n    #  重写 __len__() 方法 (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.img_paths)\n\n    # 重写 __getitem__() 方法 (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"Returns one sample of data, image and mask (X, y).\"\n        orig_img = self.load_orig_image(index)\n        mask_img = self.load_mask_image(index)\n        \n        orig_img = self.transform(orig_img)\n        mask_img = self.transform(mask_img)\n#         mask_img = mask_img[0]\n#         if orig_img.size()[0] != 3:\n#             print(\"{}: orig_img size: {}\".format(index,orig_img.size()))\n#             return None\n        # return data, mask (X, y)\n        return orig_img, mask_img\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:28.841238Z","iopub.execute_input":"2023-10-05T14:57:28.842512Z","iopub.status.idle":"2023-10-05T14:57:28.854146Z","shell.execute_reply.started":"2023-10-05T14:57:28.842476Z","shell.execute_reply":"2023-10-05T14:57:28.853239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 加载数据集","metadata":{}},{"cell_type":"code","source":"dataset =  APODataSet(img_dir = \"/kaggle/input/dltrack/apo_images\",\n                      mask_dir = \"/kaggle/input/dltrack/apo_masks\",\n                     size = [512, 512])\n\ntotal = len(dataset)\ntrain_size = int(0.8*total)\nvalidate_size = total - train_size\ntrain_data, validate_data = random_split(dataset, [train_size, validate_size])\nprint(\"dataset info\\ntotal: {}, train_size: {}, validate_size: {}\".format(total, len(train_data), len(validate_data)))\n\ntrainloader = DataLoader(dataset=train_data,\n                                     batch_size=2,\n                                     num_workers=0,\n                                     shuffle=True)\n\nvalloader = DataLoader(dataset=validate_data,\n                                    batch_size=1, \n                                    num_workers=0, \n                                    shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T15:17:56.528744Z","iopub.execute_input":"2023-10-05T15:17:56.529114Z","iopub.status.idle":"2023-10-05T15:17:56.823090Z","shell.execute_reply.started":"2023-10-05T15:17:56.529082Z","shell.execute_reply":"2023-10-05T15:17:56.821566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 检查异常图片\n要把找到的异常数据去除","metadata":{}},{"cell_type":"code","source":"for index in range(len(dataset)):\n    orig_img, mask_img = dataset[index]\n    if orig_img.size()[0] != 3:\n        print(\"{}: orig_img size: {}\".format(index,orig_img.size()))\nprint(\"[done]\")","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:28.897766Z","iopub.execute_input":"2023-10-05T14:57:28.899383Z","iopub.status.idle":"2023-10-05T14:57:35.879349Z","shell.execute_reply.started":"2023-10-05T14:57:28.899349Z","shell.execute_reply":"2023-10-05T14:57:35.878217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 随机显示一张原始图片和其对应的标记图片","metadata":{}},{"cell_type":"code","source":"idx = random.randint(0, len(dataset))\norig_img, mask_img = dataset[idx]\nprint(orig_img.size())\n\ntransform = transforms.ToPILImage()\nprint(\"showing image of {}: \".format(idx))\n\norig_img = transform(orig_img)\nmask_img = transform(mask_img)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 12))\n\nax1.imshow(orig_img)\nax1.set_title(\"origin_img\")\n\nax2.imshow(mask_img)\nax2.set_title(\"mask_img\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:35.884058Z","iopub.execute_input":"2023-10-05T14:57:35.884827Z","iopub.status.idle":"2023-10-05T14:57:36.415756Z","shell.execute_reply.started":"2023-10-05T14:57:35.884791Z","shell.execute_reply":"2023-10-05T14:57:36.414608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transfuse\n> https://github.com/Rayicer/TransFuse","metadata":{}},{"cell_type":"markdown","source":"### transformer.py","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom functools import partial\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.helpers import load_pretrained\n\nfrom timm.models.registry import register_model\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=2, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n\n        # FIXME look at relaxing size constraints\n        #assert H == self.img_size[0] and W == self.img_size[1], \\\n        #    f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=2, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        if hybrid_backbone is not None:\n            self.patch_embed = HybridEmbed(\n                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n        else:\n            self.patch_embed = PatchEmbed(\n                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim)\n\n        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n        #self.repr = nn.Linear(embed_dim, representation_size)\n        #self.repr_act = nn.Tanh()\n\n        # Classifier head\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        for blk in self.blocks:\n            x = blk(x)\n\n        x = self.norm(x)\n        return x[:, 0]\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\n\ndef _conv_filter(state_dict, patch_size=2):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    out_dict = {}\n    for k, v in state_dict.items():\n        if 'patch_embed.proj.weight' in k:\n            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n        out_dict[k] = v\n    return out_dict\n\n\n@register_model\ndef vit_small_patch16_224(pretrained=False, **kwargs):\n    if pretrained:\n        # NOTE my scale was wrong for original weights, leaving this here until I have better ones for this model\n        kwargs.setdefault('qk_scale', 768 ** -0.5)\n    model = VisionTransformer(patch_size=2, embed_dim=768, depth=8, num_heads=8, mlp_ratio=3., **kwargs)\n#     model.default_cfg = default_cfgs['vit_small_patch16_224']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n    return model\n\n\n@register_model\ndef vit_base_patch16_224(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        patch_size=2, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n#     model.default_cfg = default_cfgs['vit_base_patch16_224']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n    return model\n\n\n@register_model\ndef vit_base_patch16_384(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        img_size=384, patch_size=2, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n#     model.default_cfg = default_cfgs['vit_base_patch16_384']\n    if pretrained:\n        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n    return model\n\n\n@register_model\ndef vit_base_patch32_384(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        img_size=384, patch_size=32, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n#     model.default_cfg = default_cfgs['vit_base_patch32_384']\n    if pretrained:\n        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n    return model\n\n\n@register_model\ndef vit_large_patch16_224(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        patch_size=2, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n#     model.default_cfg = default_cfgs['vit_large_patch16_224']\n    if pretrained:\n        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n    return model\n\n\n@register_model\ndef vit_large_patch16_384(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        img_size=384, patch_size=2, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,  qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n#     model.default_cfg = default_cfgs['vit_large_patch16_384']\n    if pretrained:\n        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n    return model\n\n\n@register_model\ndef vit_large_patch32_384(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        img_size=384, patch_size=32, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,  qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n#     model.default_cfg = default_cfgs['vit_large_patch32_384']\n    if pretrained:\n        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n    return model\n\n\n@register_model\ndef vit_huge_patch16_224(pretrained=False, **kwargs):\n    model = VisionTransformer(patch_size=2, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, **kwargs)\n#     model.default_cfg = default_cfgs['vit_huge_patch16_224']\n    return model\n\n\n@register_model\ndef vit_huge_patch32_384(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        img_size=384, patch_size=32, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, **kwargs)\n#     model.default_cfg = default_cfgs['vit_huge_patch32_384']\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:36.786977Z","iopub.execute_input":"2023-10-05T14:57:36.787355Z","iopub.status.idle":"2023-10-05T14:57:36.828047Z","shell.execute_reply.started":"2023-10-05T14:57:36.787316Z","shell.execute_reply":"2023-10-05T14:57:36.826884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DeiT.py","metadata":{}},{"cell_type":"code","source":"# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\nimport torch\nimport torch.nn as nn\nfrom functools import partial\n\n# from .vision_transformer import VisionTransformer, _cfg\nfrom timm.models.registry import register_model\nfrom timm.models.layers import trunc_normal_\nimport torch.nn.functional as F\nimport numpy as np\n\n\n__all__ = [\n    'deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224',\n    'deit_tiny_distilled_patch16_224', 'deit_small_distilled_patch16_224',\n    'deit_base_distilled_patch16_224', 'deit_base_patch16_384',\n    'deit_base_distilled_patch16_384',\n]\n\n\nclass DeiT(VisionTransformer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        num_patches = self.patch_embed.num_patches\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, self.embed_dim))\n\n    def forward(self, x):\n        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n        # with slight modifications to add the dist_token\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        pe = self.pos_embed\n\n        x = x + pe\n        x = self.pos_drop(x)\n\n        for blk in self.blocks:\n            x = blk(x)\n\n        x = self.norm(x)\n        return x\n\n\n@register_model\ndef deit_small_patch16_224(pretrained=False, **kwargs):\n    model = DeiT(\n        patch_size=2, embed_dim=384, depth=8, num_heads=6, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n#     model.default_cfg = _cfg()\n    if pretrained:\n        ckpt = torch.load('pretrained/deit_small_patch16_224-cd65a155.pth')\n        model.load_state_dict(ckpt['model'], strict=False)\n    \n    pe = model.pos_embed[:, 1:, :].detach()\n    pe = pe.transpose(-1, -2)\n    pe = pe.view(pe.shape[0], pe.shape[1], int(np.sqrt(pe.shape[2])), int(np.sqrt(pe.shape[2])))\n    pe = F.interpolate(pe, size=(12, 16), mode='bilinear', align_corners=True)\n    pe = pe.flatten(2)\n    pe = pe.transpose(-1, -2)\n    model.pos_embed = nn.Parameter(pe)\n    model.head = nn.Identity()\n    return model\n\n\n@register_model\ndef deit_base_patch16_224(pretrained=False, **kwargs):\n    model = DeiT(\n        patch_size=2, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n#     model.default_cfg = _cfg()\n    if pretrained:\n        ckpt = torch.load('pretrained/deit_base_patch16_224-b5f2ef4d.pth')\n        model.load_state_dict(ckpt['model'], strict=False)\n\n    pe = model.pos_embed[:, 1:, :].detach()\n    pe = pe.transpose(-1, -2)\n    pe = pe.view(pe.shape[0], pe.shape[1], int(np.sqrt(pe.shape[2])), int(np.sqrt(pe.shape[2])))\n    pe = F.interpolate(pe, size=(12, 16), mode='bilinear', align_corners=True)\n    pe = pe.flatten(2)\n    pe = pe.transpose(-1, -2)\n    model.pos_embed = nn.Parameter(pe)\n    model.head = nn.Identity()\n    return model\n\n\n@register_model\ndef deit_base_patch16_384(pretrained=False, **kwargs):\n    model = DeiT(\n        img_size=384, patch_size=2, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n#     model.default_cfg = _cfg()\n    if pretrained:\n        ckpt = torch.load('pretrained/deit_base_patch16_384-8de9b5d1.pth')\n        model.load_state_dict(ckpt[\"model\"])\n\n    pe = model.pos_embed[:, 1:, :].detach()\n    pe = pe.transpose(-1, -2)\n    pe = pe.view(pe.shape[0], pe.shape[1], int(np.sqrt(pe.shape[2])), int(np.sqrt(pe.shape[2])))\n    pe = F.interpolate(pe, size=(24, 32), mode='bilinear', align_corners=True)\n    pe = pe.flatten(2)\n    pe = pe.transpose(-1, -2)\n    model.pos_embed = nn.Parameter(pe)\n    model.head = nn.Identity()\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:36.832571Z","iopub.execute_input":"2023-10-05T14:57:36.834896Z","iopub.status.idle":"2023-10-05T14:57:36.854645Z","shell.execute_reply.started":"2023-10-05T14:57:36.834858Z","shell.execute_reply":"2023-10-05T14:57:36.853628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### transfuse.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models import resnet34\nfrom torchvision.models import resnet50\nfrom torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\nimport torch.nn.functional as F\nimport numpy as np\nimport math\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n\nclass ChannelPool(nn.Module):\n    def forward(self, x):\n        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1)\n\n\nclass BiFusion_block(nn.Module):\n    def __init__(self, ch_1, ch_2, r_2, ch_int, ch_out, drop_rate=0.):\n        super(BiFusion_block, self).__init__()\n\n        # channel attention for F_g, use SE Block\n        self.fc1 = nn.Conv2d(ch_2, ch_2 // r_2, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(ch_2 // r_2, ch_2, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n        # spatial attention for F_l\n        self.compress = ChannelPool()\n        self.spatial = Conv(2, 1, 7, bn=True, relu=False, bias=False)\n\n        # bi-linear modelling for both\n        self.W_g = Conv(ch_1, ch_int, 1, bn=True, relu=False)\n        self.W_x = Conv(ch_2, ch_int, 1, bn=True, relu=False)\n        self.W = Conv(ch_int, ch_int, 3, bn=True, relu=True)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        self.residual = Residual(ch_1+ch_2+ch_int, ch_out)\n\n        self.dropout = nn.Dropout2d(drop_rate)\n        self.drop_rate = drop_rate\n\n        \n    def forward(self, g, x):\n        # bilinear pooling\n        W_g = self.W_g(g)\n        W_x = self.W_x(x)\n        bp = self.W(W_g*W_x)\n\n        # spatial attention for cnn branch\n        g_in = g\n        g = self.compress(g)\n        g = self.spatial(g)\n        g = self.sigmoid(g) * g_in\n\n        # channel attetion for transformer branch\n        x_in = x\n        x = x.mean((2, 3), keepdim=True)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x) * x_in\n        fuse = self.residual(torch.cat([g, x, bp], 1))\n\n        if self.drop_rate > 0:\n            return self.dropout(fuse)\n        else:\n            return fuse\n\n\nclass TransFuse_S(nn.Module):\n    def __init__(self, num_classes=1, drop_rate=0.2, normal_init=True, pretrained=False):\n        super(TransFuse_S, self).__init__()\n        self.n_classes = num_classes\n        self.n_channels = 3\n\n        self.resnet = resnet34()\n        if pretrained:\n            self.resnet.load_state_dict(torch.load('pretrained/resnet34-333f7ec4.pth'))\n        self.resnet.fc = nn.Identity()\n        self.resnet.layer4 = nn.Identity()\n\n        self.transformer = deit_small_patch16_224(pretrained=pretrained)\n\n        self.up1 = Up(in_ch1=384, out_ch=128)\n        self.up2 = Up(128, 64)\n\n        self.final_x = nn.Sequential(\n            Conv(256, 64, 1, bn=True, relu=True),\n            Conv(64, 64, 3, bn=True, relu=True),\n            Conv(64, num_classes, 3, bn=False, relu=False)\n            )\n\n        self.final_1 = nn.Sequential(\n            Conv(64, 64, 3, bn=True, relu=True),\n            Conv(64, num_classes, 3, bn=False, relu=False)\n            )\n\n        self.final_2 = nn.Sequential(\n            Conv(64, 64, 3, bn=True, relu=True),\n            Conv(64, num_classes, 3, bn=False, relu=False)\n            )\n\n        self.up_c = BiFusion_block(ch_1=256, ch_2=384, r_2=4, ch_int=256, ch_out=256, drop_rate=drop_rate/2)\n\n        self.up_c_1_1 = BiFusion_block(ch_1=128, ch_2=128, r_2=2, ch_int=128, ch_out=128, drop_rate=drop_rate/2)\n        self.up_c_1_2 = Up(in_ch1=256, out_ch=128, in_ch2=128, attn=True)\n\n        self.up_c_2_1 = BiFusion_block(ch_1=64, ch_2=64, r_2=1, ch_int=64, ch_out=64, drop_rate=drop_rate/2)\n        self.up_c_2_2 = Up(128, 64, 64, attn=True)\n\n        self.drop = nn.Dropout2d(drop_rate)\n\n        if normal_init:\n            self.init_weights()\n\n    def forward(self, imgs, labels=None):\n        # bottom-up path\n        x_b = self.transformer(imgs)\n        x_b = torch.transpose(x_b, 1, 2)\n        x_b = x_b.view(x_b.shape[0], -1, 12, 16)\n        x_b = self.drop(x_b)\n\n        x_b_1 = self.up1(x_b)\n        x_b_1 = self.drop(x_b_1)\n\n        x_b_2 = self.up2(x_b_1)  # transformer pred supervise here\n        x_b_2 = self.drop(x_b_2)\n\n        # top-down path\n        x_u = self.resnet.conv1(imgs)\n        x_u = self.resnet.bn1(x_u)\n        x_u = self.resnet.relu(x_u)\n        x_u = self.resnet.maxpool(x_u)\n\n        x_u_2 = self.resnet.layer1(x_u)\n        x_u_2 = self.drop(x_u_2)\n\n        x_u_1 = self.resnet.layer2(x_u_2)\n        x_u_1 = self.drop(x_u_1)\n\n        x_u = self.resnet.layer3(x_u_1)\n        x_u = self.drop(x_u) \n\n        # joint path\n        x_c = self.up_c(x_u, x_b)\n\n        x_c_1_1 = self.up_c_1_1(x_u_1, x_b_1)\n        x_c_1 = self.up_c_1_2(x_c, x_c_1_1)\n\n        x_c_2_1 = self.up_c_2_1(x_u_2, x_b_2)\n        x_c_2 = self.up_c_2_2(x_c_1, x_c_2_1) # joint predict low supervise here\n\n        # decoder part\n        map_x = F.interpolate(self.final_x(x_c), scale_factor=16, mode='bilinear', align_corners=True)\n        map_1 = F.interpolate(self.final_1(x_b_2), scale_factor=4, mode='bilinear', align_corners=True)\n        map_2 = F.interpolate(self.final_2(x_c_2), scale_factor=4, mode='bilinear', align_corners=True)\n        return map_x, map_1, map_2\n\n    def init_weights(self):\n        self.up1.apply(init_weights)\n        self.up2.apply(init_weights)\n        self.final_x.apply(init_weights)\n        self.final_1.apply(init_weights)\n        self.final_2.apply(init_weights)\n        self.up_c.apply(init_weights)\n        self.up_c_1_1.apply(init_weights)\n        self.up_c_1_2.apply(init_weights)\n        self.up_c_2_1.apply(init_weights)\n        self.up_c_2_2.apply(init_weights)\n\n\nclass TransFuse_L(nn.Module):\n    def __init__(self, num_classes=1, drop_rate=0.2, normal_init=True, pretrained=False):\n        super(TransFuse_L, self).__init__()\n        self.n_classes = num_classes\n        self.n_channels = 3\n        \n        self.resnet = resnet50()\n        if pretrained:\n            self.resnet.load_state_dict(torch.load('pretrained/resnet50-19c8e357.pth'))\n            \n        self.resnet.fc = nn.Identity()\n        self.resnet.layer4 = nn.Identity()\n\n        self.transformer = deit_base_patch16_224(pretrained=pretrained)\n\n        self.up1 = Up(in_ch1=768, out_ch=512)\n        self.up2 = Up(512, 256)\n\n        self.final_x = nn.Sequential(\n            Conv(1024, 256, 1, bn=True, relu=True),\n            Conv(256, 256, 3, bn=True, relu=True),\n            Conv(256, num_classes, 3, bn=False, relu=False)\n            )\n\n        self.final_1 = nn.Sequential(\n            Conv(256, 256, 3, bn=True, relu=True),\n            Conv(256, num_classes, 3, bn=False, relu=False)\n            )\n\n        self.final_2 = nn.Sequential(\n            Conv(256, 256, 3, bn=True, relu=True),\n            Conv(256, num_classes, 3, bn=False, relu=False)\n            )\n\n        self.up_c = BiFusion_block(ch_1=1024, ch_2=768, r_2=4, ch_int=1024, ch_out=1024, drop_rate=drop_rate/2)\n\n        self.up_c_1_1 = BiFusion_block(ch_1=512, ch_2=512, r_2=2, ch_int=512, ch_out=512, drop_rate=drop_rate/2)\n        self.up_c_1_2 = Up(in_ch1=1024, out_ch=512, in_ch2=512, attn=True)\n\n        self.up_c_2_1 = BiFusion_block(ch_1=256, ch_2=256, r_2=1, ch_int=256, ch_out=256, drop_rate=drop_rate/2)\n        self.up_c_2_2 = Up(512, 256, 256, attn=True)\n\n        self.drop = nn.Dropout2d(drop_rate)\n\n        if normal_init:\n            self.init_weights()\n\n    def forward(self, imgs, labels=None):\n        # bottom-up path\n        x_b = self.transformer(imgs)\n        x_b = torch.transpose(x_b, 1, 2)\n        x_b = x_b.view(x_b.shape[0], -1, 12, 16)\n        x_b = self.drop(x_b)\n\n        x_b_1 = self.up1(x_b)\n        x_b_1 = self.drop(x_b_1)\n\n        x_b_2 = self.up2(x_b_1)  # transformer pred supervise here\n        x_b_2 = self.drop(x_b_2)\n\n\n        # top-down path\n        x_u = self.resnet.conv1(imgs)\n        x_u = self.resnet.bn1(x_u)\n        x_u = self.resnet.relu(x_u)\n        x_u = self.resnet.maxpool(x_u)\n\n        x_u_2 = self.resnet.layer1(x_u)\n        x_u_2 = self.drop(x_u_2)\n\n        x_u_1 = self.resnet.layer2(x_u_2)\n        x_u_1 = self.drop(x_u_1)\n\n        x_u = self.resnet.layer3(x_u_1)\n        x_u = self.drop(x_u)\n\n\n        # joint path\n        x_c = self.up_c(x_u, x_b)\n\n        x_c_1_1 = self.up_c_1_1(x_u_1, x_b_1)\n        x_c_1 = self.up_c_1_2(x_c, x_c_1_1)\n\n        x_c_2_1 = self.up_c_2_1(x_u_2, x_b_2)\n        x_c_2 = self.up_c_2_2(x_c_1, x_c_2_1) # joint predict low supervise here\n\n\n        # decoder part\n        map_x = F.interpolate(self.final_x(x_c), scale_factor=16, mode='bilinear', align_corners=True)\n        map_1 = F.interpolate(self.final_1(x_b_2), scale_factor=4, mode='bilinear', align_corners=True)\n        map_2 = F.interpolate(self.final_2(x_c_2), scale_factor=4, mode='bilinear', align_corners=True)\n\n        return map_x, map_1, map_2\n\n    def init_weights(self):\n        self.up1.apply(init_weights)\n        self.up2.apply(init_weights)\n        self.final_x.apply(init_weights)\n        self.final_1.apply(init_weights)\n        self.final_2.apply(init_weights)\n        self.up_c.apply(init_weights)\n        self.up_c_1_1.apply(init_weights)\n        self.up_c_1_2.apply(init_weights)\n        self.up_c_2_1.apply(init_weights)\n        self.up_c_2_2.apply(init_weights)\n        \n\nclass TransFuse_L_384(nn.Module):\n    def __init__(self, num_classes=1, drop_rate=0.2, normal_init=True, pretrained=False):\n        super(TransFuse_L_384, self).__init__()\n        self.n_classes = num_classes\n        self.n_channels = 3\n        \n        self.resnet = resnet50()\n        if pretrained:\n            self.resnet.load_state_dict(torch.load('pretrained/resnet50-19c8e357.pth'))\n        self.resnet.fc = nn.Identity()\n        self.resnet.layer4 = nn.Identity()\n\n        self.transformer = deit_base_patch16_384(pretrained=pretrained)\n\n        self.up1 = Up(in_ch1=768, out_ch=512)\n        self.up2 = Up(512, 256)\n\n        self.final_x = nn.Sequential(\n            Conv(1024, 256, 1, bn=True, relu=True),\n            Conv(256, 256, 3, bn=True, relu=True),\n            Conv(256, num_classes, 3, bn=False, relu=False)\n            )\n\n        self.final_1 = nn.Sequential(\n            Conv(256, 256, 3, bn=True, relu=True),\n            Conv(256, num_classes, 3, bn=False, relu=False)\n            )\n\n        self.final_2 = nn.Sequential(\n            Conv(256, 256, 3, bn=True, relu=True),\n            Conv(256, num_classes, 3, bn=False, relu=False)\n            )\n\n        self.up_c = BiFusion_block(ch_1=1024, ch_2=768, r_2=4, ch_int=1024, ch_out=1024, drop_rate=drop_rate/2)\n\n        self.up_c_1_1 = BiFusion_block(ch_1=512, ch_2=512, r_2=2, ch_int=512, ch_out=512, drop_rate=drop_rate/2)\n        self.up_c_1_2 = Up(in_ch1=1024, out_ch=512, in_ch2=512, attn=True)\n\n        self.up_c_2_1 = BiFusion_block(ch_1=256, ch_2=256, r_2=1, ch_int=256, ch_out=256, drop_rate=drop_rate/2)\n        self.up_c_2_2 = Up(512, 256, 256, attn=True)\n\n        self.drop = nn.Dropout2d(drop_rate)\n\n        if normal_init:\n            self.init_weights()\n\n    def forward(self, imgs, labels=None):\n        # bottom-up path\n        x_b = self.transformer(imgs)\n        x_b = torch.transpose(x_b, 1, 2)\n        x_b = x_b.view(x_b.shape[0], -1, 24, 32)\n        x_b = self.drop(x_b)\n\n        x_b_1 = self.up1(x_b)\n        x_b_1 = self.drop(x_b_1)\n\n        x_b_2 = self.up2(x_b_1)  # transformer pred supervise here\n        x_b_2 = self.drop(x_b_2)\n\n\n        # top-down path\n        x_u = self.resnet.conv1(imgs)\n        x_u = self.resnet.bn1(x_u)\n        x_u = self.resnet.relu(x_u)\n        x_u = self.resnet.maxpool(x_u)\n\n        x_u_2 = self.resnet.layer1(x_u)\n        x_u_2 = self.drop(x_u_2)\n\n        x_u_1 = self.resnet.layer2(x_u_2)\n        x_u_1 = self.drop(x_u_1)\n\n        x_u = self.resnet.layer3(x_u_1)\n        x_u = self.drop(x_u)\n\n\n        # joint path\n        x_c = self.up_c(x_u, x_b)\n\n        x_c_1_1 = self.up_c_1_1(x_u_1, x_b_1)\n        x_c_1 = self.up_c_1_2(x_c, x_c_1_1)\n\n        x_c_2_1 = self.up_c_2_1(x_u_2, x_b_2)\n        x_c_2 = self.up_c_2_2(x_c_1, x_c_2_1) # joint predict low supervise here\n\n\n        # decoder part\n        map_x = F.interpolate(self.final_x(x_c), scale_factor=16, mode='bilinear', align_corners=True)\n        map_1 = F.interpolate(self.final_1(x_b_2), scale_factor=4, mode='bilinear', align_corners=True)\n        map_2 = F.interpolate(self.final_2(x_c_2), scale_factor=4, mode='bilinear', align_corners=True)\n\n        return map_x, map_1, map_2\n\n    def init_weights(self):\n        self.up1.apply(init_weights)\n        self.up2.apply(init_weights)\n        self.final_x.apply(init_weights)\n        self.final_1.apply(init_weights)\n        self.final_2.apply(init_weights)\n        self.up_c.apply(init_weights)\n        self.up_c_1_1.apply(init_weights)\n        self.up_c_1_2.apply(init_weights)\n        self.up_c_2_1.apply(init_weights)\n        self.up_c_2_2.apply(init_weights)\n\n\ndef init_weights(m):\n    \"\"\"\n    Initialize weights of layers using Kaiming Normal (He et al.) as argument of \"Apply\" function of\n    \"nn.Module\"\n    :param m: Layer to initialize\n    :return: None\n    \"\"\"\n    if isinstance(m, nn.Conv2d):\n        '''\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n        trunc_normal_(m.weight, std=math.sqrt(1.0/fan_in)/.87962566103423978)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n        '''\n        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n        if m.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n            bound = 1 / math.sqrt(fan_in)\n            nn.init.uniform_(m.bias, -bound, bound)\n        \n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n    def __init__(self, in_ch1, out_ch, in_ch2=0, attn=False):\n        super().__init__()\n\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv = DoubleConv(in_ch1+in_ch2, out_ch)\n\n        if attn:\n            self.attn_block = Attention_block(in_ch1, in_ch2, out_ch)\n        else:\n            self.attn_block = None\n\n    def forward(self, x1, x2=None):\n\n        x1 = self.up(x1)\n        # input is CHW\n        if x2 is not None:\n            diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n            diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n\n            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                            diffY // 2, diffY - diffY // 2])\n\n            if self.attn_block is not None:\n                x2 = self.attn_block(x1, x2)\n            x1 = torch.cat([x2, x1], dim=1)\n        x = x1\n        return self.conv(x)\n\n\nclass Attention_block(nn.Module):\n    def __init__(self,F_g,F_l,F_int):\n        super(Attention_block,self).__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n            nn.BatchNorm2d(F_int)\n            )\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n        \n    def forward(self,g,x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1+x1)\n        psi = self.psi(psi)\n        return x*psi\n\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels)\n        )\n        self.identity = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n                nn.BatchNorm2d(out_channels)\n                )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        return self.relu(self.double_conv(x)+self.identity(x))\n\n\nclass Residual(nn.Module):\n    def __init__(self, inp_dim, out_dim):\n        super(Residual, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.bn1 = nn.BatchNorm2d(inp_dim)\n        self.conv1 = Conv(inp_dim, int(out_dim/2), 1, relu=False)\n        self.bn2 = nn.BatchNorm2d(int(out_dim/2))\n        self.conv2 = Conv(int(out_dim/2), int(out_dim/2), 3, relu=False)\n        self.bn3 = nn.BatchNorm2d(int(out_dim/2))\n        self.conv3 = Conv(int(out_dim/2), out_dim, 1, relu=False)\n        self.skip_layer = Conv(inp_dim, out_dim, 1, relu=False)\n        if inp_dim == out_dim:\n            self.need_skip = False\n        else:\n            self.need_skip = True\n        \n    def forward(self, x):\n        if self.need_skip:\n            residual = self.skip_layer(x)\n        else:\n            residual = x\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n        out += residual\n        return out \n\n\nclass Conv(nn.Module):\n    def __init__(self, inp_dim, out_dim, kernel_size=3, stride=1, bn=False, relu=True, bias=True):\n        super(Conv, self).__init__()\n        self.inp_dim = inp_dim\n        self.conv = nn.Conv2d(inp_dim, out_dim, kernel_size, stride, padding=(kernel_size-1)//2, bias=bias)\n        self.relu = None\n        self.bn = None\n        if relu:\n            self.relu = nn.ReLU(inplace=True)\n        if bn:\n            self.bn = nn.BatchNorm2d(out_dim)\n\n    def forward(self, x):\n        assert x.size()[1] == self.inp_dim, \"{} {}\".format(x.size()[1], self.inp_dim)\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:36.859404Z","iopub.execute_input":"2023-10-05T14:57:36.859884Z","iopub.status.idle":"2023-10-05T14:57:36.931091Z","shell.execute_reply.started":"2023-10-05T14:57:36.859849Z","shell.execute_reply":"2023-10-05T14:57:36.930011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 模型测试","metadata":{}},{"cell_type":"code","source":"\n@torch.inference_mode()\ndef evaluate(net, dataloader, device, amp):\n    net.eval()\n    num_val_batches = len(dataloader)\n    dice_score = 0\n    iou_score = 0\n\n    if isinstance(model, nn.DataParallel):\n        n_classes = net.module.n_classes\n    else:\n        n_classes = net.n_classes\n    criterion = nn.CrossEntropyLoss() if n_classes > 1 else nn.BCEWithLogitsLoss()\n    dice_loss = smp.losses.DiceLoss(mode='binary', log_loss=True, from_logits = True).cuda()\n   \n    \n    # iterate over the validation set\n    with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n        for batch in tqdm(dataloader, total=num_val_batches, desc='Validation round', unit='batch', position=0 ,leave=True):\n            image, mask_true = batch\n\n            # move images and labels to correct device and type\n            image = image.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n            mask_true = mask_true.to(device=device, dtype=torch.float32)\n\n            # predict the mask\n            mask_pred = net(image)\n            dice_score += criterion(mask_pred, mask_true.float())\n            dice_score += dice_loss(mask_pred, mask_true)\n            \n            tp, fp, fn, tn = smp.metrics.get_stats(mask_pred, mask_true.long(), mode='binary', threshold=0.5)\n            iou_score += smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n            \n    dice_loss = (dice_score / max(num_val_batches, 1))\n    iou_score = (iou_score / max(num_val_batches, 1))\n    print(\"Validation dice loss: {}, IoU Score {}\".format(dice_loss, iou_score))\n    return (dice_loss, iou_score)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:36.935852Z","iopub.execute_input":"2023-10-05T14:57:36.938268Z","iopub.status.idle":"2023-10-05T14:57:36.950033Z","shell.execute_reply.started":"2023-10-05T14:57:36.938229Z","shell.execute_reply":"2023-10-05T14:57:36.949025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 模型训练","metadata":{}},{"cell_type":"code","source":"import time\nimport torch.optim as optim\nimport segmentation_models_pytorch as smp\n\nn_train = len(train_data)\nn_val = len(validate_data)\n\n\ndef train(model, device, \n          project = 'U-Net',\n          epochs: int = 60,\n          learning_rate: float = 1e-5, \n          weight_decay: float = 1e-8,\n          momentum: float = 0.999,\n          batch_size: int = 2,\n          amp: bool = False,\n          val_percent: float = 0.1,\n          gradient_clipping: float = 1.0):\n    \n    if isinstance(model, nn.DataParallel):\n        n_classes = model.module.n_classes\n        n_channels = model.module.n_channels\n    else:\n        n_classes = model.n_classes\n        n_channels = model.n_channels\n        \n    # (Initialize logging)\n    experiment = wandb.init(project=project, resume='allow', anonymous='must')\n    experiment.config.update(\n        dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n             val_percent=val_percent, amp=amp)\n    )\n\n    logging.info(f'''Starting training:\n        Epochs:          {epochs}\n        Batch size:      {batch_size}\n        Learning rate:   {learning_rate}\n        Training size:   {n_train}\n        Validation size: {n_val}\n        Device:          {device.type}\n        Mixed Precision: {amp}\n    ''')\n    \n\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n#     optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],\n#                               nesterov=config['nesterov'], weight_decay=config['weight_decay'])\n\n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2, eta_min=5e-5)\n    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n    \n    criterion = nn.CrossEntropyLoss().cuda()\n    dice_loss = smp.losses.DiceLoss(mode='binary').cuda()\n    \n    global_step = 0\n    \n\n    # 5. Begin training\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss = 0\n        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n            for batch in trainloader:\n                images, true_masks = batch\n\n                assert images.shape[1] == n_channels, \\\n                    f'Network has been defined with {n_channels} input channels, ' \\\n                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n                    'the images are loaded correctly.'\n\n                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n                \n                true_masks = true_masks.to(device=device, dtype=torch.long)\n\n                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n                    masks_pred = model(images)\n                    loss = criterion(masks_pred, true_masks.float())\n                    loss += dice_loss(masks_pred, true_masks)\n                    tp, fp, fn, tn = smp.metrics.get_stats(masks_pred, true_masks.long(), mode='binary', threshold=0.5)\n                    iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n    \n                optimizer.zero_grad(set_to_none=True)\n                grad_scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n                grad_scaler.step(optimizer)\n                grad_scaler.update()\n\n                pbar.update(images.shape[0])\n                global_step += 1\n                epoch_loss += loss.item()\n                experiment.log({\n                    'train iou': iou_score,\n                    'train loss': loss.item(),\n                    'step': global_step,\n                    'epoch': epoch\n                })\n                pbar.set_postfix(**{'loss (batch)': loss.item()})\n\n                # Evaluation round\n                division_step = (n_train // (5 * batch_size))\n                if division_step > 0:\n                    if global_step % division_step == 0:\n                        histograms = {}\n                        for tag, value in model.named_parameters():\n                            tag = tag.replace('/', '.')\n                            if not (torch.isinf(value) | torch.isnan(value)).any():\n                                histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n                            if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():\n                                histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n\n                        val_score, iou_score = evaluate(model, valloader, device, amp)\n                        model.train()\n                        scheduler.step(val_score)\n\n                        logging.info('Validation Dice score: {}'.format(val_score))\n                        try:\n                            experiment.log({\n                                'learning rate': optimizer.param_groups[0]['lr'],\n                                'validation Dice': val_score,\n                                'validation IoU Score': iou_score,\n                                'images': wandb.Image(images[0].cpu()),\n                                'masks': {\n                                    'true': wandb.Image(true_masks[0].float().cpu()),\n                                    'pred': wandb.Image(masks_pred[0].float().cpu()),\n                                },\n                                'step': global_step,\n                                'epoch': epoch,\n                                **histograms\n                            })\n                        except:\n                            pass\n            \n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:36.951576Z","iopub.execute_input":"2023-10-05T14:57:36.952216Z","iopub.status.idle":"2023-10-05T14:57:36.981743Z","shell.execute_reply.started":"2023-10-05T14:57:36.952183Z","shell.execute_reply":"2023-10-05T14:57:36.980485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train transfuse","metadata":{}},{"cell_type":"code","source":"model = TransFuse_S(pretrained=False)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(memory_format=torch.channels_last)\nmodel.to(device)\n\ntrain(model, device, project='Transfuse')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T14:57:37.014626Z","iopub.execute_input":"2023-10-05T14:57:37.014921Z","iopub.status.idle":"2023-10-05T14:58:13.835251Z","shell.execute_reply.started":"2023-10-05T14:57:37.014892Z","shell.execute_reply":"2023-10-05T14:58:13.832793Z"},"trusted":true},"execution_count":null,"outputs":[]}]}